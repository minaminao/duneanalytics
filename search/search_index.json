{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":".md-typeset h1, .md-content__button { display: none; } .md-header__topic{ font-weight: bold; } Dune\u306f\u3001\u81a8\u5927\u306a\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30c7\u30fc\u30bf\u3092\u767a\u898b\u3001\u63a2\u7d22\u3001\u53ef\u8996\u5316\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u3059\u3079\u3066\u306e\u30c4\u30fc\u30eb\u3092\u5099\u3048\u305f\u3001\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u7814\u7a76\u306e\u305f\u3081\u306e\u5f37\u529b\u306a\u30c4\u30fc\u30eb\u3067\u3059\u3002 \u30c7\u30e5\u30fc\u30f3\u306f\u3001\u3053\u3093\u306a\u7591\u554f\u3092\u89e3\u6c7a\u3059\u308b\u9375\u306b\u306a\u308a\u307e\u3059\u3002 How much volume flows through Uniswap each day? Which Dex has the highest volume? How are important Stablecoins behaving today? Dune in 5-minutes \u26a1 \u00b6 How the data flows \u00b6 \u30d1\u30d6\u30ea\u30c3\u30af\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306f open and free \u306a\u306e\u3067\u3001\u305d\u3053\u304b\u3089\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u306e\u306f\u305d\u3093\u306a\u306b\u96e3\u3057\u304f\u306a\u3044\u306f\u305a\u3067\u3059\u3088\u306d\uff1f \u30a4\u30a8\u30b9\u3067\u3082\u3042\u308a\u3001\u30ce\u30fc\u3067\u3082\u3042\u308b\u3002 \u4f8b\u3048\u3070\u3001\u56fd\u969b\u8f38\u9001\u306e\u30b9\u30d4\u30fc\u30c9\u304c\u30d1\u30ea\u306e\u6700\u65b0\u30af\u30c1\u30e5\u30fc\u30eb\u306e\u6d88\u8cbb\u8005\u9700\u8981\u306b\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3059\u308b\u304b\u3092\u5206\u6790\u3059\u308b\u305f\u3081\u306b\u3001\u5f93\u6765\u306e\u30d3\u30b8\u30cd\u30b9\u304b\u3089\u5b64\u7acb\u3057\u305f\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u306b\u6bd4\u3079\u308c\u3070...\u3002 \u305d\u3046\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u3092\u95b2\u89a7\u3001\u5206\u6790\u3059\u308b\u3053\u3068\u306f \"\u7c21\u5358 \"\u3067\u3059\u3002 \u3057\u304b\u3057\u3001Dune\u306e\u3053\u3068\u308f\u3056\u306e\u30d5\u30fc\u30c9\u306e\u4e0b\u306b\u306f\u591a\u304f\u306e\u3053\u3068\u304c\u8d77\u3053\u3063\u3066\u3044\u307e\u3059\u3002\u30a4\u30fc\u30b5\u30ea\u30a2\u30e0\u306e\u3088\u3046\u306a\u30d1\u30d6\u30ea\u30c3\u30af\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u4e0a\u306e\u72b6\u614b\u306e\u5909\u5316\u304c\u3001\u3069\u306e\u3088\u3046\u306b\u3057\u3066\u30c1\u30e3\u30fc\u30c8\u3084\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306b\u30af\u30a8\u30ea\u3059\u308b\u30c7\u30fc\u30bf\u306b\u5909\u308f\u308b\u306e\u304b\u3092\u3088\u308a\u3088\u304f\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u3001\u30d5\u30fc\u30c9\u3092\u958b\u3051\u3066\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002 1. A chain adds a block \u00b6 \u6280\u8853\u7684\u306a\u8a73\u7d30\u306f\u3055\u307e\u3056\u307e\u3067\u3059\u304c\u3001\u3059\u3079\u3066\u306e\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u4e2d\u6838\u306b\u306f\u3001\u4e00\u9023\u306e\u53d6\u5f15\u304c\u63d0\u6848\u3055\u308c\u3001\u5408\u610f\u3055\u308c\u305f\u5f8c\u3001\u4ee5\u524d\u306b\u5408\u610f\u3055\u308c\u305f\u53d6\u5f15\u3092\u542b\u3080\u30d6\u30ed\u30c3\u30af\u306e\u30c1\u30a7\u30fc\u30f3\u306e\u672b\u5c3e\u306b\u8ffd\u52a0\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 \u3069\u306e\u30d6\u30ed\u30c3\u30af\u3092\u6b21\u306e\u30d6\u30ed\u30c3\u30af\u3068\u3059\u308b\u304b\u306f\u3001\u69d8\u3005\u306a consensus mechanisms \u3001\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u30b3\u30f3\u30bb\u30f3\u30b5\u30b9\u304c\u5f97\u3089\u308c\u308b\u3068\u3001\u6700\u65b0\u306e\u30d6\u30ed\u30c3\u30af\u306b\u95a2\u3059\u308b\u60c5\u5831\u304c\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u3055\u308c\u3001\u53c2\u52a0\u8005\uff08\u300c\u30ce\u30fc\u30c9\u300d\uff09\u306b\u3053\u306e\u65b0\u3057\u3044\u30d6\u30ed\u30c3\u30af\u3092\u77e5\u3089\u305b\u3001\u81ea\u5206\u306e\u8a18\u9332\u306b\u8ffd\u52a0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u4ed5\u7d44\u307f\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u77e5\u308a\u305f\u3044\u65b9\u306f Check out this awesome Blockchain 101 demo ! 2. Node providers transmit data to Dune \u00b6 \u3053\u306e\u300c\u65b0\u3057\u3044\u30d6\u30ed\u30c3\u30af\u304c\u4f5c\u6210\u3055\u308c\u305f\u300d\u3068\u3044\u3046\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u53d7\u3051\u53d6\u308b\u306b\u306f\u3001\u8ab0\u304b\u304c blockchain node \u3092\u5b9f\u884c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 blockchain node \u306f\u3001\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u63a5\u7d9a\u3057\u3001\u4ed6\u306e\u30ce\u30fc\u30c9\u9593\u3067\u60c5\u5831\u3092\u9001\u3063\u305f\u308a\u3001\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u53d6\u5f15\u306e\u691c\u8a3c\u3084\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3092\u53ef\u80fd\u306b\u3059\u308b\u300c\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u300d\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u5b9f\u884c\u3059\u308b\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3067\u3059\u3002 \u3061\u3087\u3063\u3068\u3057\u305f\u6280\u8853\u7684\u306a\u30ce\u30a6\u30cf\u30a6\u304c\u3042\u308c\u3070\u3001\u8ab0\u3067\u3082\u30ce\u30fc\u30c9\u3092\u52d5\u304b\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u305d\u308c\u304c\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3 public \u306e\u5927\u304d\u306a\u7279\u5fb4\u3067\u3059 \u307b\u307c\u5168\u54e1\u304c\u30ce\u30fc\u30c9\u3092\u904b\u55b6\u3067\u304d\u308b\u305f\u3081\u3001\u53c2\u52a0\u8005\u306e\u8aa0\u5b9f\u3055\u3092\u4fdd\u3064\u305f\u3081\u306b\u3001\u30b7\u30b9\u30c6\u30e0\u306b\u306f\u591a\u304f\u306e\u900f\u660e\u6027\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u900f\u660e\u6027\u306b\u3088\u308a\u3001\u30c7\u30fc\u30bf\u30a2\u30ca\u30ea\u30b9\u30c8\u304c\u300c\u4f55\u304c\u8d77\u3053\u3063\u3066\u3044\u308b\u304b\u300d\u306e\u5168\u4f53\u50cf\u306b\u30a2\u30af\u30bb\u30b9\u3057\u3001\u3042\u3089\u3086\u308b\u7a2e\u985e\u306e\u5206\u6790\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001\u30cf\u30a4\u30d6\u30de\u30a4\u30f3\u30c9\u3092\u6d3b\u7528\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002 \u751f\u30c7\u30fc\u30bf\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u305f\u3081\u306e\u82e6\u52b4\u306f\u5fc5\u8981\u306a\u3044\u3002 Dune\u306e\u3088\u3046\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304c\u5927\u898f\u6a21\u306b\u904b\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u3001 node providers \u306f\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u30ce\u30fc\u30c9\u30a4\u30f3\u30d5\u30e9\u3092\u69cb\u7bc9\u30fb\u904b\u7528\u3057\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\uff08API\uff09\u3092\u901a\u3058\u3066\u79c1\u305f\u3061\u304c\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002 \u3053\u3046\u3059\u308b\u3053\u3068\u3067\u3001\u79c1\u305f\u3061\u306f\u53ef\u80fd\u306a\u9650\u308a\u6700\u9ad8\u306e\u30c7\u30fc\u30bf\u30a2\u30af\u30bb\u30b9\u4f53\u9a13\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u306b\u96c6\u4e2d\u3067\u304d\u3001\u30ce\u30fc\u30c9\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306f\u53ef\u80fd\u306a\u9650\u308a\u52b9\u7387\u7684\u306b\u30ce\u30fc\u30c9\u3092\u7a3c\u50cd\u3055\u305b\u308b\u3053\u3068\u306b\u96c6\u4e2d\u3067\u304d\u308b\u306e\u3067\u3059\u3002 3. Dune adds raw data to SQL tables \u00b6 \u30ce\u30fc\u30c9\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306f\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u53d6\u5f15\u30c7\u30fc\u30bf\u3092\u30cf\u30c3\u30b7\u30e5\u5316\u3055\u308c\u305f\u30d0\u30a4\u30c8\u30b3\u30fc\u30c9\u3068\u3057\u3066\u9001\u3063\u3066\u304d\u307e\u3059\uff08\u4f8b\u3048\u3070\u3001\u30a4\u30fc\u30b5\u30ea\u30a2\u30e0\u306e\u30c7\u30fc\u30bf\u306f the keccak256 algorithm \u3067\u30cf\u30c3\u30b7\u30e5\u5316\u3055\u308c\u3066\u3044\u307e\u3059\uff09\u3002 Dune Data Engine\u306f\u3053\u306e\u30d0\u30a4\u30c8\u30b3\u30fc\u30c9\u3092\u53d6\u308a\u51fa\u3057\u3001\" Raw Data \"\u3068\u547c\u3076\u30c6\u30fc\u30d6\u30eb\u7fa4\u306b\u62bd\u51fa\u3057\u307e\u3059\u3002 \u3053\u308c\u3089\u306f\u30c1\u30a7\u30fc\u30f3\u306b\u3088\u3063\u3066\u591a\u5c11\u7570\u306a\u308a\u307e\u3059\u304c\u3001\u4e00\u4f8b\u3068\u3057\u3066\u3001\u307b\u3068\u3093\u3069\u306e Ethereum Virtual Machine (EVM) \u30d9\u30fc\u30b9\u306e\u30c1\u30a7\u30fc\u30f3\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002 [chain].blocks - \u30c1\u30a7\u30fc\u30f3\u306b\u8ffd\u52a0\u3055\u308c\u305f\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u306e\u30b0\u30eb\u30fc\u30d7\u3002 [chain].creation_traces - create traces \u3092\u542b\u3080\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\uff08\u5185\u90e8\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u306b\u542b\u307e\u308c\u308b\u3053\u3068\u3082\u3042\u308b\uff09 \u30b9\u30de\u30fc\u30c8\u30b3\u30f3\u30c8\u30e9\u30af\u30c8\u3067\u4f5c\u6210\u3055\u308c\u305f [chain].logs \uff5e event logs \u30d6\u30ed\u30c3\u30af\u5185\u306e\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u3067\u767a\u751f\u3059\u308b [chain].traces \uff5e trace data [chain].transactions - \u3042\u308b\u30a2\u30c9\u30ec\u30b9\u304b\u3089\u5225\u306e\u30a2\u30c9\u30ec\u30b9\u306b\u9001\u4fe1\u3055\u308c\u308b\u6697\u53f7\u5316\u3055\u308c\u305f\u7f72\u540d\u4ed8\u304d\u547d\u4ee4\u3002 \u3053\u308c\u3089\u306e\u30c6\u30fc\u30d6\u30eb\u306e\u30c7\u30fc\u30bf\u306f\u4eba\u9593\u304c\u8aad\u3080\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u304c\uff08\u30d0\u30a4\u30c8\u30b3\u30fc\u30c9\u306f\u305d\u3046\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff09\u3001\u7406\u89e3\u3057\u89e3\u91c8\u3059\u308b\u306b\u306f\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306b\u95a2\u3059\u308b\u5e45\u5e83\u3044\u77e5\u8b58\u304c\u5fc5\u8981\u3067\u3059\u3002 \u3053\u306eRaw Data\u306f\u3001\u8208\u5473\u6df1\u3044\u30a4\u30f3\u30b5\u30a4\u30c8\u306b\u64cd\u4f5c\u3059\u308b\u306e\u306b\u591a\u304f\u306e\u624b\u9593\u304c\u304b\u304b\u308b\u3053\u3068\u3082\u3042\u308a\u3001Dune decodes \u306f\u3053\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002 4. Dune decodes raw data \u00b6 \u751f\u306e .log \u30c6\u30fc\u30d6\u30eb\u306f\u3001\u6b21\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u3092\u8fd4\u3057\u307e\u3059\u3002 \u3053\u306e\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u306f\u3001\u304b\u306a\u308a\u9650\u5b9a\u7684\u306a\u30c7\u30fc\u30bf\u5206\u6790\u306e\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u304c\u3042\u308b\u3002 \u3053\u306e\u30c7\u30fc\u30bf\u3092\u3088\u308a\u4f7f\u3044\u3084\u3059\u3044\u3082\u306e\u306b\u5909\u63db\u3059\u308b\u305f\u3081\u306b\u3001 Wizards submit smart contracts for decoding here . \u305d\u306e\u969b\u3001\u30b9\u30de\u30fc\u30c8\u30b3\u30f3\u30c8\u30e9\u30af\u30c8\u306e Application Binary Interface (ABI) \u3068\u3044\u3046Web2.0\u306eAPI\u306b\u4f3c\u305f\u3082\u306e\u3092\u4f7f\u3063\u3066\u3001\u30b3\u30f3\u30c8\u30e9\u30af\u30c8\u3068\u3084\u308a\u53d6\u308a\u3059\u308b\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u306e\u4e2d\u3067\u4f55\u304c\u8d77\u3053\u3063\u3066\u3044\u308b\u304b\u3092\u7406\u89e3\u3057\u307e\u3059\u3002 \u305d\u3057\u3066\u3001\u5206\u6790\u304c\u3057\u3084\u3059\u3044 Decoded Tables \u3092\u4f5c\u308b\u306e\u3067\u3059\u3002 \u4f8b\u3048\u3070\u3001\u4e0a\u8a18\u306e\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u3092\u30c7\u30b3\u30fc\u30c9\u3059\u308b\u3068\u3001\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002 5. The Dune community casts Spells \u00b6 Dune\u306f\u3001Wizards\u3068\u3044\u3046\u7d20\u6674\u3089\u3057\u3044\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306e\u529b\u3092\u501f\u308a\u3066\u3001 Spells \u3067\u30c7\u30fc\u30bf\u3092\u30c7\u30b3\u30fc\u30c9\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u3055\u3089\u306b\u4e00\u6b69\u8e0f\u307f\u8fbc\u307f\u307e\u3057\u305f\u3002 \u30b9\u30da\u30eb\u306f\u3001Dune\u3068\u6211\u3005\u306e\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306b\u3088\u3063\u3066\u69cb\u7bc9\u30fb\u7dad\u6301\u3055\u308c\u3066\u3044\u308b\u30ab\u30b9\u30bf\u30e0\u30c6\u30fc\u30d6\u30eb\u3067\u3001\u3067\u304d\u308b\u3060\u3051\u6469\u64e6\u3092\u5c11\u306a\u304f\u3057\u3066\u591a\u304f\u306e\u30c7\u30fc\u30bf\u3092\u7c21\u5358\u306b\u96c6\u8a08\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002 \u4f8b\u3048\u3070\u6700\u3082\u4eba\u6c17\u306e\u3042\u308b\u30b9\u30da\u30eb\u306e1\u3064\u3067\u3042\u308b nft.trades \u306f\u3001Solana\u4e0a\u306eMagic Eden\u3084Ethereum\u4e0a\u306eLooksRare\u306a\u3069\u306e\u53d6\u5f15\u3092\u81ea\u5206\u3067\u30b3\u30f3\u30d1\u30a4\u30eb\u3057\u306a\u304f\u3066\u3082\u3001\u30d7\u30ed\u30c8\u30b3\u30eb\u3084\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u9593\u3067NFT\u53d6\u5f15\u30c7\u30fc\u30bf\u306e\u63a2\u7d22\u3068\u5909\u63db\u3092\u7c21\u5358\u306b\u884c\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002 6. Dune Wizards make magic \u00b6 \u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u304b\u3089\u3001\u30a6\u30a3\u30b6\u30fc\u30c9\u306f\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3078\u306e\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3001\u64cd\u4f5c\u3001\u691c\u7d22\u306b\u5e83\u304f\u4f7f\u308f\u308c\u3066\u3044\u308b\u8a00\u8a9e\u3067\u3042\u308bSQL\u3092\u4f7f\u3063\u3066 Queries \u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002 \u3053\u306e\u30af\u30a8\u30ea\u304b\u3089\u3001\u79c1\u305f\u3061\u304c\u3088\u304f\u77e5\u3063\u3066\u3044\u308b Visualizations \u3068 Dashboards \u304c\u4f5c\u3089\u308c\u308b\u306e\u3067\u3059\u3002 Eg @rchen8 \u306eOpenSea\u306e\u65e5\u6b21\u30dc\u30ea\u30e5\u30fc\u30e0\u3002 Making \ud83e\ude84 with dune.com \u00b6 Dune.com\u306f\u3001Dune Data Platform\u306e\u4e0a\u306b\u69cb\u7bc9\u3055\u308c\u305f\u6700\u521d\u306e\u30ad\u30e9\u30fc\u30a2\u30d7\u30ea\u3067\u3001SQL\u3001Ethereum Virtual Machine\u3001\u30d3\u30b8\u30cd\u30b9\u306e\u77e5\u8b58\u304c\u5c11\u3057\u3067\u3082\u3042\u308c\u3070\u3001\u8ab0\u3067\u3082\u3067\u304d\u308b\u3060\u3051\u7c21\u5358\u306b\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30c7\u30fc\u30bf\u3092\u8208\u5473\u6df1\u3044\u65b9\u6cd5\u3067\u5206\u6790\u3067\u304d\u308b\u3088\u3046\u306b\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002 Dune.com\u30a2\u30d7\u30ea\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u306f\u4ee5\u4e0b\u306e\u3068\u304a\u308a\u3067\u3059\u3002 Dashboards: \u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30c7\u30fc\u30bf\u306e\u7279\u5b9a\u306e\u30b0\u30eb\u30fc\u30d7\u306b\u3064\u3044\u3066\u306e\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u4f1d\u3048\u308b\u30d3\u30b8\u30e5\u30a2\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3\u3068\u30c6\u30ad\u30b9\u30c8\u3092\u542b\u3080\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u30bb\u30c3\u30c8\u3067\u3059\u3002 Visualizations: \u8868\u5f62\u5f0f\u3067\u308f\u304b\u308a\u306b\u304f\u3044\u30c7\u30fc\u30bf\u3092\u3001\u8996\u899a\u7684\u306b\u308f\u304b\u308a\u3084\u3059\u304f\u3059\u308b\u30c1\u30e3\u30fc\u30c8\u3068\u30b0\u30e9\u30d5\u3002 Queries: Dune \u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\u3057\u3001Dune \u306e\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306b\u30c6\u30fc\u30d6\u30eb\u3068\u30d3\u30b8\u30e5\u30a2\u30ea\u30bc\u30fc\u30b7\u30e7\u30f3\u3067\u8868\u793a\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u30b3\u30de\u30f3\u30c9\u3067\u3059\u3002 Dune.com \u306e\u8a2a\u554f\u8005\u306f\u3001\u30af\u30a8\u30ea\u304b\u3089\u69cb\u7bc9\u3055\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u3001\u30c6\u30fc\u30d6\u30eb\u3001\u8996\u899a\u5316\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3092\u542b\u3080\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3092\u95b2\u89a7\u3057\u307e\u3059\u3002 Dune Wizard\uff08\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30a2\u30ca\u30ea\u30b9\u30c8\uff09\u306f\u3001\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e\u30ab\u30b9\u30bf\u30e0\u30af\u30a8\u30ea\u3092\u4f5c\u6210\u3057\u3001\u305d\u306e\u7d50\u679c\u3092\u53ef\u8996\u5316\u3057\u3001\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u305f\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u8a9e\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 Queries \u00b6 Dune\u306f\u3001\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u3092SQL\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u96c6\u7d04\u3057\u3001\u7c21\u5358\u306b\u554f\u3044\u5408\u308f\u305b\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002 Queries \u306f\u3001\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u304b\u3089\u3069\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u3092\u6211\u3005\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3067\u898b\u3064\u3051\u3066\u8fd4\u3059\u304b\u3092\u6307\u5b9a\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u3082\u3057\u304b\u3057\u305f\u3089\u3001 all the Dex trades that happened today \u3084 total value of stablecoins minted this year \u3092\u77e5\u308a\u305f\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3069\u3093\u306a\u8cea\u554f\u3067\u3082\u3001\u7b54\u3048\u306e\u767a\u898b\u306f\u30c7\u30e5\u30fc\u30f3\u30af\u30a8\u30ea\u304b\u3089\u59cb\u307e\u308a\u307e\u3059! \u30af\u30a8\u30ea\u30fc\u306f\u3001\u5f93\u6765\u306eSQL\u30af\u30a8\u30ea\u30fc\u3068\u540c\u69d8\u306b\u30c7\u30fc\u30bf\u306e\u884c\u3068\u5217\u3092\u8fd4\u3057\u3001\u305d\u308c\u3092\u4f7f\u3063\u3066\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306b\u8868\u793a\u3059\u308b\u30d3\u30b8\u30e5\u30a2\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30a2\u30ca\u30ea\u30b9\u30c8\uff08\u30a6\u30a3\u30b6\u30fc\u30c9\u3059\u306a\u308f\u3061\u3042\u306a\u305f\uff01\uff09\u304cQueries\u306e\u5b9f\u884c\u3092\u958b\u59cb\u3059\u308b\u306b\u306f\u3001\u3044\u304f\u3064\u304b\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002 \u6700\u3082\u7c21\u5358\u306a\u65b9\u6cd5\u306f\u3001Dune Spells ) \u3092\u4f7f\u3063\u3066\u3001\u3088\u304f\u4f7f\u308f\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30c6\u30fc\u30d6\u30eb\u3092\u554f\u3044\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3059\u3002\u3088\u304f\u4f7f\u308f\u308c\u308b\u30b9\u30da\u30eb\u306b\u306f\u3001 dex.trades \u3001 lending.borrow \u3001 stablecoin.transfer \u304c\u3042\u308a\u307e\u3059\u3002 \u30d6\u30ed\u30c3\u30af\u3001\u30ed\u30b0\u3001\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u306e\u3088\u3046\u306a\u751f\u306eEthereum\u30c7\u30fc\u30bf\u3092\u7167\u4f1a\u3057\u307e\u3059\u3002 \u307e\u305f\u3001\u4e2d\u592e\u96c6\u6a29\u7684\u306a\u53d6\u5f15\u6240\u30c7\u30fc\u30bf\u3092\u7167\u4f1a\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001 prices.usd \u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u307b\u3068\u3093\u3069\u3059\u3079\u3066\u306e\u6697\u53f7\u8cc7\u7523\u306e\u4fa1\u683c\u3092\u8fc5\u901f\u306b\u8fd4\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 Visualizations \u00b6 \u8868\u5f62\u5f0f\uff08\u884c\u3068\u5217\uff09\u3067\u8868\u793a\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306f\u3001\u8aad\u307f\u306b\u304f\u3044\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002 Visualizations \u306f\u3001Query \u306e\u7d50\u679c\u3092\u53d6\u308a\u8fbc\u307f\u3001\u60c5\u5831\u3092\u660e\u78ba\u3001\u6b63\u78ba\u3001\u304b\u3064 visual \u306a\u65b9\u6cd5\u3067\u63d0\u793a\u3057\u307e\u3059\u3002 Dune Visualizations\u3092\u4f7f\u3048\u3070\u3001\u3053\u3093\u306a\u98a8\u306b\u5909\u5f62\u3055\u305b\u308b\u3053\u3068\u3067\u3001\u7c21\u5358\u306b\u30c7\u30fc\u30bf\u306e\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u8a9e\u308a\u59cb\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u3002 \u3053\u306e\u3088\u3046\u306a\u3082\u306e\u306b\u3002 \u30d0\u30fc\u30c1\u30e3\u30fc\u30c8\u3067\u53ef\u8996\u5316\u3059\u308b\u3068\u30014\u670819\u65e5\u306e\u8ee2\u9001\u91cf\u304c\u6700\u3082\u591a\u304b\u3063\u305f\u3053\u3068\u304c\u308f\u304b\u308a\u3001\u6642\u7cfb\u5217\u3067\u30c8\u30ec\u30f3\u30c9\u3092\u628a\u63e1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 Dune\u306f\u3001\u30c7\u30fc\u30bf\u3092\u8996\u899a\u7684\u306b\u8868\u73fe\u3059\u308b\u305f\u3081\u306b\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u69d8\u3005\u306aVisualization\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002 Bar Charts Area Charts Scatter Charts Line Charts Pie Charts Counters Tables Dashboards \u00b6 \u7dbf\u5bc6\u306b\u8a08\u753b\u3055\u308c\u305f\u30d3\u30b8\u30e5\u30a2\u30eb\u3092\u4f7f\u3044\u3001\u8ce2\u3044\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30a2\u30ca\u30ea\u30b9\u30c8\uff08\u30a6\u30a3\u30b6\u30fc\u30c9\uff01\uff09\u306f\u3001 Dune Dashboards \u3092\u901a\u3058\u3066\u3055\u307e\u3056\u307e\u306a\u30c7\u30fc\u30bf\u306e\u96c6\u307e\u308a\u306b\u3064\u3044\u3066\u7269\u8a9e\u3092\u8a9e\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4f8b\u3048\u3070\u3001\u4e0b\u306e\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3001 Dex Metrics by @hagaetc \u3067\u306f\u3001\u30ab\u30c6\u30b4\u30ea\u30fc\u3068\u3057\u3066\u306e\u300cDEX\u300d\u304c\u6210\u9577\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u4e0a\u90e8\u306b\u306f\u3063\u304d\u308a\u3068\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u4e0b\u306b\u306f\u3001\u3069\u306eDEX\u304c\u51fa\u6765\u9ad8\u3067\u6700\u3082\u4eba\u6c17\u304c\u3042\u308b\u304b\u304c\u8868\u793a\u3055\u308c\u3001\u6700\u5f8c\u306b\u6642\u9593\u7684\u306a\u5909\u5316\u3092\u793a\u3059\u7a4d\u307f\u4e0a\u3052\u68d2\u30b0\u30e9\u30d5\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3053\u306e\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u4e00\u3064\u3092\u898b\u308b\u3060\u3051\u3067\u3001\u8ab0\u3067\u3082DEX\u5e02\u5834\u5168\u4f53\u3092\u628a\u63e1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u3002 How to navigate these docs \u00b6 \u79c1\u305f\u3061\u306f\u3001\u300c\u30c7\u30e5\u30fc\u30f3\u300d\u306e\u3059\u3079\u3066\u306b\u3064\u3044\u3066\u3001\u8ab0\u304c\u3001\u4f55\u3092\u3001\u3044\u3064\u3001\u3069\u3053\u3067\u3001\u306a\u305c\u3001\u3069\u306e\u3088\u3046\u306b\u3001\u3068\u3044\u3046\u8cea\u554f\u306b\u7b54\u3048\u308b\u305f\u3081\u306b\u3001\u3053\u308c\u3089\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u4f5c\u308a\u307e\u3057\u305f\u3002 \u3053\u3053\u3067\u306f\u3001\u5404\u30bb\u30af\u30b7\u30e7\u30f3\u306e\u5185\u5bb9\u3092\u7c21\u5358\u306b\u3054\u7d39\u4ecb\u3057\u307e\u3059\u3002 Getting Started \u306f\u3001Dune\u306e\u4f7f\u3044\u65b9\u3092\u5b66\u3076\u305f\u3081\u306e\u5834\u6240\u3067\u3059\u3002 Reference \u3067\u306f\u3001\u300c\u8ab0\u304c\u3001\u4f55\u3092\u3001\u3069\u3053\u3067\u300d\u3068\u3044\u3046\u8cea\u554f\u306b\u5bfe\u3059\u308b\u7b54\u3048\u3084\u3001\u79c1\u305f\u3061\u304c\u307e\u3068\u3081\u305f\u3044\u304f\u3064\u304b\u306e\u88dc\u8db3\u8cc7\u6599\u3092\u3054\u89a7\u3044\u305f\u3060\u3051\u307e\u3059\u3002 Spellbook \u306b\u306f\u3001\u30b9\u30da\u30eb\u306e\u4f5c\u6210\u3068\u4f7f\u7528\u306b\u5fc5\u8981\u306a\u3082\u306e\u304c\u3059\u3079\u3066\u63c3\u3063\u3066\u3044\u307e\u3059\u3002 API \u306f\u3001\u79c1\u305f\u3061\u306e API \u3092\u3042\u306a\u305f\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u7d71\u5408\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u3059\u3079\u3066\u306e\u3082\u306e\u3092\u898b\u3064\u3051\u308b\u5834\u6240\u3067\u3059\u3002 \u3082\u3057\u3001\u3042\u306a\u305f\u304c\u6756\u3092\u4f7f\u3044\u305f\u304f\u3066\u3046\u305a\u3046\u305a\u3057\u3066\u3044\u308b\u306a\u3089\u3001 Query Quick Start \u306b\u98db\u3093\u3067\u3001\u6700\u521d\u306eDune Query\u3092\u4f5c\u308b\u305f\u3081\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3092\u898b\u3066\u304f\u3060\u3055\u3044\u3002 Dune is a community effort \u00b6 Dune.com \u3067\u306f\u3001\u3059\u3079\u3066\u306e\u30af\u30a8\u30ea\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u516c\u958b\u3055\u308c\u3066\u3044\u307e\u3059\uff08\u30af\u30a8\u30ea\u306b\u30d7\u30e9\u30a4\u30d0\u30b7\u30fc\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f\u3001 Pro Plan \u3067\u5bfe\u5fdc\u53ef\u80fd\u3067\u3059\uff09\u3002 \u3053\u308c\u306b\u3088\u308a\u3001Wizard\u306f\u4ed6\u306e\u30af\u30ea\u30a8\u30a4\u30bf\u30fc\u306eQuery\u3092\u30d5\u30a9\u30fc\u30af\u3001\u30ea\u30df\u30c3\u30af\u30b9\u3057\u3066\u3001\u5f7c\u3089\u306e\u77e5\u8b58\u3084\u6d1e\u5bdf\u529b\u306e\u4e0a\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002 \u9006\u306b\u3001\u65b0\u3057\u3044Query\u3092\u4f5c\u6210\u3059\u308b\u305f\u3073\u306b\u3001Dune\u3092\u901a\u3058\u3066\u4ed6\u306e\u4eba\u304c\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u3084\u6697\u53f7\u8cc7\u7523\u306b\u3064\u3044\u3066\u65b0\u3057\u3044\u3053\u3068\u3092\u5b66\u3076\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002 \u3053\u306e\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u30eb\u30fc\u30d7\u306b\u3088\u3063\u3066\u3001\u30c7\u30e5\u30fc\u30f3\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306f\u3001\u79c1\u305f\u3061\u5168\u54e1\u304c\u3088\u308a\u591a\u304f\u3092\u5b66\u3076\u3053\u3068\u304c\u3067\u304d\u308b\u3001\u5897\u3048\u7d9a\u3051\u308b\u30af\u30a8\u30ea\u306e\u7bc4\u56f2\u3092\u901a\u3057\u3066\u3001\u5171\u306b\u6210\u529f\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u3002 \u79c1\u305f\u3061\u306e Community Discord \u306b\u53c2\u52a0\u3059\u308c\u3070\u3001\u79c1\u305f\u3061\u306e\u30c1\u30fc\u30e0\u3068\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u304b\u3089\u4e16\u754c\u30ec\u30d9\u30eb\u306e\u30b5\u30dd\u30fc\u30c8\u3092\u53d7\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u697d\u3057\u3044LIVE\u306b\u53c2\u52a0\u3059\u308b\u305f\u3081\u306b\u3001 events calendar \u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002 \u307e\u305f\u3001\u6a5f\u80fd\u8981\u671b\u3084\u30d0\u30b0\u30ec\u30dd\u30fc\u30c8\u306a\u3069\u3001\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u3042\u308c\u3070\u3001 here \u306b\u63d0\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002","title":"Welcome"},{"location":"#dune-in-5-minutes","text":"","title":"Dune in 5-minutes \u26a1"},{"location":"#how-the-data-flows","text":"\u30d1\u30d6\u30ea\u30c3\u30af\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306f open and free \u306a\u306e\u3067\u3001\u305d\u3053\u304b\u3089\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u306e\u306f\u305d\u3093\u306a\u306b\u96e3\u3057\u304f\u306a\u3044\u306f\u305a\u3067\u3059\u3088\u306d\uff1f \u30a4\u30a8\u30b9\u3067\u3082\u3042\u308a\u3001\u30ce\u30fc\u3067\u3082\u3042\u308b\u3002 \u4f8b\u3048\u3070\u3001\u56fd\u969b\u8f38\u9001\u306e\u30b9\u30d4\u30fc\u30c9\u304c\u30d1\u30ea\u306e\u6700\u65b0\u30af\u30c1\u30e5\u30fc\u30eb\u306e\u6d88\u8cbb\u8005\u9700\u8981\u306b\u3069\u306e\u3088\u3046\u306b\u5f71\u97ff\u3059\u308b\u304b\u3092\u5206\u6790\u3059\u308b\u305f\u3081\u306b\u3001\u5f93\u6765\u306e\u30d3\u30b8\u30cd\u30b9\u304b\u3089\u5b64\u7acb\u3057\u305f\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u306b\u6bd4\u3079\u308c\u3070...\u3002 \u305d\u3046\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u3092\u95b2\u89a7\u3001\u5206\u6790\u3059\u308b\u3053\u3068\u306f \"\u7c21\u5358 \"\u3067\u3059\u3002 \u3057\u304b\u3057\u3001Dune\u306e\u3053\u3068\u308f\u3056\u306e\u30d5\u30fc\u30c9\u306e\u4e0b\u306b\u306f\u591a\u304f\u306e\u3053\u3068\u304c\u8d77\u3053\u3063\u3066\u3044\u307e\u3059\u3002\u30a4\u30fc\u30b5\u30ea\u30a2\u30e0\u306e\u3088\u3046\u306a\u30d1\u30d6\u30ea\u30c3\u30af\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u4e0a\u306e\u72b6\u614b\u306e\u5909\u5316\u304c\u3001\u3069\u306e\u3088\u3046\u306b\u3057\u3066\u30c1\u30e3\u30fc\u30c8\u3084\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3059\u308b\u305f\u3081\u306b\u30af\u30a8\u30ea\u3059\u308b\u30c7\u30fc\u30bf\u306b\u5909\u308f\u308b\u306e\u304b\u3092\u3088\u308a\u3088\u304f\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u3001\u30d5\u30fc\u30c9\u3092\u958b\u3051\u3066\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002","title":"How the data flows"},{"location":"#1-a-chain-adds-a-block","text":"\u6280\u8853\u7684\u306a\u8a73\u7d30\u306f\u3055\u307e\u3056\u307e\u3067\u3059\u304c\u3001\u3059\u3079\u3066\u306e\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u4e2d\u6838\u306b\u306f\u3001\u4e00\u9023\u306e\u53d6\u5f15\u304c\u63d0\u6848\u3055\u308c\u3001\u5408\u610f\u3055\u308c\u305f\u5f8c\u3001\u4ee5\u524d\u306b\u5408\u610f\u3055\u308c\u305f\u53d6\u5f15\u3092\u542b\u3080\u30d6\u30ed\u30c3\u30af\u306e\u30c1\u30a7\u30fc\u30f3\u306e\u672b\u5c3e\u306b\u8ffd\u52a0\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 \u3069\u306e\u30d6\u30ed\u30c3\u30af\u3092\u6b21\u306e\u30d6\u30ed\u30c3\u30af\u3068\u3059\u308b\u304b\u306f\u3001\u69d8\u3005\u306a consensus mechanisms \u3001\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u30b3\u30f3\u30bb\u30f3\u30b5\u30b9\u304c\u5f97\u3089\u308c\u308b\u3068\u3001\u6700\u65b0\u306e\u30d6\u30ed\u30c3\u30af\u306b\u95a2\u3059\u308b\u60c5\u5831\u304c\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u30d6\u30ed\u30fc\u30c9\u30ad\u30e3\u30b9\u30c8\u3055\u308c\u3001\u53c2\u52a0\u8005\uff08\u300c\u30ce\u30fc\u30c9\u300d\uff09\u306b\u3053\u306e\u65b0\u3057\u3044\u30d6\u30ed\u30c3\u30af\u3092\u77e5\u3089\u305b\u3001\u81ea\u5206\u306e\u8a18\u9332\u306b\u8ffd\u52a0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u4ed5\u7d44\u307f\u306b\u3064\u3044\u3066\u8a73\u3057\u304f\u77e5\u308a\u305f\u3044\u65b9\u306f Check out this awesome Blockchain 101 demo !","title":"  1. A chain adds a block"},{"location":"#2-node-providers-transmit-data-to-dune","text":"\u3053\u306e\u300c\u65b0\u3057\u3044\u30d6\u30ed\u30c3\u30af\u304c\u4f5c\u6210\u3055\u308c\u305f\u300d\u3068\u3044\u3046\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u53d7\u3051\u53d6\u308b\u306b\u306f\u3001\u8ab0\u304b\u304c blockchain node \u3092\u5b9f\u884c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 blockchain node \u306f\u3001\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u63a5\u7d9a\u3057\u3001\u4ed6\u306e\u30ce\u30fc\u30c9\u9593\u3067\u60c5\u5831\u3092\u9001\u3063\u305f\u308a\u3001\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u53d6\u5f15\u306e\u691c\u8a3c\u3084\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3092\u53ef\u80fd\u306b\u3059\u308b\u300c\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u300d\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u3092\u5b9f\u884c\u3059\u308b\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3067\u3059\u3002 \u3061\u3087\u3063\u3068\u3057\u305f\u6280\u8853\u7684\u306a\u30ce\u30a6\u30cf\u30a6\u304c\u3042\u308c\u3070\u3001\u8ab0\u3067\u3082\u30ce\u30fc\u30c9\u3092\u52d5\u304b\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u305d\u308c\u304c\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3 public \u306e\u5927\u304d\u306a\u7279\u5fb4\u3067\u3059 \u307b\u307c\u5168\u54e1\u304c\u30ce\u30fc\u30c9\u3092\u904b\u55b6\u3067\u304d\u308b\u305f\u3081\u3001\u53c2\u52a0\u8005\u306e\u8aa0\u5b9f\u3055\u3092\u4fdd\u3064\u305f\u3081\u306b\u3001\u30b7\u30b9\u30c6\u30e0\u306b\u306f\u591a\u304f\u306e\u900f\u660e\u6027\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u900f\u660e\u6027\u306b\u3088\u308a\u3001\u30c7\u30fc\u30bf\u30a2\u30ca\u30ea\u30b9\u30c8\u304c\u300c\u4f55\u304c\u8d77\u3053\u3063\u3066\u3044\u308b\u304b\u300d\u306e\u5168\u4f53\u50cf\u306b\u30a2\u30af\u30bb\u30b9\u3057\u3001\u3042\u3089\u3086\u308b\u7a2e\u985e\u306e\u5206\u6790\u3092\u884c\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001\u30cf\u30a4\u30d6\u30de\u30a4\u30f3\u30c9\u3092\u6d3b\u7528\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u306b\u306a\u308a\u307e\u3059\u3002 \u751f\u30c7\u30fc\u30bf\u306b\u30a2\u30af\u30bb\u30b9\u3059\u308b\u305f\u3081\u306e\u82e6\u52b4\u306f\u5fc5\u8981\u306a\u3044\u3002 Dune\u306e\u3088\u3046\u306a\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u304c\u5927\u898f\u6a21\u306b\u904b\u7528\u3067\u304d\u308b\u3088\u3046\u306b\u3001 node providers \u306f\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u30ce\u30fc\u30c9\u30a4\u30f3\u30d5\u30e9\u3092\u69cb\u7bc9\u30fb\u904b\u7528\u3057\u3001\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u30a4\u30f3\u30bf\u30fc\u30d5\u30a7\u30fc\u30b9\uff08API\uff09\u3092\u901a\u3058\u3066\u79c1\u305f\u3061\u304c\u30a2\u30af\u30bb\u30b9\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002 \u3053\u3046\u3059\u308b\u3053\u3068\u3067\u3001\u79c1\u305f\u3061\u306f\u53ef\u80fd\u306a\u9650\u308a\u6700\u9ad8\u306e\u30c7\u30fc\u30bf\u30a2\u30af\u30bb\u30b9\u4f53\u9a13\u3092\u63d0\u4f9b\u3059\u308b\u3053\u3068\u306b\u96c6\u4e2d\u3067\u304d\u3001\u30ce\u30fc\u30c9\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306f\u53ef\u80fd\u306a\u9650\u308a\u52b9\u7387\u7684\u306b\u30ce\u30fc\u30c9\u3092\u7a3c\u50cd\u3055\u305b\u308b\u3053\u3068\u306b\u96c6\u4e2d\u3067\u304d\u308b\u306e\u3067\u3059\u3002","title":" 2. Node providers transmit data to Dune"},{"location":"#3-dune-adds-raw-data-to-sql-tables","text":"\u30ce\u30fc\u30c9\u30d7\u30ed\u30d0\u30a4\u30c0\u30fc\u306f\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u53d6\u5f15\u30c7\u30fc\u30bf\u3092\u30cf\u30c3\u30b7\u30e5\u5316\u3055\u308c\u305f\u30d0\u30a4\u30c8\u30b3\u30fc\u30c9\u3068\u3057\u3066\u9001\u3063\u3066\u304d\u307e\u3059\uff08\u4f8b\u3048\u3070\u3001\u30a4\u30fc\u30b5\u30ea\u30a2\u30e0\u306e\u30c7\u30fc\u30bf\u306f the keccak256 algorithm \u3067\u30cf\u30c3\u30b7\u30e5\u5316\u3055\u308c\u3066\u3044\u307e\u3059\uff09\u3002 Dune Data Engine\u306f\u3053\u306e\u30d0\u30a4\u30c8\u30b3\u30fc\u30c9\u3092\u53d6\u308a\u51fa\u3057\u3001\" Raw Data \"\u3068\u547c\u3076\u30c6\u30fc\u30d6\u30eb\u7fa4\u306b\u62bd\u51fa\u3057\u307e\u3059\u3002 \u3053\u308c\u3089\u306f\u30c1\u30a7\u30fc\u30f3\u306b\u3088\u3063\u3066\u591a\u5c11\u7570\u306a\u308a\u307e\u3059\u304c\u3001\u4e00\u4f8b\u3068\u3057\u3066\u3001\u307b\u3068\u3093\u3069\u306e Ethereum Virtual Machine (EVM) \u30d9\u30fc\u30b9\u306e\u30c1\u30a7\u30fc\u30f3\u3067\u306f\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u3082\u306e\u304c\u3042\u308a\u307e\u3059\u3002 [chain].blocks - \u30c1\u30a7\u30fc\u30f3\u306b\u8ffd\u52a0\u3055\u308c\u305f\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u306e\u30b0\u30eb\u30fc\u30d7\u3002 [chain].creation_traces - create traces \u3092\u542b\u3080\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\uff08\u5185\u90e8\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u306b\u542b\u307e\u308c\u308b\u3053\u3068\u3082\u3042\u308b\uff09 \u30b9\u30de\u30fc\u30c8\u30b3\u30f3\u30c8\u30e9\u30af\u30c8\u3067\u4f5c\u6210\u3055\u308c\u305f [chain].logs \uff5e event logs \u30d6\u30ed\u30c3\u30af\u5185\u306e\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u3067\u767a\u751f\u3059\u308b [chain].traces \uff5e trace data [chain].transactions - \u3042\u308b\u30a2\u30c9\u30ec\u30b9\u304b\u3089\u5225\u306e\u30a2\u30c9\u30ec\u30b9\u306b\u9001\u4fe1\u3055\u308c\u308b\u6697\u53f7\u5316\u3055\u308c\u305f\u7f72\u540d\u4ed8\u304d\u547d\u4ee4\u3002 \u3053\u308c\u3089\u306e\u30c6\u30fc\u30d6\u30eb\u306e\u30c7\u30fc\u30bf\u306f\u4eba\u9593\u304c\u8aad\u3080\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u304c\uff08\u30d0\u30a4\u30c8\u30b3\u30fc\u30c9\u306f\u305d\u3046\u3067\u306f\u3042\u308a\u307e\u305b\u3093\uff09\u3001\u7406\u89e3\u3057\u89e3\u91c8\u3059\u308b\u306b\u306f\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306b\u95a2\u3059\u308b\u5e45\u5e83\u3044\u77e5\u8b58\u304c\u5fc5\u8981\u3067\u3059\u3002 \u3053\u306eRaw Data\u306f\u3001\u8208\u5473\u6df1\u3044\u30a4\u30f3\u30b5\u30a4\u30c8\u306b\u64cd\u4f5c\u3059\u308b\u306e\u306b\u591a\u304f\u306e\u624b\u9593\u304c\u304b\u304b\u308b\u3053\u3068\u3082\u3042\u308a\u3001Dune decodes \u306f\u3053\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u7528\u3057\u3066\u3044\u307e\u3059\u3002","title":" 3. Dune adds raw data to SQL tables"},{"location":"#4-dune-decodes-raw-data","text":"\u751f\u306e .log \u30c6\u30fc\u30d6\u30eb\u306f\u3001\u6b21\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u3092\u8fd4\u3057\u307e\u3059\u3002 \u3053\u306e\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u306f\u3001\u304b\u306a\u308a\u9650\u5b9a\u7684\u306a\u30c7\u30fc\u30bf\u5206\u6790\u306e\u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u304c\u3042\u308b\u3002 \u3053\u306e\u30c7\u30fc\u30bf\u3092\u3088\u308a\u4f7f\u3044\u3084\u3059\u3044\u3082\u306e\u306b\u5909\u63db\u3059\u308b\u305f\u3081\u306b\u3001 Wizards submit smart contracts for decoding here . \u305d\u306e\u969b\u3001\u30b9\u30de\u30fc\u30c8\u30b3\u30f3\u30c8\u30e9\u30af\u30c8\u306e Application Binary Interface (ABI) \u3068\u3044\u3046Web2.0\u306eAPI\u306b\u4f3c\u305f\u3082\u306e\u3092\u4f7f\u3063\u3066\u3001\u30b3\u30f3\u30c8\u30e9\u30af\u30c8\u3068\u3084\u308a\u53d6\u308a\u3059\u308b\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u306e\u4e2d\u3067\u4f55\u304c\u8d77\u3053\u3063\u3066\u3044\u308b\u304b\u3092\u7406\u89e3\u3057\u307e\u3059\u3002 \u305d\u3057\u3066\u3001\u5206\u6790\u304c\u3057\u3084\u3059\u3044 Decoded Tables \u3092\u4f5c\u308b\u306e\u3067\u3059\u3002 \u4f8b\u3048\u3070\u3001\u4e0a\u8a18\u306e\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u3092\u30c7\u30b3\u30fc\u30c9\u3059\u308b\u3068\u3001\u6b21\u306e\u3088\u3046\u306b\u306a\u308a\u307e\u3059\u3002","title":" 4. Dune decodes raw data"},{"location":"#5-the-dune-community-casts-spells","text":"Dune\u306f\u3001Wizards\u3068\u3044\u3046\u7d20\u6674\u3089\u3057\u3044\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306e\u529b\u3092\u501f\u308a\u3066\u3001 Spells \u3067\u30c7\u30fc\u30bf\u3092\u30c7\u30b3\u30fc\u30c9\u3059\u308b\u3060\u3051\u3067\u306a\u304f\u3001\u3055\u3089\u306b\u4e00\u6b69\u8e0f\u307f\u8fbc\u307f\u307e\u3057\u305f\u3002 \u30b9\u30da\u30eb\u306f\u3001Dune\u3068\u6211\u3005\u306e\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306b\u3088\u3063\u3066\u69cb\u7bc9\u30fb\u7dad\u6301\u3055\u308c\u3066\u3044\u308b\u30ab\u30b9\u30bf\u30e0\u30c6\u30fc\u30d6\u30eb\u3067\u3001\u3067\u304d\u308b\u3060\u3051\u6469\u64e6\u3092\u5c11\u306a\u304f\u3057\u3066\u591a\u304f\u306e\u30c7\u30fc\u30bf\u3092\u7c21\u5358\u306b\u96c6\u8a08\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002 \u4f8b\u3048\u3070\u6700\u3082\u4eba\u6c17\u306e\u3042\u308b\u30b9\u30da\u30eb\u306e1\u3064\u3067\u3042\u308b nft.trades \u306f\u3001Solana\u4e0a\u306eMagic Eden\u3084Ethereum\u4e0a\u306eLooksRare\u306a\u3069\u306e\u53d6\u5f15\u3092\u81ea\u5206\u3067\u30b3\u30f3\u30d1\u30a4\u30eb\u3057\u306a\u304f\u3066\u3082\u3001\u30d7\u30ed\u30c8\u30b3\u30eb\u3084\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u9593\u3067NFT\u53d6\u5f15\u30c7\u30fc\u30bf\u306e\u63a2\u7d22\u3068\u5909\u63db\u3092\u7c21\u5358\u306b\u884c\u3048\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002","title":" 5. The Dune community casts Spells"},{"location":"#6-dune-wizards-make-magic","text":"\u3053\u308c\u3089\u306e\u30c7\u30fc\u30bf\u304b\u3089\u3001\u30a6\u30a3\u30b6\u30fc\u30c9\u306f\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3078\u306e\u30c7\u30fc\u30bf\u306e\u4fdd\u5b58\u3001\u64cd\u4f5c\u3001\u691c\u7d22\u306b\u5e83\u304f\u4f7f\u308f\u308c\u3066\u3044\u308b\u8a00\u8a9e\u3067\u3042\u308bSQL\u3092\u4f7f\u3063\u3066 Queries \u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002 \u3053\u306e\u30af\u30a8\u30ea\u304b\u3089\u3001\u79c1\u305f\u3061\u304c\u3088\u304f\u77e5\u3063\u3066\u3044\u308b Visualizations \u3068 Dashboards \u304c\u4f5c\u3089\u308c\u308b\u306e\u3067\u3059\u3002 Eg @rchen8 \u306eOpenSea\u306e\u65e5\u6b21\u30dc\u30ea\u30e5\u30fc\u30e0\u3002","title":" 6. Dune Wizards make magic"},{"location":"#making-with-dunecom","text":"Dune.com\u306f\u3001Dune Data Platform\u306e\u4e0a\u306b\u69cb\u7bc9\u3055\u308c\u305f\u6700\u521d\u306e\u30ad\u30e9\u30fc\u30a2\u30d7\u30ea\u3067\u3001SQL\u3001Ethereum Virtual Machine\u3001\u30d3\u30b8\u30cd\u30b9\u306e\u77e5\u8b58\u304c\u5c11\u3057\u3067\u3082\u3042\u308c\u3070\u3001\u8ab0\u3067\u3082\u3067\u304d\u308b\u3060\u3051\u7c21\u5358\u306b\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30c7\u30fc\u30bf\u3092\u8208\u5473\u6df1\u3044\u65b9\u6cd5\u3067\u5206\u6790\u3067\u304d\u308b\u3088\u3046\u306b\u8a2d\u8a08\u3055\u308c\u3066\u3044\u307e\u3059\u3002 Dune.com\u30a2\u30d7\u30ea\u306e\u57fa\u672c\u7684\u306a\u69cb\u6210\u8981\u7d20\u306f\u4ee5\u4e0b\u306e\u3068\u304a\u308a\u3067\u3059\u3002 Dashboards: \u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30c7\u30fc\u30bf\u306e\u7279\u5b9a\u306e\u30b0\u30eb\u30fc\u30d7\u306b\u3064\u3044\u3066\u306e\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u4f1d\u3048\u308b\u30d3\u30b8\u30e5\u30a2\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3\u3068\u30c6\u30ad\u30b9\u30c8\u3092\u542b\u3080\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u306e\u30bb\u30c3\u30c8\u3067\u3059\u3002 Visualizations: \u8868\u5f62\u5f0f\u3067\u308f\u304b\u308a\u306b\u304f\u3044\u30c7\u30fc\u30bf\u3092\u3001\u8996\u899a\u7684\u306b\u308f\u304b\u308a\u3084\u3059\u304f\u3059\u308b\u30c1\u30e3\u30fc\u30c8\u3068\u30b0\u30e9\u30d5\u3002 Queries: Dune \u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u304b\u3089\u30c7\u30fc\u30bf\u3092\u62bd\u51fa\u3057\u3001Dune \u306e\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306b\u30c6\u30fc\u30d6\u30eb\u3068\u30d3\u30b8\u30e5\u30a2\u30ea\u30bc\u30fc\u30b7\u30e7\u30f3\u3067\u8868\u793a\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b\u30b3\u30de\u30f3\u30c9\u3067\u3059\u3002 Dune.com \u306e\u8a2a\u554f\u8005\u306f\u3001\u30af\u30a8\u30ea\u304b\u3089\u69cb\u7bc9\u3055\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u3001\u30c6\u30fc\u30d6\u30eb\u3001\u8996\u899a\u5316\u30a6\u30a3\u30b8\u30a7\u30c3\u30c8\u3092\u542b\u3080\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3092\u95b2\u89a7\u3057\u307e\u3059\u3002 Dune Wizard\uff08\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30a2\u30ca\u30ea\u30b9\u30c8\uff09\u306f\u3001\u30c7\u30fc\u30bf\u3092\u53d6\u5f97\u3059\u308b\u305f\u3081\u306e\u30ab\u30b9\u30bf\u30e0\u30af\u30a8\u30ea\u3092\u4f5c\u6210\u3057\u3001\u305d\u306e\u7d50\u679c\u3092\u53ef\u8996\u5316\u3057\u3001\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3092\u4f7f\u7528\u3057\u3066\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u305f\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u8a9e\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002","title":"Making \ud83e\ude84 with dune.com"},{"location":"#queries","text":"Dune\u306f\u3001\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u306e\u30c7\u30fc\u30bf\u3092SQL\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u306b\u96c6\u7d04\u3057\u3001\u7c21\u5358\u306b\u554f\u3044\u5408\u308f\u305b\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3059\u3002 Queries \u306f\u3001\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u304b\u3089\u3069\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u3092\u6211\u3005\u306e\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3067\u898b\u3064\u3051\u3066\u8fd4\u3059\u304b\u3092\u6307\u5b9a\u3059\u308b\u305f\u3081\u306b\u4f7f\u7528\u3055\u308c\u307e\u3059\u3002 \u3082\u3057\u304b\u3057\u305f\u3089\u3001 all the Dex trades that happened today \u3084 total value of stablecoins minted this year \u3092\u77e5\u308a\u305f\u3044\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3069\u3093\u306a\u8cea\u554f\u3067\u3082\u3001\u7b54\u3048\u306e\u767a\u898b\u306f\u30c7\u30e5\u30fc\u30f3\u30af\u30a8\u30ea\u304b\u3089\u59cb\u307e\u308a\u307e\u3059! \u30af\u30a8\u30ea\u30fc\u306f\u3001\u5f93\u6765\u306eSQL\u30af\u30a8\u30ea\u30fc\u3068\u540c\u69d8\u306b\u30c7\u30fc\u30bf\u306e\u884c\u3068\u5217\u3092\u8fd4\u3057\u3001\u305d\u308c\u3092\u4f7f\u3063\u3066\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u306b\u8868\u793a\u3059\u308b\u30d3\u30b8\u30e5\u30a2\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30a2\u30ca\u30ea\u30b9\u30c8\uff08\u30a6\u30a3\u30b6\u30fc\u30c9\u3059\u306a\u308f\u3061\u3042\u306a\u305f\uff01\uff09\u304cQueries\u306e\u5b9f\u884c\u3092\u958b\u59cb\u3059\u308b\u306b\u306f\u3001\u3044\u304f\u3064\u304b\u306e\u65b9\u6cd5\u304c\u3042\u308a\u307e\u3059\u3002 \u6700\u3082\u7c21\u5358\u306a\u65b9\u6cd5\u306f\u3001Dune Spells ) \u3092\u4f7f\u3063\u3066\u3001\u3088\u304f\u4f7f\u308f\u308c\u308b\u30c7\u30fc\u30bf\u30fb\u30c6\u30fc\u30d6\u30eb\u3092\u554f\u3044\u5408\u308f\u305b\u308b\u3053\u3068\u3067\u3059\u3002\u3088\u304f\u4f7f\u308f\u308c\u308b\u30b9\u30da\u30eb\u306b\u306f\u3001 dex.trades \u3001 lending.borrow \u3001 stablecoin.transfer \u304c\u3042\u308a\u307e\u3059\u3002 \u30d6\u30ed\u30c3\u30af\u3001\u30ed\u30b0\u3001\u30c8\u30e9\u30f3\u30b6\u30af\u30b7\u30e7\u30f3\u306e\u3088\u3046\u306a\u751f\u306eEthereum\u30c7\u30fc\u30bf\u3092\u7167\u4f1a\u3057\u307e\u3059\u3002 \u307e\u305f\u3001\u4e2d\u592e\u96c6\u6a29\u7684\u306a\u53d6\u5f15\u6240\u30c7\u30fc\u30bf\u3092\u7167\u4f1a\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3067\u3059\u3002\u4f8b\u3048\u3070\u3001 prices.usd \u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u307b\u3068\u3093\u3069\u3059\u3079\u3066\u306e\u6697\u53f7\u8cc7\u7523\u306e\u4fa1\u683c\u3092\u8fc5\u901f\u306b\u8fd4\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002","title":"Queries"},{"location":"#visualizations","text":"\u8868\u5f62\u5f0f\uff08\u884c\u3068\u5217\uff09\u3067\u8868\u793a\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306f\u3001\u8aad\u307f\u306b\u304f\u3044\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002 Visualizations \u306f\u3001Query \u306e\u7d50\u679c\u3092\u53d6\u308a\u8fbc\u307f\u3001\u60c5\u5831\u3092\u660e\u78ba\u3001\u6b63\u78ba\u3001\u304b\u3064 visual \u306a\u65b9\u6cd5\u3067\u63d0\u793a\u3057\u307e\u3059\u3002 Dune Visualizations\u3092\u4f7f\u3048\u3070\u3001\u3053\u3093\u306a\u98a8\u306b\u5909\u5f62\u3055\u305b\u308b\u3053\u3068\u3067\u3001\u7c21\u5358\u306b\u30c7\u30fc\u30bf\u306e\u30b9\u30c8\u30fc\u30ea\u30fc\u3092\u8a9e\u308a\u59cb\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u3002 \u3053\u306e\u3088\u3046\u306a\u3082\u306e\u306b\u3002 \u30d0\u30fc\u30c1\u30e3\u30fc\u30c8\u3067\u53ef\u8996\u5316\u3059\u308b\u3068\u30014\u670819\u65e5\u306e\u8ee2\u9001\u91cf\u304c\u6700\u3082\u591a\u304b\u3063\u305f\u3053\u3068\u304c\u308f\u304b\u308a\u3001\u6642\u7cfb\u5217\u3067\u30c8\u30ec\u30f3\u30c9\u3092\u628a\u63e1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 Dune\u306f\u3001\u30c7\u30fc\u30bf\u3092\u8996\u899a\u7684\u306b\u8868\u73fe\u3059\u308b\u305f\u3081\u306b\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u69d8\u3005\u306aVisualization\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002 Bar Charts Area Charts Scatter Charts Line Charts Pie Charts Counters Tables","title":"Visualizations"},{"location":"#dashboards","text":"\u7dbf\u5bc6\u306b\u8a08\u753b\u3055\u308c\u305f\u30d3\u30b8\u30e5\u30a2\u30eb\u3092\u4f7f\u3044\u3001\u8ce2\u3044\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u30a2\u30ca\u30ea\u30b9\u30c8\uff08\u30a6\u30a3\u30b6\u30fc\u30c9\uff01\uff09\u306f\u3001 Dune Dashboards \u3092\u901a\u3058\u3066\u3055\u307e\u3056\u307e\u306a\u30c7\u30fc\u30bf\u306e\u96c6\u307e\u308a\u306b\u3064\u3044\u3066\u7269\u8a9e\u3092\u8a9e\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4f8b\u3048\u3070\u3001\u4e0b\u306e\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u3001 Dex Metrics by @hagaetc \u3067\u306f\u3001\u30ab\u30c6\u30b4\u30ea\u30fc\u3068\u3057\u3066\u306e\u300cDEX\u300d\u304c\u6210\u9577\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u4e0a\u90e8\u306b\u306f\u3063\u304d\u308a\u3068\u793a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u305d\u306e\u4e0b\u306b\u306f\u3001\u3069\u306eDEX\u304c\u51fa\u6765\u9ad8\u3067\u6700\u3082\u4eba\u6c17\u304c\u3042\u308b\u304b\u304c\u8868\u793a\u3055\u308c\u3001\u6700\u5f8c\u306b\u6642\u9593\u7684\u306a\u5909\u5316\u3092\u793a\u3059\u7a4d\u307f\u4e0a\u3052\u68d2\u30b0\u30e9\u30d5\u3092\u898b\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3053\u306e\u30c0\u30c3\u30b7\u30e5\u30dc\u30fc\u30c9\u4e00\u3064\u3092\u898b\u308b\u3060\u3051\u3067\u3001\u8ab0\u3067\u3082DEX\u5e02\u5834\u5168\u4f53\u3092\u628a\u63e1\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u3002","title":"Dashboards"},{"location":"#how-to-navigate-these-docs","text":"\u79c1\u305f\u3061\u306f\u3001\u300c\u30c7\u30e5\u30fc\u30f3\u300d\u306e\u3059\u3079\u3066\u306b\u3064\u3044\u3066\u3001\u8ab0\u304c\u3001\u4f55\u3092\u3001\u3044\u3064\u3001\u3069\u3053\u3067\u3001\u306a\u305c\u3001\u3069\u306e\u3088\u3046\u306b\u3001\u3068\u3044\u3046\u8cea\u554f\u306b\u7b54\u3048\u308b\u305f\u3081\u306b\u3001\u3053\u308c\u3089\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u4f5c\u308a\u307e\u3057\u305f\u3002 \u3053\u3053\u3067\u306f\u3001\u5404\u30bb\u30af\u30b7\u30e7\u30f3\u306e\u5185\u5bb9\u3092\u7c21\u5358\u306b\u3054\u7d39\u4ecb\u3057\u307e\u3059\u3002 Getting Started \u306f\u3001Dune\u306e\u4f7f\u3044\u65b9\u3092\u5b66\u3076\u305f\u3081\u306e\u5834\u6240\u3067\u3059\u3002 Reference \u3067\u306f\u3001\u300c\u8ab0\u304c\u3001\u4f55\u3092\u3001\u3069\u3053\u3067\u300d\u3068\u3044\u3046\u8cea\u554f\u306b\u5bfe\u3059\u308b\u7b54\u3048\u3084\u3001\u79c1\u305f\u3061\u304c\u307e\u3068\u3081\u305f\u3044\u304f\u3064\u304b\u306e\u88dc\u8db3\u8cc7\u6599\u3092\u3054\u89a7\u3044\u305f\u3060\u3051\u307e\u3059\u3002 Spellbook \u306b\u306f\u3001\u30b9\u30da\u30eb\u306e\u4f5c\u6210\u3068\u4f7f\u7528\u306b\u5fc5\u8981\u306a\u3082\u306e\u304c\u3059\u3079\u3066\u63c3\u3063\u3066\u3044\u307e\u3059\u3002 API \u306f\u3001\u79c1\u305f\u3061\u306e API \u3092\u3042\u306a\u305f\u306e\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u7d71\u5408\u3059\u308b\u305f\u3081\u306b\u5fc5\u8981\u306a\u3059\u3079\u3066\u306e\u3082\u306e\u3092\u898b\u3064\u3051\u308b\u5834\u6240\u3067\u3059\u3002 \u3082\u3057\u3001\u3042\u306a\u305f\u304c\u6756\u3092\u4f7f\u3044\u305f\u304f\u3066\u3046\u305a\u3046\u305a\u3057\u3066\u3044\u308b\u306a\u3089\u3001 Query Quick Start \u306b\u98db\u3093\u3067\u3001\u6700\u521d\u306eDune Query\u3092\u4f5c\u308b\u305f\u3081\u306e\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u3092\u898b\u3066\u304f\u3060\u3055\u3044\u3002","title":"How to navigate these docs"},{"location":"#dune-is-a-community-effort","text":"Dune.com \u3067\u306f\u3001\u3059\u3079\u3066\u306e\u30af\u30a8\u30ea\u3068\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u516c\u958b\u3055\u308c\u3066\u3044\u307e\u3059\uff08\u30af\u30a8\u30ea\u306b\u30d7\u30e9\u30a4\u30d0\u30b7\u30fc\u304c\u5fc5\u8981\u306a\u5834\u5408\u306f\u3001 Pro Plan \u3067\u5bfe\u5fdc\u53ef\u80fd\u3067\u3059\uff09\u3002 \u3053\u308c\u306b\u3088\u308a\u3001Wizard\u306f\u4ed6\u306e\u30af\u30ea\u30a8\u30a4\u30bf\u30fc\u306eQuery\u3092\u30d5\u30a9\u30fc\u30af\u3001\u30ea\u30df\u30c3\u30af\u30b9\u3057\u3066\u3001\u5f7c\u3089\u306e\u77e5\u8b58\u3084\u6d1e\u5bdf\u529b\u306e\u4e0a\u306b\u69cb\u7bc9\u3059\u308b\u3053\u3068\u304c\u5bb9\u6613\u306b\u306a\u308a\u307e\u3059\u3002 \u9006\u306b\u3001\u65b0\u3057\u3044Query\u3092\u4f5c\u6210\u3059\u308b\u305f\u3073\u306b\u3001Dune\u3092\u901a\u3058\u3066\u4ed6\u306e\u4eba\u304c\u30d6\u30ed\u30c3\u30af\u30c1\u30a7\u30fc\u30f3\u3084\u6697\u53f7\u8cc7\u7523\u306b\u3064\u3044\u3066\u65b0\u3057\u3044\u3053\u3068\u3092\u5b66\u3076\u306e\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002 \u3053\u306e\u30dd\u30b8\u30c6\u30a3\u30d6\u306a\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u30eb\u30fc\u30d7\u306b\u3088\u3063\u3066\u3001\u30c7\u30e5\u30fc\u30f3\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u306f\u3001\u79c1\u305f\u3061\u5168\u54e1\u304c\u3088\u308a\u591a\u304f\u3092\u5b66\u3076\u3053\u3068\u304c\u3067\u304d\u308b\u3001\u5897\u3048\u7d9a\u3051\u308b\u30af\u30a8\u30ea\u306e\u7bc4\u56f2\u3092\u901a\u3057\u3066\u3001\u5171\u306b\u6210\u529f\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u306e\u3067\u3059\u3002 \u79c1\u305f\u3061\u306e Community Discord \u306b\u53c2\u52a0\u3059\u308c\u3070\u3001\u79c1\u305f\u3061\u306e\u30c1\u30fc\u30e0\u3068\u30b3\u30df\u30e5\u30cb\u30c6\u30a3\u304b\u3089\u4e16\u754c\u30ec\u30d9\u30eb\u306e\u30b5\u30dd\u30fc\u30c8\u3092\u53d7\u3051\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u697d\u3057\u3044LIVE\u306b\u53c2\u52a0\u3059\u308b\u305f\u3081\u306b\u3001 events calendar \u3092\u30c1\u30a7\u30c3\u30af\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\u3002 \u307e\u305f\u3001\u6a5f\u80fd\u8981\u671b\u3084\u30d0\u30b0\u30ec\u30dd\u30fc\u30c8\u306a\u3069\u3001\u30d5\u30a3\u30fc\u30c9\u30d0\u30c3\u30af\u304c\u3042\u308c\u3070\u3001 here \u306b\u63d0\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002","title":"Dune is a community effort"},{"location":"api/","text":"Welcome to the Dune API \u00b6 The Dune API gives you full access to the queries and data you can see on the Dune website. This means you can execute and read results from any public query, as well as any personal private queries your Dune account has access to. This documentation describes all of the available API calls and properties of the returned objects. If you have any questions or feedback, please reach out to api-feedback@dune.com or our # dune-api Discord channel! Warning At this time, the Dune API is in private beta. This means that the way it works and the data it returns may change with advanced notice. How does the API work? \u00b6 The API currently lets users: Execute a query Check the status of an execution Get the results of an execution These results are currently stored separately from anything you see on the Dune.com website. This means the only way to get query results from the Dune API is to execute a query using the Dune API. Similarly, results from API executions are not currently reflected on Dune\u2019s website. API Getting Started \u00b6 Obtaining an API Key \u00b6 Dune API is currently in private beta with a closed set of users, but will be publicly available in November! Picking a Programming Language \u00b6 While you can consume our API in the language of your choice - see the API Reference section - we currently have quick start guides for Python and Node.js . What can I build with the Dune API? \u00b6 With Dune, you have access to almost all of the data from today's most popular blockchain ecosystems. There is no limit to what you can build on top of this data! With so many possibilities, it can be challenging to figure out what to work on at times, so here are some ideas to help you ideate: New Kinds of Interfaces for Blockchain Data \u00b6 Our community champion @0xBoxer has this tutorial where he walks us through creating a dashboard for personalized metrics for any wallet. With the Dune API it's possible to build a much nicer interface for this or any dashboard on Dune inside of your own super-smooth, super-cool app UX. Or Excel, Google sheets, Notion Pages, Discord Bots, Telegram Bots, there are no limits. With the API, the Data Can Flow anywhere! Real World Examples from Cow Protocol \u00b6 @bh2smith from our community (and Cow Protocol ), gave a talk Thursday at DuneCon where he walked us through some real-world examples of how Cow Protocol has been using the API. Check out the replay here ! Important Links \u00b6 API Documentation - you're already here, check out the sidebar to learn more! #dune-api Discord Channel API Client (Community Sourced)","title":"API"},{"location":"api/#welcome-to-the-dune-api","text":"The Dune API gives you full access to the queries and data you can see on the Dune website. This means you can execute and read results from any public query, as well as any personal private queries your Dune account has access to. This documentation describes all of the available API calls and properties of the returned objects. If you have any questions or feedback, please reach out to api-feedback@dune.com or our # dune-api Discord channel! Warning At this time, the Dune API is in private beta. This means that the way it works and the data it returns may change with advanced notice.","title":"Welcome to the Dune API"},{"location":"api/#how-does-the-api-work","text":"The API currently lets users: Execute a query Check the status of an execution Get the results of an execution These results are currently stored separately from anything you see on the Dune.com website. This means the only way to get query results from the Dune API is to execute a query using the Dune API. Similarly, results from API executions are not currently reflected on Dune\u2019s website.","title":"How does the API work?"},{"location":"api/#api-getting-started","text":"","title":"API Getting Started"},{"location":"api/#obtaining-an-api-key","text":"Dune API is currently in private beta with a closed set of users, but will be publicly available in November!","title":"Obtaining an API Key"},{"location":"api/#picking-a-programming-language","text":"While you can consume our API in the language of your choice - see the API Reference section - we currently have quick start guides for Python and Node.js .","title":"Picking a Programming Language"},{"location":"api/#what-can-i-build-with-the-dune-api","text":"With Dune, you have access to almost all of the data from today's most popular blockchain ecosystems. There is no limit to what you can build on top of this data! With so many possibilities, it can be challenging to figure out what to work on at times, so here are some ideas to help you ideate:","title":"What can I build with the Dune API?"},{"location":"api/#new-kinds-of-interfaces-for-blockchain-data","text":"Our community champion @0xBoxer has this tutorial where he walks us through creating a dashboard for personalized metrics for any wallet. With the Dune API it's possible to build a much nicer interface for this or any dashboard on Dune inside of your own super-smooth, super-cool app UX. Or Excel, Google sheets, Notion Pages, Discord Bots, Telegram Bots, there are no limits. With the API, the Data Can Flow anywhere!","title":"New Kinds of Interfaces for Blockchain Data"},{"location":"api/#real-world-examples-from-cow-protocol","text":"@bh2smith from our community (and Cow Protocol ), gave a talk Thursday at DuneCon where he walked us through some real-world examples of how Cow Protocol has been using the API. Check out the replay here !","title":"Real World Examples from Cow Protocol"},{"location":"api/#important-links","text":"API Documentation - you're already here, check out the sidebar to learn more! #dune-api Discord Channel API Client (Community Sourced)","title":"Important Links"},{"location":"api/FAQ/TOS/","text":"Find our API Terms of Service Here","title":"TOS"},{"location":"api/FAQ/billing-pricing/","text":"FAQ: Billing & Pricing \u00b6 Is the API free? \u00b6 We are offering limited-time free trials for select users. The API will be a paid service after any trial period. How will API Billing work with the new Team plans? \u00b6 Over the next several months we\u2019ll be rolling out new \u201cTeam Plans\u201d which will have their own credit card associated with that billing method. API calls that come from a user who is a member of Team [X] for a query made by Team [X] are automatically billed to Team [X]. API calls that come from a user and do not belong to their team are billed to their personal accounts. We therefore recommend any individual users who recieved API keys during their trial consolidate their keys and usage into a single account at the end of their trial, This helps streamline Team API usage easier and avoids the need for every API key holder to keep their credit card information on file. How will the API be priced? \u00b6 The API will be priced based on # of executions and # of data points returned. More details on this will be shared in the coming weeks! What\u2019s a datapoint? \u00b6 A datapoint can in most cases be thought of rows * columns with an additional limit of 50 avg bytes per cell in a set of results. This can be expressed as: Datapoints = max(rows*columns, ceil(totalbytes/50)) Do I get charged datapoints for every execution? \u00b6 We charge the data points in the result for the 1st read result of every distinct query execution and every subsequent 100th read.","title":"Billing & Pricing"},{"location":"api/FAQ/billing-pricing/#faq-billing-pricing","text":"","title":"FAQ: Billing &amp; Pricing"},{"location":"api/FAQ/billing-pricing/#is-the-api-free","text":"We are offering limited-time free trials for select users. The API will be a paid service after any trial period.","title":"Is the API free?"},{"location":"api/FAQ/billing-pricing/#how-will-api-billing-work-with-the-new-team-plans","text":"Over the next several months we\u2019ll be rolling out new \u201cTeam Plans\u201d which will have their own credit card associated with that billing method. API calls that come from a user who is a member of Team [X] for a query made by Team [X] are automatically billed to Team [X]. API calls that come from a user and do not belong to their team are billed to their personal accounts. We therefore recommend any individual users who recieved API keys during their trial consolidate their keys and usage into a single account at the end of their trial, This helps streamline Team API usage easier and avoids the need for every API key holder to keep their credit card information on file.","title":"How will API Billing work with the new Team plans?"},{"location":"api/FAQ/billing-pricing/#how-will-the-api-be-priced","text":"The API will be priced based on # of executions and # of data points returned. More details on this will be shared in the coming weeks!","title":"How will the API be priced?"},{"location":"api/FAQ/billing-pricing/#whats-a-datapoint","text":"A datapoint can in most cases be thought of rows * columns with an additional limit of 50 avg bytes per cell in a set of results. This can be expressed as: Datapoints = max(rows*columns, ceil(totalbytes/50))","title":"What\u2019s a datapoint?"},{"location":"api/FAQ/billing-pricing/#do-i-get-charged-datapoints-for-every-execution","text":"We charge the data points in the result for the 1st read result of every distinct query execution and every subsequent 100th read.","title":"Do I get charged datapoints for every execution?"},{"location":"api/FAQ/functionality/","text":"FAQ: Functionality \u00b6 General \u00b6 How many Requests Per Minute can I make? \u00b6 The API is currently set to a rate limit of 40 requests per minute. This will ultimately vary with the introduction of our API plans of varying tiers. Please reach out to our API team if you require higher throughput in the interim! Are there specified SLAs? \u00b6 SLAs will be available in the future on Enterprise pricing plans. Executing Queries \u00b6 How do I find a query id? \u00b6 When navigating to a query, it\u2019s the first number after \u201c/queries/\u201d in the URL. Does the API support Query Parameters? \u00b6 The API does support Query Parameters! For Dune Queries that include Parameters, you can pass parameter data as part of the Execute Query ID endpoint ! Learn more about building Dune Queries with Parameters here . And learn how to pass parameter data using cURL here and with Python here . What are the performance and overall differences between the Dune API and the Dune web app? What are the differences in what I can query? \u00b6 There are no major performance differences or differences in what can be accessed between the two. The Dune API simply gives you programmatic access to the capabilities and data sets that can already be accessed from the Dune web app. What is the execution timeout limit and can I request a longer limit? \u00b6 Initially, the query execution timeout limit will match the Dune web app - 30 minutes. Later, we plan to allow overrides of this, but we\u2019d need to adjust our query execution billing to reflect this as well. When do you plan on supporting the use of sending raw SQL directly from the server? \u00b6 We don\u2019t have a committed timeline yet, but this is tentatively planned to be a feature only available on Enterprise plans. Can I query using both Dune v2 Engine and the original v1 databases? \u00b6 Currently yes, but we\u2019re slowly deprecating usage and support of the old engine, so we recommend using the new Dune v2 Engine as much as possible. Check Execution Status \u00b6 What is the difference between the states \u201cExecuting\u201d and \u201cPending\u201d? \u00b6 Pending means, the execution is waiting for an available execution connection slot. Executing means the query is currently executing against the database. Reading Results Data \u00b6 Can I ingest data by getting a direct connection to the database instead? \u00b6 Not currently. In the interim we recommend periodically fetching from \u201cmax(latestBlockNumber) - 2\u201d to \u201clastFetchedBlockNumber\u201d in regular intervals. Fetching from 2 behind the latest block number ensures you receive full sets of data from each new request. Are query results data saved for faster retrieval? \u00b6 Yes! How long are the results data from an execution stored for? \u00b6 Currently set to 2 years but we may reduce this to something closer to 90 days in the future. This is visible on the API response on the \u201cexpires_at\u201d field in the execution status and results body. How much data can I retrieve in a single API result call? \u00b6 There is currently a 500MB limit, but there is a chance we increase this for certain paid plans. The API does not currently return an explicit error upon hitting this limit but will instead fail (timeout) when attempting to retrieve the results.","title":"Functionality"},{"location":"api/FAQ/functionality/#faq-functionality","text":"","title":"FAQ: Functionality"},{"location":"api/FAQ/functionality/#general","text":"","title":"General"},{"location":"api/FAQ/functionality/#how-many-requests-per-minute-can-i-make","text":"The API is currently set to a rate limit of 40 requests per minute. This will ultimately vary with the introduction of our API plans of varying tiers. Please reach out to our API team if you require higher throughput in the interim!","title":"How many Requests Per Minute can I make?"},{"location":"api/FAQ/functionality/#are-there-specified-slas","text":"SLAs will be available in the future on Enterprise pricing plans.","title":"Are there specified SLAs?"},{"location":"api/FAQ/functionality/#executing-queries","text":"","title":"Executing Queries"},{"location":"api/FAQ/functionality/#how-do-i-find-a-query-id","text":"When navigating to a query, it\u2019s the first number after \u201c/queries/\u201d in the URL.","title":"How do I find a query id?"},{"location":"api/FAQ/functionality/#does-the-api-support-query-parameters","text":"The API does support Query Parameters! For Dune Queries that include Parameters, you can pass parameter data as part of the Execute Query ID endpoint ! Learn more about building Dune Queries with Parameters here . And learn how to pass parameter data using cURL here and with Python here .","title":"Does the API support Query Parameters?"},{"location":"api/FAQ/functionality/#what-are-the-performance-and-overall-differences-between-the-dune-api-and-the-dune-web-app-what-are-the-differences-in-what-i-can-query","text":"There are no major performance differences or differences in what can be accessed between the two. The Dune API simply gives you programmatic access to the capabilities and data sets that can already be accessed from the Dune web app.","title":"What are the performance and overall differences between the Dune API and the Dune web app? What are the differences in what I can query?"},{"location":"api/FAQ/functionality/#what-is-the-execution-timeout-limit-and-can-i-request-a-longer-limit","text":"Initially, the query execution timeout limit will match the Dune web app - 30 minutes. Later, we plan to allow overrides of this, but we\u2019d need to adjust our query execution billing to reflect this as well.","title":"What is the execution timeout limit and can I request a longer limit?"},{"location":"api/FAQ/functionality/#when-do-you-plan-on-supporting-the-use-of-sending-raw-sql-directly-from-the-server","text":"We don\u2019t have a committed timeline yet, but this is tentatively planned to be a feature only available on Enterprise plans.","title":"When do you plan on supporting the use of sending raw SQL directly from the server?"},{"location":"api/FAQ/functionality/#can-i-query-using-both-dune-v2-engine-and-the-original-v1-databases","text":"Currently yes, but we\u2019re slowly deprecating usage and support of the old engine, so we recommend using the new Dune v2 Engine as much as possible.","title":"Can I query using both Dune v2 Engine and the original v1 databases?"},{"location":"api/FAQ/functionality/#check-execution-status","text":"","title":"Check Execution Status"},{"location":"api/FAQ/functionality/#what-is-the-difference-between-the-states-executing-and-pending","text":"Pending means, the execution is waiting for an available execution connection slot. Executing means the query is currently executing against the database.","title":"What is the difference between the states \u201cExecuting\u201d and \u201cPending\u201d?"},{"location":"api/FAQ/functionality/#reading-results-data","text":"","title":"Reading Results Data"},{"location":"api/FAQ/functionality/#can-i-ingest-data-by-getting-a-direct-connection-to-the-database-instead","text":"Not currently. In the interim we recommend periodically fetching from \u201cmax(latestBlockNumber) - 2\u201d to \u201clastFetchedBlockNumber\u201d in regular intervals. Fetching from 2 behind the latest block number ensures you receive full sets of data from each new request.","title":"Can I ingest data by getting a direct connection to the database instead?"},{"location":"api/FAQ/functionality/#are-query-results-data-saved-for-faster-retrieval","text":"Yes!","title":"Are query results data saved for faster retrieval?"},{"location":"api/FAQ/functionality/#how-long-are-the-results-data-from-an-execution-stored-for","text":"Currently set to 2 years but we may reduce this to something closer to 90 days in the future. This is visible on the API response on the \u201cexpires_at\u201d field in the execution status and results body.","title":"How long are the results data from an execution stored for?"},{"location":"api/FAQ/functionality/#how-much-data-can-i-retrieve-in-a-single-api-result-call","text":"There is currently a 500MB limit, but there is a chance we increase this for certain paid plans. The API does not currently return an explicit error upon hitting this limit but will instead fail (timeout) when attempting to retrieve the results.","title":"How much data can I retrieve in a single API result call?"},{"location":"api/FAQ/other/","text":"Please reach out to api-feedback@dune.com or our # dune-api Discord channel for the fastest path towards getting additional questions answered!","title":"Other"},{"location":"api/api-reference/","text":"Learn about our API endpoints and common errors here: Authentication Execute Query ID Execution Status Execution Results Cancel Execution Error Codes","title":"API Reference"},{"location":"api/api-reference/authentication/","text":"The Dune API uses API keys to authenticate requests. Your API key is used to determine the permissions of private queries you may call, as well as which account to bill for the requests, so be sure to keep them secure! Do not share your secret API keys in publicly accessible areas such as GitHub, client-side code, and so forth. Authentication with the API is performed by adding an \u201cx-dune-api-key\u201d property to the request header. This is needed on all request types! Here's one example of doing this with an Execute POST API request: curl -X POST -H x-dune-api-key:{{api_key}} \"https://api.dune.com/api/v1/query/{{query_id}}/execute\"","title":"Authentication"},{"location":"api/api-reference/cancel-execution/","text":"[POST] Cancel Execution \u00b6 Here's how to cancel your Dune API execution requests. Arguments \u00b6 None. Returns \u00b6 Returns a boolean for whether the execution is successfully canceled. Example Request \u00b6 You need to pass the execution_id you obtained from making a Execute Query ID POST request to the complete a Cancel Execution API request. POST v1/execution/{{execution_id}}/cancel https://api.dune.com/api/v1/execution/{{execution_id}}/cancel cURL \u00b6 curl -X POST -H x-dune-api-key:{{api_key}} \"https://api.dune.com/api/v1/execution/{{execution_id}}/cancel\" Example Response \u00b6 Dune API responses are delivered in JSON format. { \"success\" : true } success : A boolean, indicating whether the request to cancel the query execution was made successfully.","title":"Cancel Execution"},{"location":"api/api-reference/cancel-execution/#post-cancel-execution","text":"Here's how to cancel your Dune API execution requests.","title":"[POST] Cancel Execution"},{"location":"api/api-reference/cancel-execution/#arguments","text":"None.","title":"Arguments"},{"location":"api/api-reference/cancel-execution/#returns","text":"Returns a boolean for whether the execution is successfully canceled.","title":"Returns"},{"location":"api/api-reference/cancel-execution/#example-request","text":"You need to pass the execution_id you obtained from making a Execute Query ID POST request to the complete a Cancel Execution API request. POST v1/execution/{{execution_id}}/cancel https://api.dune.com/api/v1/execution/{{execution_id}}/cancel","title":"Example Request"},{"location":"api/api-reference/cancel-execution/#curl","text":"curl -X POST -H x-dune-api-key:{{api_key}} \"https://api.dune.com/api/v1/execution/{{execution_id}}/cancel\"","title":"cURL"},{"location":"api/api-reference/cancel-execution/#example-response","text":"Dune API responses are delivered in JSON format. { \"success\" : true } success : A boolean, indicating whether the request to cancel the query execution was made successfully.","title":"Example Response"},{"location":"api/api-reference/errors/","text":"The good news: you are very early to Dune API. \ud83e\uddd9\u200d\u2642\ufe0f The bad news: that means we're still working out the gremlins. \ud83d\udc79 For now, the Dune API doesn't always throw error messages, so in some cases when things do not work as expected you'll need to decode what happened using our response objects. Note If this is too technical/difficult/confusing, don't break your wand - reach out to us on the #dune-api Discord channel and we'll help out when you're stuck! Here are a few common error scenarios: Invalid API Key \u00b6 Response Object \u00b6 {'error': 'invalid API Key'} Checks \u00b6 Make sure that you are passing your API key to our endpoint in a header . See the section on Authentication for how to do that, and our quick start guides for specific language examples. If you are already passing the API key in an header, make sure that it is correctly entered. An Internal Error Occurred \u00b6 Response Object \u00b6 {'error': 'An internal error occurred'} Checks \u00b6 If you are using one of our GET endpoints, ensure that the query_id you have entered is correct. If you are using one of our POST endpoints, ensure that the execution_id you obtained from your GET endpoint has been correctly passed on to your POST endpoint. The documentation here isn't exhaustive! While we are working on making it better, again, the best place to get any support is our #dune-api Discord channel .","title":"Error Codes"},{"location":"api/api-reference/errors/#invalid-api-key","text":"","title":"Invalid API Key"},{"location":"api/api-reference/errors/#response-object","text":"{'error': 'invalid API Key'}","title":"Response Object"},{"location":"api/api-reference/errors/#checks","text":"Make sure that you are passing your API key to our endpoint in a header . See the section on Authentication for how to do that, and our quick start guides for specific language examples. If you are already passing the API key in an header, make sure that it is correctly entered.","title":"Checks"},{"location":"api/api-reference/errors/#an-internal-error-occurred","text":"","title":"An Internal Error Occurred"},{"location":"api/api-reference/errors/#response-object_1","text":"{'error': 'An internal error occurred'}","title":"Response Object"},{"location":"api/api-reference/errors/#checks_1","text":"If you are using one of our GET endpoints, ensure that the query_id you have entered is correct. If you are using one of our POST endpoints, ensure that the execution_id you obtained from your GET endpoint has been correctly passed on to your POST endpoint. The documentation here isn't exhaustive! While we are working on making it better, again, the best place to get any support is our #dune-api Discord channel .","title":"Checks"},{"location":"api/api-reference/execute-query-id/","text":"[POST] Execute Query ID \u00b6 Here's how to execute (run) a query with or without parameters to retrieve data. Arguments \u00b6 None required. You may optionally add query parameters ( see this example ). Returns \u00b6 Returns an execution_id for the specified request. Example Request \u00b6 POST v1/query/{{query_id}}/execute https://api.dune.com/api/v1/query/{{query_id}}/execute cURL \u00b6 curl -X POST -H x-dune-api-key:{{api_key}} \"https://api.dune.com/api/v1/query/{{query_id}}/execute\" cURL with Parameters \u00b6 curl -X POST -d '{\"query_parameters\": { \"param1\":24}}' -H x-dune-api-key:{{api_key}} \"https://api.dune.com/api/v1/query/{{query_id}}/execute\" Example Response \u00b6 Dune API responses are delivered in JSON format. { \"execution_id\" : \"01GB1Y2MRA4C9PNQ0EQYVT4K6R\" , \"state\" : \"QUERY_STATE_PENDING\" } execution_id : A unique ID that is generated every time this API is called. You might want to save it to later pass on to other API endpoints. state : The current state of the query's execution. Check our FAQ section to see what different status codes mean.","title":"Execute Query ID"},{"location":"api/api-reference/execute-query-id/#post-execute-query-id","text":"Here's how to execute (run) a query with or without parameters to retrieve data.","title":"[POST] Execute Query ID"},{"location":"api/api-reference/execute-query-id/#arguments","text":"None required. You may optionally add query parameters ( see this example ).","title":"Arguments"},{"location":"api/api-reference/execute-query-id/#returns","text":"Returns an execution_id for the specified request.","title":"Returns"},{"location":"api/api-reference/execute-query-id/#example-request","text":"POST v1/query/{{query_id}}/execute https://api.dune.com/api/v1/query/{{query_id}}/execute","title":"Example Request"},{"location":"api/api-reference/execute-query-id/#curl","text":"curl -X POST -H x-dune-api-key:{{api_key}} \"https://api.dune.com/api/v1/query/{{query_id}}/execute\"","title":"cURL"},{"location":"api/api-reference/execute-query-id/#curl-with-parameters","text":"curl -X POST -d '{\"query_parameters\": { \"param1\":24}}' -H x-dune-api-key:{{api_key}} \"https://api.dune.com/api/v1/query/{{query_id}}/execute\"","title":"cURL with Parameters"},{"location":"api/api-reference/execute-query-id/#example-response","text":"Dune API responses are delivered in JSON format. { \"execution_id\" : \"01GB1Y2MRA4C9PNQ0EQYVT4K6R\" , \"state\" : \"QUERY_STATE_PENDING\" } execution_id : A unique ID that is generated every time this API is called. You might want to save it to later pass on to other API endpoints. state : The current state of the query's execution. Check our FAQ section to see what different status codes mean.","title":"Example Response"},{"location":"api/api-reference/execution-results/","text":"[GET] Execution Results \u00b6 Here's how to get the results data of an execution request. Arguments \u00b6 None. Returns \u00b6 Returns back the status, metadata, and query results from a query execution. Example Request \u00b6 You need to pass the execution_id you obtained from making a Execute Query ID POST request to the complete an Execution Results API request. GET v1/execution/{{execution_id}}/results https://api.dune.com/api/v1/execution/{{execution_id}}/results cURL \u00b6 curl -X GET \"https://api.dune.com/api/v1/execution/{{execution_id}}/results\" -H x-dune-api-key:{{api_key}} Example Response \u00b6 Dune API responses are delivered in JSON format. { \"execution_id\" : \"01GBM4W2N0NMCGPZYW8AYK4YF1\" , \"query_id\" : 980708 , \"state\" : \"QUERY_STATE_COMPLETED\" , \"submitted_at\" : \"2022-08-29T06:33:24.913138Z\" , \"expires_at\" : \"2024-08-28T06:36:41.58847Z\" , \"execution_started_at\" : \"2022-08-29T06:33:24.916543Z\" , \"execution_ended_at\" : \"2022-08-29T06:36:41.588467Z\" , \"result\" : { \"rows\" : [ { \"TableName\" : \"eth_blocks\" , \"ct\" : 6296 }, { \"TableName\" : \"eth_traces\" , \"ct\" : 4474223 }, { \"TableName\" : \"eth_creation_traces\" , \"ct\" : 10155 }, { \"TableName\" : \"eth_logs\" , \"ct\" : 2137508 }, { \"TableName\" : \"eth_transactions\" , \"ct\" : 1039890 }, { \"TableName\" : \"sol_transactions\" , \"ct\" : 37185158 }, { \"TableName\" : \"bnb_transactions\" , \"ct\" : 2942005 }, { \"TableName\" : \"optimism_transactions\" , \"ct\" : 120973 } ], \"metadata\" : { \"column_names\" : [ \"ct\" , \"TableName\" ], \"result_set_bytes\" : 194 , \"total_row_count\" : 8 , \"datapoint_count\" : 16 , \"pending_time_millis\" : 8 , \"execution_time_millis\" : 24 } } } execution_id : The execution ID for which this API was called. query_id : The ID of the Dune Query executed with this request. state : The current state of the query's execution. Check our FAQ section to see what different status codes for state mean. submitted_at : The timestamp at which the API for executing this query was called. expires_at : The time upto which results from this query's execution shall be stored in our Database. execution_started_at : The time at which query execution started for this request in our servers. execution_ended_at : The time at which the query execution for this request got completed in our servers. result : rows : The actual rows of data being returned for this request. metadata : Some properties of the queried data being returned. column_names : Names of the columns in the data returned. result_set_bytes : The size of the returned data. total_row_count : The number of rows in the data. datapoint_count : Total number of datapoints returned with this request, should equal to ( total_row_count x number of columns). pending_time_millis : The time (in milliseconds) it took to assign a slot in our server for this request. execution_time_millis : The time (in milliseconds) it took for the actual execution of the query with this request. Reading Results Data FAQ \u00b6 Can I ingest data by getting a direct connection to the database instead? \u00b6 Not currently. In the interim we recommend periodically fetching from \u201cmax(latestBlockNumber) - 2\u201d to \u201clastFetchedBlockNumber\u201d in regular intervals. Fetching from 2 behind the latest block number ensures you receive full sets of data from each new request. Are query results data saved for faster retrieval? \u00b6 Yes How long are the results data from an execution stored for? \u00b6 Currently set to 2 years but we may reduce this to something closer to 90 days in the future. This is visible on the API response on the \u201cexpires_at\u201d field in the execution status and results body. How much data can I retrieve in a single API result call? \u00b6 There is currently a 1GB limit, but there is a chance we reduce this overall or based on varying paid plan types.","title":"Execution Results"},{"location":"api/api-reference/execution-results/#get-execution-results","text":"Here's how to get the results data of an execution request.","title":"[GET] Execution Results"},{"location":"api/api-reference/execution-results/#arguments","text":"None.","title":"Arguments"},{"location":"api/api-reference/execution-results/#returns","text":"Returns back the status, metadata, and query results from a query execution.","title":"Returns"},{"location":"api/api-reference/execution-results/#example-request","text":"You need to pass the execution_id you obtained from making a Execute Query ID POST request to the complete an Execution Results API request. GET v1/execution/{{execution_id}}/results https://api.dune.com/api/v1/execution/{{execution_id}}/results","title":"Example Request"},{"location":"api/api-reference/execution-results/#curl","text":"curl -X GET \"https://api.dune.com/api/v1/execution/{{execution_id}}/results\" -H x-dune-api-key:{{api_key}}","title":"cURL"},{"location":"api/api-reference/execution-results/#example-response","text":"Dune API responses are delivered in JSON format. { \"execution_id\" : \"01GBM4W2N0NMCGPZYW8AYK4YF1\" , \"query_id\" : 980708 , \"state\" : \"QUERY_STATE_COMPLETED\" , \"submitted_at\" : \"2022-08-29T06:33:24.913138Z\" , \"expires_at\" : \"2024-08-28T06:36:41.58847Z\" , \"execution_started_at\" : \"2022-08-29T06:33:24.916543Z\" , \"execution_ended_at\" : \"2022-08-29T06:36:41.588467Z\" , \"result\" : { \"rows\" : [ { \"TableName\" : \"eth_blocks\" , \"ct\" : 6296 }, { \"TableName\" : \"eth_traces\" , \"ct\" : 4474223 }, { \"TableName\" : \"eth_creation_traces\" , \"ct\" : 10155 }, { \"TableName\" : \"eth_logs\" , \"ct\" : 2137508 }, { \"TableName\" : \"eth_transactions\" , \"ct\" : 1039890 }, { \"TableName\" : \"sol_transactions\" , \"ct\" : 37185158 }, { \"TableName\" : \"bnb_transactions\" , \"ct\" : 2942005 }, { \"TableName\" : \"optimism_transactions\" , \"ct\" : 120973 } ], \"metadata\" : { \"column_names\" : [ \"ct\" , \"TableName\" ], \"result_set_bytes\" : 194 , \"total_row_count\" : 8 , \"datapoint_count\" : 16 , \"pending_time_millis\" : 8 , \"execution_time_millis\" : 24 } } } execution_id : The execution ID for which this API was called. query_id : The ID of the Dune Query executed with this request. state : The current state of the query's execution. Check our FAQ section to see what different status codes for state mean. submitted_at : The timestamp at which the API for executing this query was called. expires_at : The time upto which results from this query's execution shall be stored in our Database. execution_started_at : The time at which query execution started for this request in our servers. execution_ended_at : The time at which the query execution for this request got completed in our servers. result : rows : The actual rows of data being returned for this request. metadata : Some properties of the queried data being returned. column_names : Names of the columns in the data returned. result_set_bytes : The size of the returned data. total_row_count : The number of rows in the data. datapoint_count : Total number of datapoints returned with this request, should equal to ( total_row_count x number of columns). pending_time_millis : The time (in milliseconds) it took to assign a slot in our server for this request. execution_time_millis : The time (in milliseconds) it took for the actual execution of the query with this request.","title":"Example Response"},{"location":"api/api-reference/execution-results/#reading-results-data-faq","text":"","title":"Reading Results Data FAQ"},{"location":"api/api-reference/execution-results/#can-i-ingest-data-by-getting-a-direct-connection-to-the-database-instead","text":"Not currently. In the interim we recommend periodically fetching from \u201cmax(latestBlockNumber) - 2\u201d to \u201clastFetchedBlockNumber\u201d in regular intervals. Fetching from 2 behind the latest block number ensures you receive full sets of data from each new request.","title":"Can I ingest data by getting a direct connection to the database instead?"},{"location":"api/api-reference/execution-results/#are-query-results-data-saved-for-faster-retrieval","text":"Yes","title":"Are query results data saved for faster retrieval?"},{"location":"api/api-reference/execution-results/#how-long-are-the-results-data-from-an-execution-stored-for","text":"Currently set to 2 years but we may reduce this to something closer to 90 days in the future. This is visible on the API response on the \u201cexpires_at\u201d field in the execution status and results body.","title":"How long are the results data from an execution stored for?"},{"location":"api/api-reference/execution-results/#how-much-data-can-i-retrieve-in-a-single-api-result-call","text":"There is currently a 1GB limit, but there is a chance we reduce this overall or based on varying paid plan types.","title":"How much data can I retrieve in a single API result call?"},{"location":"api/api-reference/execution-status/","text":"[GET] Execution Status \u00b6 Here's how to check the status of an execution request. Arguments \u00b6 None. Returns \u00b6 Returns the status of a query execution along with relevant metadata of the results if the execution is completed. Example Request \u00b6 You need to pass the execution_id you obtained from making a Execute Query ID POST request to the complete an Execution Status API request. GET v1/execution/{{execution_id}}/status https://api.dune.com/api/v1/execution/{{execution_id}}/status cURL \u00b6 curl -X GET \"https://api.dune.com/api/v1/execution/{{execution_id}}/status\" -H x-dune-api-key:{{api_key}} Example Response \u00b6 Dune API responses are delivered in JSON format. Query Executing \u00b6 { \"execution_id\" : \"01GBM4W2N0NMCGPZYW8AYK4YF1\" , \"query_id\" : 980708 , \"state\" : \"QUERY_STATE_EXECUTING\" , \"submitted_at\" : \"2022-08-29T06:33:24.913138Z\" , \"expires_at\" : \"1970-01-01T00:00:00Z\" , \"execution_started_at\" : \"2022-08-29T06:33:24.916543331Z\" } Execution Complete \u00b6 { \"execution_id\" : \"01GBM4W2N0NMCGPZYW8AYK4YF1\" , \"query_id\" : 980708 , \"state\" : \"QUERY_STATE_COMPLETED\" , \"submitted_at\" : \"2022-08-29T06:33:24.913138Z\" , \"expires_at\" : \"2024-08-28T06:36:41.58847Z\" , \"execution_started_at\" : \"2022-08-29T06:33:24.916543Z\" , \"execution_ended_at\" : \"2022-08-29T06:36:41.588467Z\" , \"result_metadata\" : { \"column_names\" : [ \"ct\" , \"TableName\" ], \"result_set_bytes\" : 194 , \"total_row_count\" : 8 , \"datapoint_count\" : 16 , \"pending_time_millis\" : 8 , \"execution_time_millis\" : 24 } } execution_id : The execution ID with which this API was called. query_id : The ID of the Dune Query being executed with this request. state : The current state of the query's execution. Check our FAQ section to see what different status codes for state mean. submitted_at : The timestamp at which the API for executing this query was called. expires_at : The time upto which results from this query's execution shall be stored in our Database. execution_started_at : The time at which execution started for this request in our servers. execution_ended_at : The time at which the execution for this request got completed in our servers. result_metadata : Some properties of the response data generated with this request. column_names : Names of the columns in the response data that is generated for this request. result_set_bytes : The size of the response data in bytes total_row_count : The number of rows in response data datapoint_count : Total number of datapoints returned with this request, should equal to ( total_row_count x number of columns). pending_time_millis : The time (in milliseconds) it took to assign a slot in our server for this request. execution_time_millis : The time (in milliseconds) it took for the actual execution of the query with this request. Check Execution Status FAQ \u00b6 What is the difference between the states \u201cExecuting\u201d and \u201cPending\u201d? \u00b6 Pending means, the execution is waiting for an available execution connection slot. Executing means the query is currently executing against the database. Here is a list of all Status Codes for reference. QUERY_STATE_PENDING - query execution is waiting for execution slot QUERY_STATE_EXECUTING - query is executing QUERY_STATE_FAILED - execution failed QUERY_STATE_COMPLETED - execution completed successfully QUERY_STATE_CANCELLED - execution cancelled by user QUERY_STATE_EXPIRED - query execution expired, result no longer available","title":"Execution Status"},{"location":"api/api-reference/execution-status/#get-execution-status","text":"Here's how to check the status of an execution request.","title":"[GET] Execution Status"},{"location":"api/api-reference/execution-status/#arguments","text":"None.","title":"Arguments"},{"location":"api/api-reference/execution-status/#returns","text":"Returns the status of a query execution along with relevant metadata of the results if the execution is completed.","title":"Returns"},{"location":"api/api-reference/execution-status/#example-request","text":"You need to pass the execution_id you obtained from making a Execute Query ID POST request to the complete an Execution Status API request. GET v1/execution/{{execution_id}}/status https://api.dune.com/api/v1/execution/{{execution_id}}/status","title":"Example Request"},{"location":"api/api-reference/execution-status/#curl","text":"curl -X GET \"https://api.dune.com/api/v1/execution/{{execution_id}}/status\" -H x-dune-api-key:{{api_key}}","title":"cURL"},{"location":"api/api-reference/execution-status/#example-response","text":"Dune API responses are delivered in JSON format.","title":"Example Response"},{"location":"api/api-reference/execution-status/#query-executing","text":"{ \"execution_id\" : \"01GBM4W2N0NMCGPZYW8AYK4YF1\" , \"query_id\" : 980708 , \"state\" : \"QUERY_STATE_EXECUTING\" , \"submitted_at\" : \"2022-08-29T06:33:24.913138Z\" , \"expires_at\" : \"1970-01-01T00:00:00Z\" , \"execution_started_at\" : \"2022-08-29T06:33:24.916543331Z\" }","title":"Query Executing"},{"location":"api/api-reference/execution-status/#execution-complete","text":"{ \"execution_id\" : \"01GBM4W2N0NMCGPZYW8AYK4YF1\" , \"query_id\" : 980708 , \"state\" : \"QUERY_STATE_COMPLETED\" , \"submitted_at\" : \"2022-08-29T06:33:24.913138Z\" , \"expires_at\" : \"2024-08-28T06:36:41.58847Z\" , \"execution_started_at\" : \"2022-08-29T06:33:24.916543Z\" , \"execution_ended_at\" : \"2022-08-29T06:36:41.588467Z\" , \"result_metadata\" : { \"column_names\" : [ \"ct\" , \"TableName\" ], \"result_set_bytes\" : 194 , \"total_row_count\" : 8 , \"datapoint_count\" : 16 , \"pending_time_millis\" : 8 , \"execution_time_millis\" : 24 } } execution_id : The execution ID with which this API was called. query_id : The ID of the Dune Query being executed with this request. state : The current state of the query's execution. Check our FAQ section to see what different status codes for state mean. submitted_at : The timestamp at which the API for executing this query was called. expires_at : The time upto which results from this query's execution shall be stored in our Database. execution_started_at : The time at which execution started for this request in our servers. execution_ended_at : The time at which the execution for this request got completed in our servers. result_metadata : Some properties of the response data generated with this request. column_names : Names of the columns in the response data that is generated for this request. result_set_bytes : The size of the response data in bytes total_row_count : The number of rows in response data datapoint_count : Total number of datapoints returned with this request, should equal to ( total_row_count x number of columns). pending_time_millis : The time (in milliseconds) it took to assign a slot in our server for this request. execution_time_millis : The time (in milliseconds) it took for the actual execution of the query with this request.","title":"Execution Complete"},{"location":"api/api-reference/execution-status/#check-execution-status-faq","text":"","title":"Check Execution Status FAQ"},{"location":"api/api-reference/execution-status/#what-is-the-difference-between-the-states-executing-and-pending","text":"Pending means, the execution is waiting for an available execution connection slot. Executing means the query is currently executing against the database. Here is a list of all Status Codes for reference. QUERY_STATE_PENDING - query execution is waiting for execution slot QUERY_STATE_EXECUTING - query is executing QUERY_STATE_FAILED - execution failed QUERY_STATE_COMPLETED - execution completed successfully QUERY_STATE_CANCELLED - execution cancelled by user QUERY_STATE_EXPIRED - query execution expired, result no longer available","title":"What is the difference between the states \u201cExecuting\u201d and \u201cPending\u201d?"},{"location":"api/quick-start/","text":"If you're looking for example queries to use (such as NFT or ERC20 balances or holders), then check out these endpoints . Get started with our API fast using these quick start guides: Python Javascript Community Clients","title":"API Quick Start"},{"location":"api/quick-start/api-js/","text":"Warning This guide is not yet comprehensive. If you have questions, please reach out to our team via the # dune-api channel on Discord. Let's get started using the Dune API via JavaScript! We'll show you one of the several ways the API can be consumed via JavaScript, in this case using the node-fetch package. You can also try to use the native fetch funcionality which is available in the newest LTS version of node, node 18. We would be upgrading this guide in the near future to use the same. Prerequisites This Quick Start Guide assumes you have some level of familiarity with Node.js (Node), Node Package Manager (NPM) and Node Version Manager (NVM). To start, make sure you're using the current LTS version of Node.js (Node 18) and the latest version of NPM: Getting Set Up \u00b6 nvm use --lts npm install latest Then install the node-fetch package: npm install node-fetch Next, create a project directory and initiate an ESMcompatible Node project: mkdir dune_api_js cd dune_api_js npm init esm --yes This will initiate a project for you which will include a package.json file. Open this file and add the following line to it: \"type\" : \"module\" Example Dune API Script \u00b6 For the example here, we have used a simple example Query that fetches a small set of data, this query has the query_id , 1258228 . Replace YOUR_API_KEY with your Dune API key in the following code, then add it to the main.js file in your project: import { Headers } from 'node-fetch' ; import fetch from 'node-fetch' ; // Add the API key to an header object const meta = { \"x-dune-api-key\" : \"YOUR_API_KEY\" }; const header = new Headers ( meta ); // Call the Dune API const response = await fetch ( 'https://api.dune.com/api/v1/query/1258228/execute' , { method : 'POST' , headers : header }); const body = await response . text (); // Log the returned response console . log ( body ); Example Dune API Script for a Parameterized Query \u00b6 If you are working with a parameterized query, use the script in this section instead of the previous one. For the example here, we have used a simple query that takes in a wallet address as a parameter. This query has the query_id , 1258228 . And we pass an example address as a value to the wallet_address parameter. Replace YOUR_API_KEY with your Dune API key in the following code, then add it to the main.js file in your project: import { Headers } from 'node-fetch' ; import fetch from 'node-fetch' ; // Add the API key to an header object const meta = { \"x-dune-api-key\" : \"YOUR_API_KEY\" }; const header = new Headers ( meta ); // Add parameters we would pass to the query var params = { \"query_parameters\" : { \"wallet_address\" : \"0xb10f35351ff21bb81dc02d4fd901ac5ae34e8dc4\" }}; var body = JSON . stringify ( params ); // Call the Dune API const response = await fetch ( 'https://api.dune.com/api/v1/query/638435/execute' , { method : 'POST' , headers : header , body : body // This is where we pass the parameters }); const response_object = await response . text (); // Log the returned response console . log ( response_object ); Running the Script \u00b6 You can now run your script from the command line. node main.js You should see a response being returned. You can also edit the Query URL to fetch data from any other Queries you'd like! \ud83e\ude84 If it is a parameterized query, you would need to accordingly update the parameters as well. Please note that the code here only calls the API end point that starts the execution of the query. To fetch the data generated from the execution of this query, you would need to call other API endpoints. See the API Reference section to learn more about various endpoints the Dune API currently offers. Link You can also find some code from this tutorial in this Github Repository .","title":"Javascript"},{"location":"api/quick-start/api-js/#getting-set-up","text":"nvm use --lts npm install latest Then install the node-fetch package: npm install node-fetch Next, create a project directory and initiate an ESMcompatible Node project: mkdir dune_api_js cd dune_api_js npm init esm --yes This will initiate a project for you which will include a package.json file. Open this file and add the following line to it: \"type\" : \"module\"","title":"Getting Set Up"},{"location":"api/quick-start/api-js/#example-dune-api-script","text":"For the example here, we have used a simple example Query that fetches a small set of data, this query has the query_id , 1258228 . Replace YOUR_API_KEY with your Dune API key in the following code, then add it to the main.js file in your project: import { Headers } from 'node-fetch' ; import fetch from 'node-fetch' ; // Add the API key to an header object const meta = { \"x-dune-api-key\" : \"YOUR_API_KEY\" }; const header = new Headers ( meta ); // Call the Dune API const response = await fetch ( 'https://api.dune.com/api/v1/query/1258228/execute' , { method : 'POST' , headers : header }); const body = await response . text (); // Log the returned response console . log ( body );","title":"Example Dune API Script"},{"location":"api/quick-start/api-js/#example-dune-api-script-for-a-parameterized-query","text":"If you are working with a parameterized query, use the script in this section instead of the previous one. For the example here, we have used a simple query that takes in a wallet address as a parameter. This query has the query_id , 1258228 . And we pass an example address as a value to the wallet_address parameter. Replace YOUR_API_KEY with your Dune API key in the following code, then add it to the main.js file in your project: import { Headers } from 'node-fetch' ; import fetch from 'node-fetch' ; // Add the API key to an header object const meta = { \"x-dune-api-key\" : \"YOUR_API_KEY\" }; const header = new Headers ( meta ); // Add parameters we would pass to the query var params = { \"query_parameters\" : { \"wallet_address\" : \"0xb10f35351ff21bb81dc02d4fd901ac5ae34e8dc4\" }}; var body = JSON . stringify ( params ); // Call the Dune API const response = await fetch ( 'https://api.dune.com/api/v1/query/638435/execute' , { method : 'POST' , headers : header , body : body // This is where we pass the parameters }); const response_object = await response . text (); // Log the returned response console . log ( response_object );","title":"Example Dune API Script for a Parameterized Query"},{"location":"api/quick-start/api-js/#running-the-script","text":"You can now run your script from the command line. node main.js You should see a response being returned. You can also edit the Query URL to fetch data from any other Queries you'd like! \ud83e\ude84 If it is a parameterized query, you would need to accordingly update the parameters as well. Please note that the code here only calls the API end point that starts the execution of the query. To fetch the data generated from the execution of this query, you would need to call other API endpoints. See the API Reference section to learn more about various endpoints the Dune API currently offers. Link You can also find some code from this tutorial in this Github Repository .","title":"Running the Script"},{"location":"api/quick-start/api-py/","text":"Let's get started using the Dune API via Python! In this example we'll be using Python3. We recommend using a virtual environment and the pip package manager. Prerequisites This Quick Start Guide assumes you have some prior experience using Python, though we aimed to make the code here easy to follow. If you have questions, please reach out to our team via the # dune-api channel on Discord. Getting Set Up \u00b6 We'll primarily be working with the requests library to access the API, so let's install it: pip install requests We'll also use pandas to load the data returned from APIs into a neat DataFrame (table), and jupyter notebooks to have a nice interactive interface to do all of this. So let us install these as well: pip install pandas pip install jupyter notebook We recommend following rest of the Quick Start in a jupyter notebook. You can start the interface with this simple command: jupyter notebook Import the necessary libraries \u00b6 from requests import get , post import pandas as pd API Keys \u00b6 Any call you make to the Dune API will require you to pass your API key with your call's header: API_KEY = \"YOUR_API_KEY\" HEADER = { \"x-dune-api-key\" : API_KEY } Simplifying URL generation \u00b6 Though not a necessary step, using this function will make it easier to generate URLs for different API endpoints: BASE_URL = \"https://api.dune.com/api/v1/\" def make_api_url ( module , action , ID ): \"\"\" We shall use this function to generate a URL to call the API. \"\"\" url = BASE_URL + module + \"/\" + ID + \"/\" + action return url Wrapping API endpoints in functions \u00b6 The Dune API currently has four primary end points as documented in the API Reference section. We are going to wrap these up in neat functions which will make using the Dune API as easy as a flick of the \ud83e\ude84: def execute_query ( query_id ): \"\"\" Takes in the query ID. Calls the API to execute the query. Returns the execution ID of the instance which is executing the query. \"\"\" url = make_api_url ( \"query\" , \"execute\" , query_id ) response = post ( url , headers = HEADER ) execution_id = response . json ()[ 'execution_id' ] return execution_id def get_query_status ( execution_id ): \"\"\" Takes in an execution ID. Fetches the status of query execution using the API Returns the status response object \"\"\" url = make_api_url ( \"execution\" , \"status\" , execution_id ) response = get ( url , headers = HEADER ) return response def get_query_results ( execution_id ): \"\"\" Takes in an execution ID. Fetches the results returned from the query using the API Returns the results response object \"\"\" url = make_api_url ( \"execution\" , \"results\" , execution_id ) response = get ( url , headers = HEADER ) return response def cancel_query_execution ( execution_id ): \"\"\" Takes in an execution ID. Cancels the ongoing execution of the query. Returns the response object. \"\"\" url = make_api_url ( \"execution\" , \"cancel\" , execution_id ) response = get ( url , headers = HEADER ) return response Using the Dune API \u00b6 Execute a Query \u00b6 To Execute a Query , you can pass any query_id from Dune that you want to fetch data from, then pass it to the execute_query function: Function Call \u00b6 execution_id = execute_query ( \"1258228\" ) Output \u00b6 This function returns an execution_id which will look something like the sample output shown here: '01GCQKPC4QZ6Q8645C3JC4WBT1' This execution ID is the required input for rest of the API functions. Get Query Execution Status \u00b6 To get the Query Execution Status , take the execution_id that was returned from the execute_query function in the previous section, then pass it to get_query_status function as shown here: Function Call \u00b6 response = get_query_status ( execution_id ) Output \u00b6 The response object returned by this function will look something like the example shown here: { 'execu t io n _id' : ' 01 GCQKPC 4 QZ 6 Q 8645 C 3 JC 4 WBT 1 ' , 'query_id' : 1258228 , 's tate ' : 'QUERY_STATE_COMPLETED' , 'submi tte d_a t ' : ' 2022-09-12 T 01 : 05 : 51.781328 Z' , 'expires_a t ' : ' 2024-09-11 T 01 : 05 : 51.82013 Z' , 'execu t io n _s tarte d_a t ' : ' 2022-09-12 T 01 : 05 : 51.806752 Z' , 'execu t io n _e n ded_a t ' : ' 2022-09-12 T 01 : 05 : 51.820127 Z' , 'resul t _me ta da ta ' : { 'colum n _ na mes' : [ 'block_ t ime' , ' t oke n _a_symbol' , ' t oke n _b_symbol' , ' t oke n _a_amou nt ' , ' t oke n _b_amou nt ' , 'projec t ' , 'versio n ' , 'ca te gory' , ' tra der_a' , ' tra der_b' , ' t oke n _a_amou nt _raw' , ' t oke n _b_amou nt _raw' , 'usd_amou nt ' , ' t oke n _a_address' , ' t oke n _b_address' , 'excha n ge_co ntra c t _address' , ' t x_hash' , ' t x_ fr om' , ' t x_ t o' , ' tra ce_address' , 'ev t _i n dex' , ' tra de_id' ], 'resul t _se t _by tes ' : 5048 , ' t o tal _row_cou nt ' : 10 , 'da ta poi nt _cou nt ' : 220 , 'pe n di n g_ t ime_millis' : 25 , 'execu t io n _ t ime_millis' : 13 }} In most cases, you will primarily be concerned with accessing the state property in this JSON object, which in this case is QUERY_STATE_COMPLETED . Get Query Results \u00b6 Finally, let's load the results from the now-completed execution of our Query. Function Call \u00b6 response = get_query_results ( execution_id ) Lets wrap the data received from this JSON response object up into a neat pandas Dataframe. data = pd . DataFrame ( response . json ()[ 'result' ][ 'rows' ]) Output \u00b6 If everything worked smoothly, you should see your data in the data variable returned by this function: 0 2021 - 05 - 14 T15 : 17 : 39 + 00 : 00 DEX 191 \\ xf82d8ec196fb0d56c6b82a8b1870f09502a49f88 Uniswap \\ xa2b4c0af19cc16a6cfacce81f192b024d625817d 7.819632e+11 781963170639542600000 KISHU \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 ... WETH [] 1 \\ x75e29a7676717b99da65c6faad2e7644d00e2053 None \\ x75e29a7676717b99da65c6faad2e7644d00e2053 \\ x6bc05c2bc156a60c1cacfc379540ad00b7280796613b ... \\ x7a250d5630b4cf539739df2c5dacb4c659f2488d 10387.825000 2 1 2022 - 04 - 06 T07 : 01 : 37 + 00 : 00 DEX 11 \\ x6591c4bcd6d7a1eb4e537da8b78676c1576ba244 Uniswap \\ xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48 1.007936e+04 10079361085 USDC \\ x0391d2021f89dc339f60fff84546ea23e337750f ... BOND [] 1 \\ x0000006daea1723962647b7e189d311d757fb793 None \\ x0000495194ec698fcf89ccf8abb445daf01db497 \\ x8b962e59ca9f1d91e465a7af289b4b4c9c7c64c6d30d ... \\ x0000006daea1723962647b7e189d311d757fb793 10093.794730 2 2 2022 - 04 - 06 T07 : 10 : 12 + 00 : 00 DEX 438 \\ xa25b34d2ec38e338bde108c8c4040be88945d024 Uniswap \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 1.015798e-01 101579832516438100 WETH \\ x8020734a29ee290fb81992874bd1de01a16c4204 ... None [] 1 \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 None \\ xaac6fb32fd0a7a51768bddd4ac2f643445bd01af \\ x8bbaff042cea60af88fac791c4d20f84ed7d21601c41 ... \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 342.732387 2 3 2022 - 04 - 06 T07 : 10 : 12 + 00 : 00 DEX 339 \\ x8ef79d6c328c25da633559c20c75f638a4863462 Uniswap \\ xa71d0588eaf47f12b13cf8ec750430d21df04974 1.058343e+09 1058343424775444053499052032.0 QOM \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 ... WETH [] 1 \\ x7540000cab63979795c7d4b326cadbb00ed24a04 None \\ x7540000cab63979795c7d4b326cadbb00ed24a04 \\ x8bea318de386a65ac1c0c88f13e39654c3d4ec53a412 ... \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 263.520686 2 4 2022 - 04 - 06 T07 : 15 : 58 + 00 : 00 DEX 149 \\ x9c84f58bb51fabd18698efe95f5bab4f33e96e8f Uniswap \\ xb620be8a1949aa9532e6a3510132864ef9bc3f82 NaN 21168910617154070511616.0 None \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 ... WETH [] 1 \\ xdf29ee8f6d1b407808eb0270f5b128dc28303684 None \\ xdf29ee8f6d1b407808eb0270f5b128dc28303684 \\ x8bf5a55a772b3c3423ee628bd459655a1d7bd09a5c69 ... \\ xdef171fe48cf0115b1d80b88dc8eab59176fee57 675.194000 2 5 2022 - 04 - 06 T07 : 03 : 20 + 00 : 00 DEX 266 \\ x847e0b52589c9e6fa2dcc42b8ffb34ec924d4cf8 Uniswap \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 8.903535e-04 890353516515079 WETH \\ x9cf77be84214beb066f26a4ea1c38ddcc2afbcf7 ... None [] 1 \\ x7a250d5630b4cf539739df2c5dacb4c659f2488d None \\ xf2d229cc832661de2aa56249c5b7991006868522 \\ x8c00c8c20b1f3f1b447c579165c2759c688981dbc408 ... \\ x1b2cf79d0a3622f25fbe10e968b3b25a348e008b 3.004792 2 6 2021 - 05 - 17 T16 : 04 : 09 + 00 : 00 DEX 88 \\ x0d4a11d5eeaac28ec3f61d100daf4d40471f1852 Uniswap \\ xdac17f958d2ee523a2206206994597c13d831ec7 1.003227e+02 100322742 USDT \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 ... WETH [] 1 \\ x773dd321873fe70553acc295b1b49a104d968cc8 None \\ x7af55e2ab6e74f338d674537958ad236d17ab3ac \\ x6bc07c4f53719ad8d1a0f5f99d2db3699fa9dce888e3 ... \\ x8df6084e3b84a65ab9dd2325b5422e5debd8944a 100.372301 2 7 2022 - 04 - 06 T07 : 24 : 39 + 00 : 00 DEX 219 \\ xaa51ea59c985a92ce881517a8896931d4a86e9e3 Uniswap \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 3.214029e-01 321402936315917950 WETH \\ x4846b0cce69121e4d25b6efe7738eaf27bca7e7f ... None [] 1 \\ x7a250d5630b4cf539739df2c5dacb4c659f2488d None \\ xa053dbafba05e307a7bddede09c7feb235dc34b1 \\ x8c86abc9c4eaff2b8de48351360781bc153cd16fa108 ... \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 1084.606349 2 8 2021 - 05 - 17 T16 : 04 : 09 + 00 : 00 DEX 91 \\ x773dd321873fe70553acc295b1b49a104d968cc8 Uniswap \\ x95ad61b0a150d79219dcf64e1e6cc01f0b64c4ce 6.477303e+06 6477302710423104532774912.0 SHIB \\ xdac17f958d2ee523a2206206994597c13d831ec7 ... USDT [] 1 \\ x8df6084e3b84a65ab9dd2325b5422e5debd8944a None \\ x7af55e2ab6e74f338d674537958ad236d17ab3ac \\ x6bc07c4f53719ad8d1a0f5f99d2db3699fa9dce888e3 ... \\ x8df6084e3b84a65ab9dd2325b5422e5debd8944a 103.636843 2 9 2022 - 04 - 06 T07 : 24 : 39 + 00 : 00 DEX 234 \\ xaa51ea59c985a92ce881517a8896931d4a86e9e3 Uniswap \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 1.127058e-01 112705776325968480 WETH \\ x4846b0cce69121e4d25b6efe7738eaf27bca7e7f ... None [] 1 \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 None \\ xa053dbafba05e307a7bddede09c7feb235dc34b1 \\ x8c86abc9c4eaff2b8de48351360781bc153cd16fa108 ... \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 380.336913 2 So you now have data from your Dune query. In a table. In Python. \ud83e\uddd9\ud83e\ude84 Cancel Query Execution \u00b6 Some queries can take a long time to execute (minutes). Depending on your workflow, you may want to interrupt execution at times. Here's how to do that: response = cancel_query_execution ( execution_id ) When you have a running Query and call this function, you'll get a response object returned to you confirming the cancellation of query execution. Parameterized Queries \u00b6 Only one step changes when you are working with parameterized queries - you need to pass query parameters to the execution endpoint of our API. There is no change to working with rest of the endpoints after this step. So let's define a function execute_query_with_params to call the execute endpoint for parameterized queries: def execute_query_with_params ( query_id , param_dict ): \"\"\" Takes in the query ID. And a dictionary containing parameter values. Calls the API to execute the query. Returns the execution ID of the instance which is executing the query. \"\"\" url = make_api_url ( \"query\" , \"execute\" , query_id ) response = post ( url , headers = HEADER , json = { \"query_parameters\" : param_dict }) execution_id = response . json ()[ 'execution_id' ] return execution_id Create a Dictionary of parameters \u00b6 For our example, we're creating a dictionary with just one key, the wallet_address , for use in a query that returns the total amount spent on gas from a given wallet_address : parameters = { \"wallet_address\" : \"0xb10f35351ff21bb81dc02d4fd901ac5ae34e8dc4\" } Pass the parameters dictionary to the execution endpoint \u00b6 Now let's make use of the function that we just defined to achieve this: execution_id = execute_query_with_params ( \"638435\" , parameters ) And that is it! Once you get the execution_id from this POST endpoint, you can use it with all the GET endpoints of the API, just like you would with a simple query without parameters. Complete-code The complete code for this tutorial is available on this link .","title":"Python"},{"location":"api/quick-start/api-py/#getting-set-up","text":"We'll primarily be working with the requests library to access the API, so let's install it: pip install requests We'll also use pandas to load the data returned from APIs into a neat DataFrame (table), and jupyter notebooks to have a nice interactive interface to do all of this. So let us install these as well: pip install pandas pip install jupyter notebook We recommend following rest of the Quick Start in a jupyter notebook. You can start the interface with this simple command: jupyter notebook","title":"Getting Set Up"},{"location":"api/quick-start/api-py/#import-the-necessary-libraries","text":"from requests import get , post import pandas as pd","title":"Import the necessary libraries"},{"location":"api/quick-start/api-py/#api-keys","text":"Any call you make to the Dune API will require you to pass your API key with your call's header: API_KEY = \"YOUR_API_KEY\" HEADER = { \"x-dune-api-key\" : API_KEY }","title":"API Keys"},{"location":"api/quick-start/api-py/#simplifying-url-generation","text":"Though not a necessary step, using this function will make it easier to generate URLs for different API endpoints: BASE_URL = \"https://api.dune.com/api/v1/\" def make_api_url ( module , action , ID ): \"\"\" We shall use this function to generate a URL to call the API. \"\"\" url = BASE_URL + module + \"/\" + ID + \"/\" + action return url","title":"Simplifying URL generation"},{"location":"api/quick-start/api-py/#wrapping-api-endpoints-in-functions","text":"The Dune API currently has four primary end points as documented in the API Reference section. We are going to wrap these up in neat functions which will make using the Dune API as easy as a flick of the \ud83e\ude84: def execute_query ( query_id ): \"\"\" Takes in the query ID. Calls the API to execute the query. Returns the execution ID of the instance which is executing the query. \"\"\" url = make_api_url ( \"query\" , \"execute\" , query_id ) response = post ( url , headers = HEADER ) execution_id = response . json ()[ 'execution_id' ] return execution_id def get_query_status ( execution_id ): \"\"\" Takes in an execution ID. Fetches the status of query execution using the API Returns the status response object \"\"\" url = make_api_url ( \"execution\" , \"status\" , execution_id ) response = get ( url , headers = HEADER ) return response def get_query_results ( execution_id ): \"\"\" Takes in an execution ID. Fetches the results returned from the query using the API Returns the results response object \"\"\" url = make_api_url ( \"execution\" , \"results\" , execution_id ) response = get ( url , headers = HEADER ) return response def cancel_query_execution ( execution_id ): \"\"\" Takes in an execution ID. Cancels the ongoing execution of the query. Returns the response object. \"\"\" url = make_api_url ( \"execution\" , \"cancel\" , execution_id ) response = get ( url , headers = HEADER ) return response","title":"Wrapping API endpoints in functions"},{"location":"api/quick-start/api-py/#using-the-dune-api","text":"","title":"Using the Dune API"},{"location":"api/quick-start/api-py/#execute-a-query","text":"To Execute a Query , you can pass any query_id from Dune that you want to fetch data from, then pass it to the execute_query function:","title":"Execute a Query"},{"location":"api/quick-start/api-py/#function-call","text":"execution_id = execute_query ( \"1258228\" )","title":"Function Call"},{"location":"api/quick-start/api-py/#output","text":"This function returns an execution_id which will look something like the sample output shown here: '01GCQKPC4QZ6Q8645C3JC4WBT1' This execution ID is the required input for rest of the API functions.","title":"Output"},{"location":"api/quick-start/api-py/#get-query-execution-status","text":"To get the Query Execution Status , take the execution_id that was returned from the execute_query function in the previous section, then pass it to get_query_status function as shown here:","title":"Get Query Execution Status"},{"location":"api/quick-start/api-py/#function-call_1","text":"response = get_query_status ( execution_id )","title":"Function Call"},{"location":"api/quick-start/api-py/#output_1","text":"The response object returned by this function will look something like the example shown here: { 'execu t io n _id' : ' 01 GCQKPC 4 QZ 6 Q 8645 C 3 JC 4 WBT 1 ' , 'query_id' : 1258228 , 's tate ' : 'QUERY_STATE_COMPLETED' , 'submi tte d_a t ' : ' 2022-09-12 T 01 : 05 : 51.781328 Z' , 'expires_a t ' : ' 2024-09-11 T 01 : 05 : 51.82013 Z' , 'execu t io n _s tarte d_a t ' : ' 2022-09-12 T 01 : 05 : 51.806752 Z' , 'execu t io n _e n ded_a t ' : ' 2022-09-12 T 01 : 05 : 51.820127 Z' , 'resul t _me ta da ta ' : { 'colum n _ na mes' : [ 'block_ t ime' , ' t oke n _a_symbol' , ' t oke n _b_symbol' , ' t oke n _a_amou nt ' , ' t oke n _b_amou nt ' , 'projec t ' , 'versio n ' , 'ca te gory' , ' tra der_a' , ' tra der_b' , ' t oke n _a_amou nt _raw' , ' t oke n _b_amou nt _raw' , 'usd_amou nt ' , ' t oke n _a_address' , ' t oke n _b_address' , 'excha n ge_co ntra c t _address' , ' t x_hash' , ' t x_ fr om' , ' t x_ t o' , ' tra ce_address' , 'ev t _i n dex' , ' tra de_id' ], 'resul t _se t _by tes ' : 5048 , ' t o tal _row_cou nt ' : 10 , 'da ta poi nt _cou nt ' : 220 , 'pe n di n g_ t ime_millis' : 25 , 'execu t io n _ t ime_millis' : 13 }} In most cases, you will primarily be concerned with accessing the state property in this JSON object, which in this case is QUERY_STATE_COMPLETED .","title":"Output"},{"location":"api/quick-start/api-py/#get-query-results","text":"Finally, let's load the results from the now-completed execution of our Query.","title":"Get Query Results"},{"location":"api/quick-start/api-py/#function-call_2","text":"response = get_query_results ( execution_id ) Lets wrap the data received from this JSON response object up into a neat pandas Dataframe. data = pd . DataFrame ( response . json ()[ 'result' ][ 'rows' ])","title":"Function Call"},{"location":"api/quick-start/api-py/#output_2","text":"If everything worked smoothly, you should see your data in the data variable returned by this function: 0 2021 - 05 - 14 T15 : 17 : 39 + 00 : 00 DEX 191 \\ xf82d8ec196fb0d56c6b82a8b1870f09502a49f88 Uniswap \\ xa2b4c0af19cc16a6cfacce81f192b024d625817d 7.819632e+11 781963170639542600000 KISHU \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 ... WETH [] 1 \\ x75e29a7676717b99da65c6faad2e7644d00e2053 None \\ x75e29a7676717b99da65c6faad2e7644d00e2053 \\ x6bc05c2bc156a60c1cacfc379540ad00b7280796613b ... \\ x7a250d5630b4cf539739df2c5dacb4c659f2488d 10387.825000 2 1 2022 - 04 - 06 T07 : 01 : 37 + 00 : 00 DEX 11 \\ x6591c4bcd6d7a1eb4e537da8b78676c1576ba244 Uniswap \\ xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48 1.007936e+04 10079361085 USDC \\ x0391d2021f89dc339f60fff84546ea23e337750f ... BOND [] 1 \\ x0000006daea1723962647b7e189d311d757fb793 None \\ x0000495194ec698fcf89ccf8abb445daf01db497 \\ x8b962e59ca9f1d91e465a7af289b4b4c9c7c64c6d30d ... \\ x0000006daea1723962647b7e189d311d757fb793 10093.794730 2 2 2022 - 04 - 06 T07 : 10 : 12 + 00 : 00 DEX 438 \\ xa25b34d2ec38e338bde108c8c4040be88945d024 Uniswap \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 1.015798e-01 101579832516438100 WETH \\ x8020734a29ee290fb81992874bd1de01a16c4204 ... None [] 1 \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 None \\ xaac6fb32fd0a7a51768bddd4ac2f643445bd01af \\ x8bbaff042cea60af88fac791c4d20f84ed7d21601c41 ... \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 342.732387 2 3 2022 - 04 - 06 T07 : 10 : 12 + 00 : 00 DEX 339 \\ x8ef79d6c328c25da633559c20c75f638a4863462 Uniswap \\ xa71d0588eaf47f12b13cf8ec750430d21df04974 1.058343e+09 1058343424775444053499052032.0 QOM \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 ... WETH [] 1 \\ x7540000cab63979795c7d4b326cadbb00ed24a04 None \\ x7540000cab63979795c7d4b326cadbb00ed24a04 \\ x8bea318de386a65ac1c0c88f13e39654c3d4ec53a412 ... \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 263.520686 2 4 2022 - 04 - 06 T07 : 15 : 58 + 00 : 00 DEX 149 \\ x9c84f58bb51fabd18698efe95f5bab4f33e96e8f Uniswap \\ xb620be8a1949aa9532e6a3510132864ef9bc3f82 NaN 21168910617154070511616.0 None \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 ... WETH [] 1 \\ xdf29ee8f6d1b407808eb0270f5b128dc28303684 None \\ xdf29ee8f6d1b407808eb0270f5b128dc28303684 \\ x8bf5a55a772b3c3423ee628bd459655a1d7bd09a5c69 ... \\ xdef171fe48cf0115b1d80b88dc8eab59176fee57 675.194000 2 5 2022 - 04 - 06 T07 : 03 : 20 + 00 : 00 DEX 266 \\ x847e0b52589c9e6fa2dcc42b8ffb34ec924d4cf8 Uniswap \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 8.903535e-04 890353516515079 WETH \\ x9cf77be84214beb066f26a4ea1c38ddcc2afbcf7 ... None [] 1 \\ x7a250d5630b4cf539739df2c5dacb4c659f2488d None \\ xf2d229cc832661de2aa56249c5b7991006868522 \\ x8c00c8c20b1f3f1b447c579165c2759c688981dbc408 ... \\ x1b2cf79d0a3622f25fbe10e968b3b25a348e008b 3.004792 2 6 2021 - 05 - 17 T16 : 04 : 09 + 00 : 00 DEX 88 \\ x0d4a11d5eeaac28ec3f61d100daf4d40471f1852 Uniswap \\ xdac17f958d2ee523a2206206994597c13d831ec7 1.003227e+02 100322742 USDT \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 ... WETH [] 1 \\ x773dd321873fe70553acc295b1b49a104d968cc8 None \\ x7af55e2ab6e74f338d674537958ad236d17ab3ac \\ x6bc07c4f53719ad8d1a0f5f99d2db3699fa9dce888e3 ... \\ x8df6084e3b84a65ab9dd2325b5422e5debd8944a 100.372301 2 7 2022 - 04 - 06 T07 : 24 : 39 + 00 : 00 DEX 219 \\ xaa51ea59c985a92ce881517a8896931d4a86e9e3 Uniswap \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 3.214029e-01 321402936315917950 WETH \\ x4846b0cce69121e4d25b6efe7738eaf27bca7e7f ... None [] 1 \\ x7a250d5630b4cf539739df2c5dacb4c659f2488d None \\ xa053dbafba05e307a7bddede09c7feb235dc34b1 \\ x8c86abc9c4eaff2b8de48351360781bc153cd16fa108 ... \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 1084.606349 2 8 2021 - 05 - 17 T16 : 04 : 09 + 00 : 00 DEX 91 \\ x773dd321873fe70553acc295b1b49a104d968cc8 Uniswap \\ x95ad61b0a150d79219dcf64e1e6cc01f0b64c4ce 6.477303e+06 6477302710423104532774912.0 SHIB \\ xdac17f958d2ee523a2206206994597c13d831ec7 ... USDT [] 1 \\ x8df6084e3b84a65ab9dd2325b5422e5debd8944a None \\ x7af55e2ab6e74f338d674537958ad236d17ab3ac \\ x6bc07c4f53719ad8d1a0f5f99d2db3699fa9dce888e3 ... \\ x8df6084e3b84a65ab9dd2325b5422e5debd8944a 103.636843 2 9 2022 - 04 - 06 T07 : 24 : 39 + 00 : 00 DEX 234 \\ xaa51ea59c985a92ce881517a8896931d4a86e9e3 Uniswap \\ xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2 1.127058e-01 112705776325968480 WETH \\ x4846b0cce69121e4d25b6efe7738eaf27bca7e7f ... None [] 1 \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 None \\ xa053dbafba05e307a7bddede09c7feb235dc34b1 \\ x8c86abc9c4eaff2b8de48351360781bc153cd16fa108 ... \\ x68b3465833fb72a70ecdf485e0e4c7bd8665fc45 380.336913 2 So you now have data from your Dune query. In a table. In Python. \ud83e\uddd9\ud83e\ude84","title":"Output"},{"location":"api/quick-start/api-py/#cancel-query-execution","text":"Some queries can take a long time to execute (minutes). Depending on your workflow, you may want to interrupt execution at times. Here's how to do that: response = cancel_query_execution ( execution_id ) When you have a running Query and call this function, you'll get a response object returned to you confirming the cancellation of query execution.","title":"Cancel Query Execution"},{"location":"api/quick-start/api-py/#parameterized-queries","text":"Only one step changes when you are working with parameterized queries - you need to pass query parameters to the execution endpoint of our API. There is no change to working with rest of the endpoints after this step. So let's define a function execute_query_with_params to call the execute endpoint for parameterized queries: def execute_query_with_params ( query_id , param_dict ): \"\"\" Takes in the query ID. And a dictionary containing parameter values. Calls the API to execute the query. Returns the execution ID of the instance which is executing the query. \"\"\" url = make_api_url ( \"query\" , \"execute\" , query_id ) response = post ( url , headers = HEADER , json = { \"query_parameters\" : param_dict }) execution_id = response . json ()[ 'execution_id' ] return execution_id","title":"Parameterized Queries"},{"location":"api/quick-start/api-py/#create-a-dictionary-of-parameters","text":"For our example, we're creating a dictionary with just one key, the wallet_address , for use in a query that returns the total amount spent on gas from a given wallet_address : parameters = { \"wallet_address\" : \"0xb10f35351ff21bb81dc02d4fd901ac5ae34e8dc4\" }","title":"Create a Dictionary of parameters"},{"location":"api/quick-start/api-py/#pass-the-parameters-dictionary-to-the-execution-endpoint","text":"Now let's make use of the function that we just defined to achieve this: execution_id = execute_query_with_params ( \"638435\" , parameters ) And that is it! Once you get the execution_id from this POST endpoint, you can use it with all the GET endpoints of the API, just like you would with a simple query without parameters. Complete-code The complete code for this tutorial is available on this link .","title":"Pass the parameters dictionary to the execution endpoint"},{"location":"api/quick-start/api-ready-queries/","text":"Here we have four queries, you can use the query ID from the URL in any of the API guides . Or, you can fork the query and change it however you like - then use that new query ID in the API. Get the ERC20 balances for a given address \u00b6 The query ID is 1616880 . Parameters Description Valid Choices address The address that you would like to get balances for must be a valid EVM address blocknumber The cutoff block for checking balances 0 if you want most recent block, otherwise any blocknumber that has been processed will work (~3 minute/15 block delay) chain The EVM chain you'd like to check balances for ethereum , polygon , bnb , optimism , arbitrum , avalanche_c , gnosis dust Keep or remove dust tokens (worth less than $0.01) keep or remove Output columns Output Column Description symbol the token symbol, if we have it notional_value the notional amount of tokens held, rounded 5 decimals total_value the $USD value of tokens held, rounded 3 decimals token_price the $USD price of the token Get all the holders and their balances for a given ERC20 address \u00b6 The query ID is 1618116 . Parameters Description Valid Choices address The ERC20 token address you would like to get holders of must be a valid EVM address blocknumber The cutoff block for checking balances 0 if you want most recent block, otherwise any blocknumber that has been processed will work (~3 minute/15 block delay) chain The EVM chain you'd like to check balances for ethereum , polygon , bnb , optimism , arbitrum , avalanche_c , gnosis Output columns Output Column Description holder the address of the holder holder_ens the ens of the holder address, if any notional_value the notional amount of tokens held, rounded 5 decimals total_value the $USD value of tokens held, rounded 3 decimals token_price the $USD price of the token Get the NFT balances for a given address \u00b6 The query ID is 1617158 . Parameters Description Valid Choices address The address that you would like to get balances for must be a valid EVM address blocknumber The cutoff block for checking balances 0 if you want most recent block, otherwise any blocknumber that has been processed will work (~3 minute/15 block delay) chain The EVM chain you'd like to check balances for ethereum , polygon , bnb , optimism , arbitrum , avalanche_c , gnosis Output columns Output Column Description symbol the symbol of the NFT, if we have it name the name of the NFT, if we have it category the category of the NFT, if we have it token_id the token_id of the NFT contract_address the contract_address of the NFT acquired_how was it minted or transfered/bought acquired_on_block_number the block_number that the NFT was received on Get all the holders and their balances for a given NFT address \u00b6 The query ID is 1618122 . Parameters Description Valid Choices address The NFT address that you would like to holders of must be a valid EVM address blocknumber The cutoff block for checking balances 0 if you want most recent block, otherwise any blocknumber that has been processed will work (~3 minute/15 block delay) chain The EVM chain you'd like to check balances for ethereum , polygon , bnb , optimism , arbitrum , avalanche_c , gnosis Output columns Output Column Description holder the address of the holder holder_ens the ens of the holder address, if any tokens_held how many NFTs from this contract is held token_ids an array of all the token ids held","title":"API Ready Queries"},{"location":"api/quick-start/api-ready-queries/#get-the-erc20-balances-for-a-given-address","text":"The query ID is 1616880 . Parameters Description Valid Choices address The address that you would like to get balances for must be a valid EVM address blocknumber The cutoff block for checking balances 0 if you want most recent block, otherwise any blocknumber that has been processed will work (~3 minute/15 block delay) chain The EVM chain you'd like to check balances for ethereum , polygon , bnb , optimism , arbitrum , avalanche_c , gnosis dust Keep or remove dust tokens (worth less than $0.01) keep or remove Output columns Output Column Description symbol the token symbol, if we have it notional_value the notional amount of tokens held, rounded 5 decimals total_value the $USD value of tokens held, rounded 3 decimals token_price the $USD price of the token","title":"Get the ERC20 balances for a given address"},{"location":"api/quick-start/api-ready-queries/#get-all-the-holders-and-their-balances-for-a-given-erc20-address","text":"The query ID is 1618116 . Parameters Description Valid Choices address The ERC20 token address you would like to get holders of must be a valid EVM address blocknumber The cutoff block for checking balances 0 if you want most recent block, otherwise any blocknumber that has been processed will work (~3 minute/15 block delay) chain The EVM chain you'd like to check balances for ethereum , polygon , bnb , optimism , arbitrum , avalanche_c , gnosis Output columns Output Column Description holder the address of the holder holder_ens the ens of the holder address, if any notional_value the notional amount of tokens held, rounded 5 decimals total_value the $USD value of tokens held, rounded 3 decimals token_price the $USD price of the token","title":"Get all the holders and their balances for a given ERC20 address"},{"location":"api/quick-start/api-ready-queries/#get-the-nft-balances-for-a-given-address","text":"The query ID is 1617158 . Parameters Description Valid Choices address The address that you would like to get balances for must be a valid EVM address blocknumber The cutoff block for checking balances 0 if you want most recent block, otherwise any blocknumber that has been processed will work (~3 minute/15 block delay) chain The EVM chain you'd like to check balances for ethereum , polygon , bnb , optimism , arbitrum , avalanche_c , gnosis Output columns Output Column Description symbol the symbol of the NFT, if we have it name the name of the NFT, if we have it category the category of the NFT, if we have it token_id the token_id of the NFT contract_address the contract_address of the NFT acquired_how was it minted or transfered/bought acquired_on_block_number the block_number that the NFT was received on","title":"Get the NFT balances for a given address"},{"location":"api/quick-start/api-ready-queries/#get-all-the-holders-and-their-balances-for-a-given-nft-address","text":"The query ID is 1618122 . Parameters Description Valid Choices address The NFT address that you would like to holders of must be a valid EVM address blocknumber The cutoff block for checking balances 0 if you want most recent block, otherwise any blocknumber that has been processed will work (~3 minute/15 block delay) chain The EVM chain you'd like to check balances for ethereum , polygon , bnb , optimism , arbitrum , avalanche_c , gnosis Output columns Output Column Description holder the address of the holder holder_ens the ens of the holder address, if any tokens_held how many NFTs from this contract is held token_ids an array of all the token ids held","title":"Get all the holders and their balances for a given NFT address"},{"location":"api/quick-start/community-clients/","text":"Our community once again comes through for the win with the below API clients. If you've built your own let us know about it in our #dune-api Discord Channel ! Disclaimer While we love that our community has taken the lead, these clients are not directly maintained by the Dune team. Cow Protocol Python Client \u00b6 Built by @bh2smith and the team at Cow Protocol , you can find this client on PyPi . Get started quickly with a simple pip install command: pip install dune-client And if you want to learn more about how it's built and works, check out the client's GitHub page here . Cow Protocol Node.js Client \u00b6 @bh2smith also built a Node.js client you can find here . Install it like this: yarn add @cowprotocol/ts-dune-client","title":"Community Clients"},{"location":"api/quick-start/community-clients/#cow-protocol-python-client","text":"Built by @bh2smith and the team at Cow Protocol , you can find this client on PyPi . Get started quickly with a simple pip install command: pip install dune-client And if you want to learn more about how it's built and works, check out the client's GitHub page here .","title":"Cow Protocol Python Client"},{"location":"api/quick-start/community-clients/#cow-protocol-nodejs-client","text":"@bh2smith also built a Node.js client you can find here . Install it like this: yarn add @cowprotocol/ts-dune-client","title":"Cow Protocol Node.js Client"},{"location":"getting-started/","text":"Getting Started is the place to get oriented and learn how to use Dune! Dune is made for technical and non-technical users alike. \u00b6 Whether you are a seasoned SQL developer, blockchain researcher, business analyst or none of the above - you can use Dune to start analyzing Blockchain data in an instant. With the help of our community, we've created a lot of helpful guides and tools to aid you in your journey of becoming a full-fledged Dune Wizard. The absolute quickest way to get started with Dune is to follow along with our Getting Started Video Series here and to check out our Getting Started Dashboard here . The next steps to dive deeper depend on whether you: Already know SQL Are more of a just-in-case or just-in-time learner. Just-in-Case Just-in-Time Don't Know SQL? Just-in-Case learning is school-style - read and watch a lot of content up front to get a general understanding, then start experimenting with doing. If this is your preferred learning style, our long form Dune Guides go a lot deeper than our videos while still guiding you through simple projects to give you a general understanding of how to create with Dune. OurNetwork Course is also provides a thorough, broad overview. Just-in-Time learning is for those who learn by doing. If you already have an idea of what you want to do with Dune and just need to unblock yourself by finding some specific tactical knowledge, Core Features likely contains the information you need. Tables and Spellbook are more advanced features worth experimenting with once you've understood the basics. Be sure to ask whatever questions you have in the #beginners Discord channel ! If you're not very familiar with SQL, start by exploring our SQL Guides here . What do you need to know to Become a Great Wizard? \u00b6 How to Use SQL \u00b6 A fundamental understanding of SQL is needed to be able to successfully query for data on Dune. SQL is widely used in the Software Development industry and you can find a lot of non-Dune specific documentation about it. This often times helps with answering Query related questions since most answers can easily be found in the internet. Note By default, this documentation shows information for our Dune V1 Engine, which runs on a PostgreSQL database. Our new data platform, Dune Engine V2 is currently in beta with a Databricks SQL query engine. It offers exciting features like better scaling, cross chain Queries and Spellbook . Dune Engine V2 will become the default over the next few months, so we recommend you try it out! Basic SQL PostgreSQL Databricks SQL If you're not very familiar with SQL in general, we recommend starting with our SQL Guides . The official PostgreSQL documentation is great. Dune runs on PostgreSQL 12.2. The official Databricks SQL documentation is super helpful. How to parse Ethereum Virtual Machine data \u00b6 The data you will find on chain and in Dune's database is almost all pulled from Ethereum Virtual Machine based environments. Thus, understanding how the Ethereum Virtual Machine and smart contracts work as a whole is an important foundation for being able to find, understand, and use much of the data available in Dune. If you are able to read most of the data in Etherscan, you're already well on your way to being able to create insightful Queries and Visualizations with Dune. Unfortunately we haven't yet found one great resource we can point you to currently as each smart contract has it's own rules. We have written up a few words on this in our section on Decoded Data . What communities, protocols, and businesses care about \u00b6 This is something that might come as a surprise to you but a key component of being an effective data-analyzing Wizard is understanding how to separate \"signal from noise.\" Some data is interesting and valuable, some data isn't. Knowing how to surface the interesting bits by making them clear and easy to understand is a fundamental part of becoming a great data-analyzing Wizard. Ask yourself: What Data is interesting and needed for my audience/community/project/company to make better decisions? There are thousands of ways to go about finding metrics that are interesting, though talking with the community or founders is usually the best starting place. How to find a freelancer to help you build Dune Dashboards \u00b6 There are quite a few people in the crypto industry who either specialize in building on Dune or have the necessary skills to quickly get up to speed on the particulars. To reach out to this pool of freelancers, you can fill out this questionnaire and hopefully freelancers will get back to you in little to no time. If that yields no results, posting the bounty on relevant social channels and spreading it in your networks may help. If hiring a freelancer for the first time, please be sure to review their past work and Dashboards to verify that they are indeed capable of the kind of problem solving you need. Additional Tools and Support \u00b6 Check out our Support page to learn the best way to get help if you can't find the answers you're looking for in our docs. And give our Wizard Tools page a gander to learn more about all the awesome non-Dune tools our wizards use to make \ud83c\udf87.","title":"Getting Started"},{"location":"getting-started/#dune-is-made-for-technical-and-non-technical-users-alike","text":"Whether you are a seasoned SQL developer, blockchain researcher, business analyst or none of the above - you can use Dune to start analyzing Blockchain data in an instant. With the help of our community, we've created a lot of helpful guides and tools to aid you in your journey of becoming a full-fledged Dune Wizard. The absolute quickest way to get started with Dune is to follow along with our Getting Started Video Series here and to check out our Getting Started Dashboard here . The next steps to dive deeper depend on whether you: Already know SQL Are more of a just-in-case or just-in-time learner. Just-in-Case Just-in-Time Don't Know SQL? Just-in-Case learning is school-style - read and watch a lot of content up front to get a general understanding, then start experimenting with doing. If this is your preferred learning style, our long form Dune Guides go a lot deeper than our videos while still guiding you through simple projects to give you a general understanding of how to create with Dune. OurNetwork Course is also provides a thorough, broad overview. Just-in-Time learning is for those who learn by doing. If you already have an idea of what you want to do with Dune and just need to unblock yourself by finding some specific tactical knowledge, Core Features likely contains the information you need. Tables and Spellbook are more advanced features worth experimenting with once you've understood the basics. Be sure to ask whatever questions you have in the #beginners Discord channel ! If you're not very familiar with SQL, start by exploring our SQL Guides here .","title":"Dune is made for technical and non-technical users alike."},{"location":"getting-started/#what-do-you-need-to-know-to-become-a-great-wizard","text":"","title":"What do you need to know to Become a Great Wizard?"},{"location":"getting-started/#how-to-use-sql","text":"A fundamental understanding of SQL is needed to be able to successfully query for data on Dune. SQL is widely used in the Software Development industry and you can find a lot of non-Dune specific documentation about it. This often times helps with answering Query related questions since most answers can easily be found in the internet. Note By default, this documentation shows information for our Dune V1 Engine, which runs on a PostgreSQL database. Our new data platform, Dune Engine V2 is currently in beta with a Databricks SQL query engine. It offers exciting features like better scaling, cross chain Queries and Spellbook . Dune Engine V2 will become the default over the next few months, so we recommend you try it out! Basic SQL PostgreSQL Databricks SQL If you're not very familiar with SQL in general, we recommend starting with our SQL Guides . The official PostgreSQL documentation is great. Dune runs on PostgreSQL 12.2. The official Databricks SQL documentation is super helpful.","title":"How to Use SQL"},{"location":"getting-started/#how-to-parse-ethereum-virtual-machine-data","text":"The data you will find on chain and in Dune's database is almost all pulled from Ethereum Virtual Machine based environments. Thus, understanding how the Ethereum Virtual Machine and smart contracts work as a whole is an important foundation for being able to find, understand, and use much of the data available in Dune. If you are able to read most of the data in Etherscan, you're already well on your way to being able to create insightful Queries and Visualizations with Dune. Unfortunately we haven't yet found one great resource we can point you to currently as each smart contract has it's own rules. We have written up a few words on this in our section on Decoded Data .","title":"How to parse Ethereum Virtual Machine data"},{"location":"getting-started/#what-communities-protocols-and-businesses-care-about","text":"This is something that might come as a surprise to you but a key component of being an effective data-analyzing Wizard is understanding how to separate \"signal from noise.\" Some data is interesting and valuable, some data isn't. Knowing how to surface the interesting bits by making them clear and easy to understand is a fundamental part of becoming a great data-analyzing Wizard. Ask yourself: What Data is interesting and needed for my audience/community/project/company to make better decisions? There are thousands of ways to go about finding metrics that are interesting, though talking with the community or founders is usually the best starting place.","title":"What communities, protocols, and businesses care about"},{"location":"getting-started/#how-to-find-a-freelancer-to-help-you-build-dune-dashboards","text":"There are quite a few people in the crypto industry who either specialize in building on Dune or have the necessary skills to quickly get up to speed on the particulars. To reach out to this pool of freelancers, you can fill out this questionnaire and hopefully freelancers will get back to you in little to no time. If that yields no results, posting the bounty on relevant social channels and spreading it in your networks may help. If hiring a freelancer for the first time, please be sure to review their past work and Dashboards to verify that they are indeed capable of the kind of problem solving you need.","title":"How to find a freelancer to help you build Dune Dashboards"},{"location":"getting-started/#additional-tools-and-support","text":"Check out our Support page to learn the best way to get help if you can't find the answers you're looking for in our docs. And give our Wizard Tools page a gander to learn more about all the awesome non-Dune tools our wizards use to make \ud83c\udf87.","title":"Additional Tools and Support"},{"location":"getting-started/dashboards/","text":"Dashboards are where Dune's content lives and gets discovered. Dashboards on Dune consist of widgets. Widgets can either be Visualizations or a text box. It is also possible to embed images/GIFs inside of the text box. You can freely resize every widget to match the layout you want to create. Creating a Dashboard \u00b6 You can create a new Dashboard by navigating to our \"Discover\" page and clicking on the \"new Dashboard\" button on the right. The initial name that you give to your Dashboard will also be the URL slug. You can't change the URL slug afterwards, so be mindful of the name you choose. Changing the Dashboards display name is always possible though. Adding Visualizations \u00b6 You can simply add Visualizations to your Dashboard by going into the editor mode and clicking on the corresponding button. To go into editor mode first open one of your own Dashboards and click on the edit button on the top right. Adding text boxes \u00b6 To add text boxes to your Dashboard you have to go into editor mode first and can afterwards click on \"add text widget\". This will open a simple text editor window. Text boxes support a subset of markdown. You can manipulate text and embed images and GIFs. Text manipulation \u00b6 This is a short list to markdown syntax. A more advanced markdown guide can be found here . Element Markdown Syntax Heading # H1 ## H2 ### H3 Bold **bold text** Italic *italicized text Ordered List 1. First item 2. Second item 3. Third item Unordered List - First item - Second item - Third item Code `code` Horizontal Rule --- Link [title](https://www.example.com) Embedding Images and GIFs \u00b6 Our text boxes can also be used to embed images or GIFs into your Dashboard. The Syntax for embedding images is: Image ![alt text](image url) Since you can't store images locally on our servers, you need to upload your images somewhere else or find the raw file somewhere on the internet. In practice this might look like this: ![text](https://pbs.twimg.com/media/FEWVLQwWUAQcqLY?format=jpg&name=medium) --this is an image stored on twitters servers You can resize the image by simply resizing the widget it is contained in. You can combine images and text in one widget. Arranging the layout of the Dashboard \u00b6 You can arrange the different widgets on the Dashboard in whatever way you like. Widgets will always try to move upwards, so if you want to create a visual divider section in your Dashboard it is advised to create a big text box as a divider.","title":"Dashboards"},{"location":"getting-started/dashboards/#creating-a-dashboard","text":"You can create a new Dashboard by navigating to our \"Discover\" page and clicking on the \"new Dashboard\" button on the right. The initial name that you give to your Dashboard will also be the URL slug. You can't change the URL slug afterwards, so be mindful of the name you choose. Changing the Dashboards display name is always possible though.","title":"Creating a Dashboard"},{"location":"getting-started/dashboards/#adding-visualizations","text":"You can simply add Visualizations to your Dashboard by going into the editor mode and clicking on the corresponding button. To go into editor mode first open one of your own Dashboards and click on the edit button on the top right.","title":"Adding Visualizations"},{"location":"getting-started/dashboards/#adding-text-boxes","text":"To add text boxes to your Dashboard you have to go into editor mode first and can afterwards click on \"add text widget\". This will open a simple text editor window. Text boxes support a subset of markdown. You can manipulate text and embed images and GIFs.","title":"Adding text boxes"},{"location":"getting-started/dashboards/#text-manipulation","text":"This is a short list to markdown syntax. A more advanced markdown guide can be found here . Element Markdown Syntax Heading # H1 ## H2 ### H3 Bold **bold text** Italic *italicized text Ordered List 1. First item 2. Second item 3. Third item Unordered List - First item - Second item - Third item Code `code` Horizontal Rule --- Link [title](https://www.example.com)","title":"Text manipulation"},{"location":"getting-started/dashboards/#embedding-images-and-gifs","text":"Our text boxes can also be used to embed images or GIFs into your Dashboard. The Syntax for embedding images is: Image ![alt text](image url) Since you can't store images locally on our servers, you need to upload your images somewhere else or find the raw file somewhere on the internet. In practice this might look like this: ![text](https://pbs.twimg.com/media/FEWVLQwWUAQcqLY?format=jpg&name=medium) --this is an image stored on twitters servers You can resize the image by simply resizing the widget it is contained in. You can combine images and text in one widget.","title":"Embedding Images and GIFs"},{"location":"getting-started/dashboards/#arranging-the-layout-of-the-dashboard","text":"You can arrange the different widgets on the Dashboard in whatever way you like. Widgets will always try to move upwards, so if you want to create a visual divider section in your Dashboard it is advised to create a big text box as a divider.","title":"Arranging the layout of the Dashboard"},{"location":"getting-started/decoding-contracts/","text":"Dune contains an extensive catalog of Decoded Contracts, brought into the platform through Wizard submissions! Instead of working with raw transaction, log, and trace data, contracts are decoded into human-readable tables for each event and function defined in the smart contract's ABI ( Application Binary Interface ). Learn more about how Decoding works and what Decoded tables are available here . Submitting a new contract for decoding \u00b6 Contracts can be submitted for decoding through: The New contract form The My Creations > Contracts Tab Within the dataset explorer in the Query editor's sidebar: The contract submission form, which consists of 2 steps: 1. Blockchain and address \u00b6 We first ask for the contract's address and blockchain. Requesting this data first has two purposes: To enable us to review for potential duplicate contracts and pending submissions. To automate parts of the submission process where we can. The latter is usually accomplished by fetching potentially useful metadata from Dune and other third party sources where relevant. For instance, below here's an example of submitting the USDT contract ( 0x94b008aA00579c1307B0EF2c499aD98a8ce58e58 ) in Optimism: If we can find the contract through a third party source, we will show a green check mark next to the address field. This means we were able to fetch information such as the contract's name and ABI (Application Binary Interface). 2. Contract details \u00b6 After pressing Next, we ask for other information about the contract that we need in order to decode it: If we found the contract through other third party sources, you will only have to fill in the project name. We have some naming conventions on that, partly due to our technical setup and also to make finding data more predictable. Project Names Rules All lowercase No spaces (underscore \"_\" if needed) Added \"_v2\" or other version names at the end if applicable eg augur , tornado_cash , uniswap_v2 Once you submit it, you are done! The contract will be stored in our queue, which we manually review for quality assurance purposes. Note Submission might take a few days to get processed, please be patient with us! \ud83d\ude4f Advanced options \u00b6 In some instances, Dune can automatically detect and index multiple contract addresses under the same submission. This is useful for examples such as AMM pools where there often exists one contract instance per pair. We have two strategies for detecting other contracts for decoding: Bytecode match. We use the bytecode of the contract address in the submission to find other matches in the whole chain history. Factory instances. We find all other contracts created by the same address as the one responsible for creating the submitted contract. In both cases, we assume that all the contracts found through either method correspond to the same blockchain, project name, contract name and ABI. If you want us to index more than one contract, toggle on Advanced options and select \"Yes\" to the first question, \"Are there several instances of this contract?\" Then, to the second question - \"Is it created by a factory contract?\" - select \"No\" to index all other contracts with the same bytecode or \"Yes\" to index all other contracts originating from the same creator: Warning Only use these options if you know what you're doing and are extremely familiar with the project's architecture and deployment hierarchy. Incorrectly applying these settings may lead to a rejected submission. Tracking your submissions \u00b6 You can view your submissions and their processing status at any time by navigating to My Creations > Contracts : Frequently Asked Questions \u00b6 How do I submit contract information manually? \u00b6 Note If the contract being manually submitted is a Proxy contract, we recommend you to move on to the next section. Although we try to fetch contract information such as the ABI, sometimes this information might not be available through our sources. In those instances, you will need to manually input the contract's name and its ABI. If the contract has been verified by the chain's block explorer, you should be able to find this information there. Find a list of each chain's main block explorer here ! How do I submit a Proxy contract? \u00b6 In order to properly decode transactions towards contracts that fit the Proxy pattern , Dune needs to map the Proxy contract's address with the implementation contract's ABI. We avoid monitoring the implementation contract's address because its logic is accessed in transactions via the DelegateCall function . If we did monitor the implementation contract's address directly, we would miss out on any event logs in its logic since these are actually fired by the caller (the Proxy in this case) when calling a function through DelegateCall . Warning When submitting Proxy-patterned contracts to Dune, you should input the Proxy contract's address and, if you have it, the Implementation contract's ABI.** When you submit the Proxy contract's address, we'll attempt to fetch the proxy's contract name and the implementation address it's pointing towards to source the Implementation contract's ABI. If we can't find the Implementation contract's ABI, you'll need to find it using the relevant chain's blockchain explorer and input it manually. How do I re-submit a contract? \u00b6 Dune assumes each address in the blockchain can map to at most 1 contract. For this reason, submitting a contract with an address that already exists in [blockchain].contracts will override it for Decoding purposes. This has a couple potential dangerous side effects: If the project or contract name has changed, we will generate new tables for all of the contract's methods and events. In turn, previous tables will stop updating, data will be fragmented, and Queries will stop working. If the ABI has changed in a way that modifies an existing table's parameters, Queries that depend on such table might break or become inaccurate. If you attempt to submit a contract that already exists, we'll first present a warning note and ask you to confirm you want to proceed: Then, at the bottom of the Details page, we'll ask you to explain why you're resubmitting the contract so we can assess whether it's worth overriding the contract's data: If we believe the risk of accepting a re-submission is higher than the added value, we'll reject your resubmission. If you think we're wrong (we're only human!), feel free to reach out in our #decoding Discord channel and we'll discuss it further with you! How do I submit Diamond Proxy contracts? \u00b6 Similar to vanilla Proxy contracts, EIP-2535 contracts can be supported by passing in the address of the Diamond Proxy as well as a single ABI representing the totality of all the facets interfaces . My submission got rejected, why? \u00b6 In the interest of data quality, we reject duplicative, incorrect or low quality submissions. To avoid rejection, be sure to submit accurate contract information! \ud83d\ude4f For all other questions: \u00b6 Head over to the #decoding Discord channel and we'll be happy to help!","title":"Decoding Contracts"},{"location":"getting-started/decoding-contracts/#submitting-a-new-contract-for-decoding","text":"Contracts can be submitted for decoding through: The New contract form The My Creations > Contracts Tab Within the dataset explorer in the Query editor's sidebar: The contract submission form, which consists of 2 steps:","title":"Submitting a new contract for decoding"},{"location":"getting-started/decoding-contracts/#1-blockchain-and-address","text":"We first ask for the contract's address and blockchain. Requesting this data first has two purposes: To enable us to review for potential duplicate contracts and pending submissions. To automate parts of the submission process where we can. The latter is usually accomplished by fetching potentially useful metadata from Dune and other third party sources where relevant. For instance, below here's an example of submitting the USDT contract ( 0x94b008aA00579c1307B0EF2c499aD98a8ce58e58 ) in Optimism: If we can find the contract through a third party source, we will show a green check mark next to the address field. This means we were able to fetch information such as the contract's name and ABI (Application Binary Interface).","title":"1. Blockchain and address"},{"location":"getting-started/decoding-contracts/#2-contract-details","text":"After pressing Next, we ask for other information about the contract that we need in order to decode it: If we found the contract through other third party sources, you will only have to fill in the project name. We have some naming conventions on that, partly due to our technical setup and also to make finding data more predictable. Project Names Rules All lowercase No spaces (underscore \"_\" if needed) Added \"_v2\" or other version names at the end if applicable eg augur , tornado_cash , uniswap_v2 Once you submit it, you are done! The contract will be stored in our queue, which we manually review for quality assurance purposes. Note Submission might take a few days to get processed, please be patient with us! \ud83d\ude4f","title":"2. Contract details"},{"location":"getting-started/decoding-contracts/#advanced-options","text":"In some instances, Dune can automatically detect and index multiple contract addresses under the same submission. This is useful for examples such as AMM pools where there often exists one contract instance per pair. We have two strategies for detecting other contracts for decoding: Bytecode match. We use the bytecode of the contract address in the submission to find other matches in the whole chain history. Factory instances. We find all other contracts created by the same address as the one responsible for creating the submitted contract. In both cases, we assume that all the contracts found through either method correspond to the same blockchain, project name, contract name and ABI. If you want us to index more than one contract, toggle on Advanced options and select \"Yes\" to the first question, \"Are there several instances of this contract?\" Then, to the second question - \"Is it created by a factory contract?\" - select \"No\" to index all other contracts with the same bytecode or \"Yes\" to index all other contracts originating from the same creator: Warning Only use these options if you know what you're doing and are extremely familiar with the project's architecture and deployment hierarchy. Incorrectly applying these settings may lead to a rejected submission.","title":"Advanced options"},{"location":"getting-started/decoding-contracts/#tracking-your-submissions","text":"You can view your submissions and their processing status at any time by navigating to My Creations > Contracts :","title":"Tracking your submissions"},{"location":"getting-started/decoding-contracts/#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"getting-started/decoding-contracts/#how-do-i-submit-contract-information-manually","text":"Note If the contract being manually submitted is a Proxy contract, we recommend you to move on to the next section. Although we try to fetch contract information such as the ABI, sometimes this information might not be available through our sources. In those instances, you will need to manually input the contract's name and its ABI. If the contract has been verified by the chain's block explorer, you should be able to find this information there. Find a list of each chain's main block explorer here !","title":"How do I submit contract information manually?"},{"location":"getting-started/decoding-contracts/#how-do-i-submit-a-proxy-contract","text":"In order to properly decode transactions towards contracts that fit the Proxy pattern , Dune needs to map the Proxy contract's address with the implementation contract's ABI. We avoid monitoring the implementation contract's address because its logic is accessed in transactions via the DelegateCall function . If we did monitor the implementation contract's address directly, we would miss out on any event logs in its logic since these are actually fired by the caller (the Proxy in this case) when calling a function through DelegateCall . Warning When submitting Proxy-patterned contracts to Dune, you should input the Proxy contract's address and, if you have it, the Implementation contract's ABI.** When you submit the Proxy contract's address, we'll attempt to fetch the proxy's contract name and the implementation address it's pointing towards to source the Implementation contract's ABI. If we can't find the Implementation contract's ABI, you'll need to find it using the relevant chain's blockchain explorer and input it manually.","title":"How do I submit a Proxy contract?"},{"location":"getting-started/decoding-contracts/#how-do-i-re-submit-a-contract","text":"Dune assumes each address in the blockchain can map to at most 1 contract. For this reason, submitting a contract with an address that already exists in [blockchain].contracts will override it for Decoding purposes. This has a couple potential dangerous side effects: If the project or contract name has changed, we will generate new tables for all of the contract's methods and events. In turn, previous tables will stop updating, data will be fragmented, and Queries will stop working. If the ABI has changed in a way that modifies an existing table's parameters, Queries that depend on such table might break or become inaccurate. If you attempt to submit a contract that already exists, we'll first present a warning note and ask you to confirm you want to proceed: Then, at the bottom of the Details page, we'll ask you to explain why you're resubmitting the contract so we can assess whether it's worth overriding the contract's data: If we believe the risk of accepting a re-submission is higher than the added value, we'll reject your resubmission. If you think we're wrong (we're only human!), feel free to reach out in our #decoding Discord channel and we'll discuss it further with you!","title":"How do I re-submit a contract?"},{"location":"getting-started/decoding-contracts/#how-do-i-submit-diamond-proxy-contracts","text":"Similar to vanilla Proxy contracts, EIP-2535 contracts can be supported by passing in the address of the Diamond Proxy as well as a single ABI representing the totality of all the facets interfaces .","title":"How do I submit Diamond Proxy contracts?"},{"location":"getting-started/decoding-contracts/#my-submission-got-rejected-why","text":"In the interest of data quality, we reject duplicative, incorrect or low quality submissions. To avoid rejection, be sure to submit accurate contract information! \ud83d\ude4f","title":"My submission got rejected, why?"},{"location":"getting-started/decoding-contracts/#for-all-other-questions","text":"Head over to the #decoding Discord channel and we'll be happy to help!","title":"For all other questions:"},{"location":"getting-started/embeds/","text":"Screenshots are boring tech of the past. To save you from having to take screenshots that might not look so great but will definitely be out of date a few minutes after you take them, we've built a native embed function that works across most web platforms. You can generate embed links by clicking on any query title and selecting the embed function in the top right corner. Note The embed button works as a stand alone link and as a way to embed your live graphs into websites/apps. If a Query has no Visualizations, the link will be to the Query Results table. If you have multiple Visualizations, the link will be for whichever Visualization you've selected when you clicked the Embed button. Using Embeds on different platforms \u00b6 Twitter \u00b6 Twitter renders and updates Dune Visualizations automatically! Simply paste your embed link and let the magic happen. \ud83e\ude84 Discord \u00b6 Dune embeds work very well in Discord, simply drop the embed link in the chat and the corresponding Visualization will be displayed. This also lends itself very well to programming a bot to return the corresponding charts on command. Web Pages \u00b6 You can use Dune's embed links to add live Visualizations to any web page using an iframe Here is a code snippet example: <iframe src=\"https://dune.com/embeds/208941/391702/2cbe40da-a0e4-43ac-896b-fef6d4d9fda7\" height=\"500\" width=\"500\" title=\"chart 1\"></iframe> A great showcase for this is the cryptoart.io website. Mirror.xyz \u00b6 Dune Visualizations can easily be embedded into articles on mirror.xyz. Simply generate an embed link and postfix it with ?display=iframe EG: https://dune.com/embeds/208941/391702/34ee3319-1cac-40e1-a08d-160bd93693cc?display=iframe Known Issues \u00b6 Unfortunately, embeds do not work in a couple of fairly popular web platforms, including: Substack Medium GitBook Parameterized embeds \u00b6 Embed links also work with parameterized Queries, but it is a bit tricky to get them to work: The embed link you generate won't include the necessary parameters yet, even if you ran the Query with them. We are working on automating this, but for now you'll need to manually prefix the parameter link with the parameters: link?[name_of_parameter_1]=[xxxx]&?[name_of_parameter_2]=[yyyy]&[...] EG: https://dune.com/embeds/118220/238460/aa002dd3-f9e2-4d63-86c8-b765569306c6NFT?address=0xff9c1b15b16263c61d017ee9f65c50e4ae0113d7&rolling_n_trades=500","title":"Embeds"},{"location":"getting-started/embeds/#using-embeds-on-different-platforms","text":"","title":"Using Embeds on different platforms"},{"location":"getting-started/embeds/#twitter","text":"Twitter renders and updates Dune Visualizations automatically! Simply paste your embed link and let the magic happen. \ud83e\ude84","title":"Twitter"},{"location":"getting-started/embeds/#discord","text":"Dune embeds work very well in Discord, simply drop the embed link in the chat and the corresponding Visualization will be displayed. This also lends itself very well to programming a bot to return the corresponding charts on command.","title":"Discord"},{"location":"getting-started/embeds/#web-pages","text":"You can use Dune's embed links to add live Visualizations to any web page using an iframe Here is a code snippet example: <iframe src=\"https://dune.com/embeds/208941/391702/2cbe40da-a0e4-43ac-896b-fef6d4d9fda7\" height=\"500\" width=\"500\" title=\"chart 1\"></iframe> A great showcase for this is the cryptoart.io website.","title":"Web Pages"},{"location":"getting-started/embeds/#mirrorxyz","text":"Dune Visualizations can easily be embedded into articles on mirror.xyz. Simply generate an embed link and postfix it with ?display=iframe EG: https://dune.com/embeds/208941/391702/34ee3319-1cac-40e1-a08d-160bd93693cc?display=iframe","title":"Mirror.xyz"},{"location":"getting-started/embeds/#known-issues","text":"Unfortunately, embeds do not work in a couple of fairly popular web platforms, including: Substack Medium GitBook","title":"Known Issues"},{"location":"getting-started/embeds/#parameterized-embeds","text":"Embed links also work with parameterized Queries, but it is a bit tricky to get them to work: The embed link you generate won't include the necessary parameters yet, even if you ran the Query with them. We are working on automating this, but for now you'll need to manually prefix the parameter link with the parameters: link?[name_of_parameter_1]=[xxxx]&?[name_of_parameter_2]=[yyyy]&[...] EG: https://dune.com/embeds/118220/238460/aa002dd3-f9e2-4d63-86c8-b765569306c6NFT?address=0xff9c1b15b16263c61d017ee9f65c50e4ae0113d7&rolling_n_trades=500","title":"Parameterized embeds"},{"location":"getting-started/teams/","text":"Dune Teams are shared workspaces for Wizards to collaborate within. Why Teams? \u00b6 Creating a Team several benefits: \ud83e\udd1d Collaborate on the same content. Teams make it easy for multiple Wizards to work on shared Queries and Dashboards. \ud83d\uddc2 A separate workspace for each Team. Keep your personal work separate from the Teams you work with. Easily access your Team\u2019s Queries and Dashboards through My Creations. \ud83d\uddbc A new Team profile. Showcase all of your Team\u2019s work in one place. \ud83d\udc65 User roles. Onboard Team members as viewers, editors or admins. Getting started \u00b6 Creating a Team \u00b6 Head over to Settings > Teams to create your Team. Make sure to add a bit of \u2728 to your Team\u2019s profile. Here's ours: @dune Adding users \u00b6 You can invite other Dune Wizards to join your Team in the People section of your Team\u2019s Settings page. When you invite someone you'll need to input their Dune Username ( have them sign up here if they don't have one yet). You'll also need to assign them one of the following Roles : \ud83d\udc40 Viewer: can see the Team\u2019s content through My Creations and will be listed as a Team member in the Team page. \u270f\ufe0f Editor: in addition to the above, they can create and edit Queries under the Team\u2019s domain. \u2699\ufe0f Admin: in addition to the above, they can manage the Team and its content. Info When you invite a Wizard to join your team we'll email them a link to join. They can also directly go to Settings > Teams and accept their invite there. Team content \u00b6 Team Queries and Dashboards are created the same way they are for individual accounts. When you\u2019re prompted to save them for the first time, you can pick your Team as the Owner so your teammates can also access your new Query/Dashboard: ![Team content](images/teams-1.png Transferring content to your Team \u00b6 You can transfer any Query or a Dashboard you own to your Team by going to the Query's or Dashboard\u2019s settings and changing the owner there: ![Transferring content](images/teams-2.png Warning Once you transfer content to a Team, you will only be able to transfer it out of the Team if you are an Admin. If you accidentally transfer content to a Team you will have to ask your Team's Admin to transfer it back to you. Finding your Team\u2019s content \u00b6 Go to My Creations. Here you can see all of your personal content as well as your Team\u2019s. You can filter by owner to narrow down your search: ![Finding your Team's content](images/teams-3.png FAQ \u00b6 Can I have private content? \ud83e\udd77 We\u2019re planning to add support for private content in Teams later this year. Stay tuned! I have feedback, how do I reach out? Come join our #general-feedback Discord channel and we'll be glad to help \ud83d\ude47\u200d\u2642\ufe0f","title":"Teams"},{"location":"getting-started/teams/#why-teams","text":"Creating a Team several benefits: \ud83e\udd1d Collaborate on the same content. Teams make it easy for multiple Wizards to work on shared Queries and Dashboards. \ud83d\uddc2 A separate workspace for each Team. Keep your personal work separate from the Teams you work with. Easily access your Team\u2019s Queries and Dashboards through My Creations. \ud83d\uddbc A new Team profile. Showcase all of your Team\u2019s work in one place. \ud83d\udc65 User roles. Onboard Team members as viewers, editors or admins.","title":"Why Teams?"},{"location":"getting-started/teams/#getting-started","text":"","title":"Getting started"},{"location":"getting-started/teams/#creating-a-team","text":"Head over to Settings > Teams to create your Team. Make sure to add a bit of \u2728 to your Team\u2019s profile. Here's ours: @dune","title":"Creating a Team"},{"location":"getting-started/teams/#adding-users","text":"You can invite other Dune Wizards to join your Team in the People section of your Team\u2019s Settings page. When you invite someone you'll need to input their Dune Username ( have them sign up here if they don't have one yet). You'll also need to assign them one of the following Roles : \ud83d\udc40 Viewer: can see the Team\u2019s content through My Creations and will be listed as a Team member in the Team page. \u270f\ufe0f Editor: in addition to the above, they can create and edit Queries under the Team\u2019s domain. \u2699\ufe0f Admin: in addition to the above, they can manage the Team and its content. Info When you invite a Wizard to join your team we'll email them a link to join. They can also directly go to Settings > Teams and accept their invite there.","title":"Adding users"},{"location":"getting-started/teams/#team-content","text":"Team Queries and Dashboards are created the same way they are for individual accounts. When you\u2019re prompted to save them for the first time, you can pick your Team as the Owner so your teammates can also access your new Query/Dashboard: ![Team content](images/teams-1.png","title":"Team content"},{"location":"getting-started/teams/#transferring-content-to-your-team","text":"You can transfer any Query or a Dashboard you own to your Team by going to the Query's or Dashboard\u2019s settings and changing the owner there: ![Transferring content](images/teams-2.png Warning Once you transfer content to a Team, you will only be able to transfer it out of the Team if you are an Admin. If you accidentally transfer content to a Team you will have to ask your Team's Admin to transfer it back to you.","title":"Transferring content to your Team"},{"location":"getting-started/teams/#finding-your-teams-content","text":"Go to My Creations. Here you can see all of your personal content as well as your Team\u2019s. You can filter by owner to narrow down your search: ![Finding your Team's content](images/teams-3.png","title":"Finding your Team\u2019s content"},{"location":"getting-started/teams/#faq","text":"Can I have private content? \ud83e\udd77 We\u2019re planning to add support for private content in Teams later this year. Stay tuned! I have feedback, how do I reach out? Come join our #general-feedback Discord channel and we'll be glad to help \ud83d\ude47\u200d\u2642\ufe0f","title":"FAQ"},{"location":"getting-started/guides/dune-guides/","text":"Some of our best Wizard Community Members have produced outstanding written tutorials for Dune. These guides are all great and we recommend reading all of them, BIG THANK YOU to the Wizards who created them. You are amazing! \ud83e\ude84 The general process for surfacing data with Dune \u00b6 Compile a List of all relevant contracts and submit them for decoding . Surface the data using SQL queries Visualize the results Assemble Visualizations on a Dashboard. Make the Dashboard pretty Enjoy your data PostgreSQL Databricks SQL Andrew Hong \u00b6 Your guide to basic SQL while learning Ethereum at the same time (Part 1) Your guide to intermediate SQL while learning Ethereum at the same time (Part 2) Learning SQL and Ethereum (Part 3) SQL on Ethereum: How to Work With All the Data from a Transaction Alex Manuskin \u00b6 How to get started with querying on Dune Analytics Paul Pivat \u00b6 Lean foundational Ethereum topics with SQL Alex Kroeger \u00b6 How to use Dune Analytics like a degen Chuxin \u00b6 Select * from web3 Gracelily \u00b6 PostgreSQL Query Optimization Tricks - How to Make Queries Faster in Dune Analytics Twigblock \u00b6 Build an Ethereum Metrics Dashboard Learn to Analyze Ethereum Gas Prices 0xPhilan \u00b6 Dune Analytics: A Guide for Complete Beginners James Bachini \u00b6 Dune Analytics Tutorial | How To Create A Dune Analytics Dashboard Kirubakumaresh \u00b6 Buid Ethereum Metrics Dashboard OurNetwork Course \u00b6 Note This course is based on Dune's V1 engine. Much of the content is still applicable, but the SQL dialect and some table names have changed in Dune V2. In collaboration with the Dune Team and Community, our friends at OurNetwork created a course with an ambitious goal: teach 30 people web3 data analytics in 30 days. Hosted by some of our community's top Wizards, you can now access the presentations for free! As it covers all of the important topics you'll need to know to effectively analyze blockchain data and become a full-fledged Dune Wizard, it's one of the best places to start your Dune Journey. More details and all of the course materials can be found here: OurNetwork Course Please consider buying an edition of the Mirror post to support the teachers of this course. Videos are also available on YouTube:","title":"Dune Guides"},{"location":"getting-started/guides/dune-guides/#the-general-process-for-surfacing-data-with-dune","text":"Compile a List of all relevant contracts and submit them for decoding . Surface the data using SQL queries Visualize the results Assemble Visualizations on a Dashboard. Make the Dashboard pretty Enjoy your data PostgreSQL Databricks SQL","title":"The general process for surfacing data with Dune"},{"location":"getting-started/guides/dune-guides/#andrew-hong","text":"Your guide to basic SQL while learning Ethereum at the same time (Part 1) Your guide to intermediate SQL while learning Ethereum at the same time (Part 2) Learning SQL and Ethereum (Part 3) SQL on Ethereum: How to Work With All the Data from a Transaction","title":"Andrew Hong  "},{"location":"getting-started/guides/dune-guides/#alex-manuskin","text":"How to get started with querying on Dune Analytics","title":"Alex Manuskin "},{"location":"getting-started/guides/dune-guides/#paul-pivat","text":"Lean foundational Ethereum topics with SQL","title":"Paul Pivat "},{"location":"getting-started/guides/dune-guides/#alex-kroeger","text":"How to use Dune Analytics like a degen","title":"Alex Kroeger "},{"location":"getting-started/guides/dune-guides/#chuxin","text":"Select * from web3","title":"Chuxin "},{"location":"getting-started/guides/dune-guides/#gracelily","text":"PostgreSQL Query Optimization Tricks - How to Make Queries Faster in Dune Analytics","title":"Gracelily "},{"location":"getting-started/guides/dune-guides/#twigblock","text":"Build an Ethereum Metrics Dashboard Learn to Analyze Ethereum Gas Prices","title":"Twigblock"},{"location":"getting-started/guides/dune-guides/#0xphilan","text":"Dune Analytics: A Guide for Complete Beginners","title":"0xPhilan "},{"location":"getting-started/guides/dune-guides/#james-bachini","text":"Dune Analytics Tutorial | How To Create A Dune Analytics Dashboard","title":"James Bachini "},{"location":"getting-started/guides/dune-guides/#kirubakumaresh","text":"Buid Ethereum Metrics Dashboard","title":"Kirubakumaresh"},{"location":"getting-started/guides/dune-guides/#ournetwork-course","text":"Note This course is based on Dune's V1 engine. Much of the content is still applicable, but the SQL dialect and some table names have changed in Dune V2. In collaboration with the Dune Team and Community, our friends at OurNetwork created a course with an ambitious goal: teach 30 people web3 data analytics in 30 days. Hosted by some of our community's top Wizards, you can now access the presentations for free! As it covers all of the important topics you'll need to know to effectively analyze blockchain data and become a full-fledged Dune Wizard, it's one of the best places to start your Dune Journey. More details and all of the course materials can be found here: OurNetwork Course Please consider buying an edition of the Mirror post to support the teachers of this course. Videos are also available on YouTube:","title":"OurNetwork Course"},{"location":"getting-started/guides/sql-guides/","text":"To be a successful Dune Wizard, you'll need to have a good understanding of SQL in order to query data from Dune's database. We've compiled a few resources that we think are helpful in your journey to becoming a Dune Wizard below. Basic SQL PostgreSQL Databricks SQL Andrew Hong \u00b6 Your guide to basic SQL while learning Ethereum at the same time (Part 1) Your guide to intermediate SQL while learning Ethereum at the same time (Part 2) Learning SQL and Ethereum (Part 3) SQL on Ethereum: How to Work With All the Data from a Transaction Twigblock \u00b6 Getting Started with SQL FreecodeCamp.org Video Playlist (FREE) Amigoscode Youtube Video (FREE) UDEMY Course SQL & PostgreSQL for Beginners ($10) Mode SQL Tutorial (FREE) Introduction to SQL by DataCamp (FREE) Coursera Learn Spark SQL","title":"SQL Guides"},{"location":"getting-started/guides/sql-guides/#andrew-hong","text":"Your guide to basic SQL while learning Ethereum at the same time (Part 1) Your guide to intermediate SQL while learning Ethereum at the same time (Part 2) Learning SQL and Ethereum (Part 3) SQL on Ethereum: How to Work With All the Data from a Transaction","title":"Andrew Hong  "},{"location":"getting-started/guides/sql-guides/#twigblock","text":"Getting Started with SQL FreecodeCamp.org Video Playlist (FREE) Amigoscode Youtube Video (FREE) UDEMY Course SQL & PostgreSQL for Beginners ($10) Mode SQL Tutorial (FREE) Introduction to SQL by DataCamp (FREE) Coursera Learn Spark SQL","title":"Twigblock"},{"location":"getting-started/guides/video-tutorial/","text":"Note These videos all reference our Dune V1 Engine. While the general concepts are still applicable, some specifics related to Query execution are different in Dune Engine V2. We have produced a video tutorial series which you can follow below or on our Youtube Channel . The \u26a1 Quick 5-minute Dune Overview \u00b6 Intro to the tutorial series \u00b6 Episode 1: Dune Use cases and applications \u00b6 Episode 2: How the Dune database works \u00b6 Episode 3.1: How to use Dune Queries to recreate Fees.wtf \u00b6","title":"Video Tutorials"},{"location":"getting-started/guides/video-tutorial/#the-quick-5-minute-dune-overview","text":"","title":"The \u26a1 Quick 5-minute Dune Overview"},{"location":"getting-started/guides/video-tutorial/#intro-to-the-tutorial-series","text":"","title":"Intro to the tutorial series"},{"location":"getting-started/guides/video-tutorial/#episode-1-dune-use-cases-and-applications","text":"","title":"Episode 1: Dune Use cases and applications"},{"location":"getting-started/guides/video-tutorial/#episode-2-how-the-dune-database-works","text":"","title":"Episode 2: How the Dune database works"},{"location":"getting-started/guides/video-tutorial/#episode-31-how-to-use-dune-queries-to-recreate-feeswtf","text":"","title":"Episode 3.1: How to use Dune Queries to recreate Fees.wtf"},{"location":"getting-started/queries/","text":"Queries are the heart of Dune's magic \ud83d\udc96 \u00b6 Queries the primary building blocks Wizards use to pull blockchain data from Dune's database, transform it into something interesting with SQL functions, then and either make visualizations and dashboards to share using dune.com or just about anything imaginable via the Dune API . Through the \u2728 of Dune, Queries let Wizards answer any question! To create a new Query on dune.com , simply click New Query in the top right corner: This will take you to the Query Editor where you can build Dune Queries. The Query Editor is made up of three parts: The Data Explorer on the left The Query Window on the right The Query Results at the bottom You can change the size of each of part by dragging the corresponding Dune logo around: Let's take a look at each of part of the Query Editor in more detail in the following pages: 1. Data Explorer 2. Query Window 3. Query Results","title":"Queries"},{"location":"getting-started/queries/#queries-are-the-heart-of-dunes-magic","text":"Queries the primary building blocks Wizards use to pull blockchain data from Dune's database, transform it into something interesting with SQL functions, then and either make visualizations and dashboards to share using dune.com or just about anything imaginable via the Dune API . Through the \u2728 of Dune, Queries let Wizards answer any question! To create a new Query on dune.com , simply click New Query in the top right corner: This will take you to the Query Editor where you can build Dune Queries. The Query Editor is made up of three parts: The Data Explorer on the left The Query Window on the right The Query Results at the bottom You can change the size of each of part by dragging the corresponding Dune logo around: Let's take a look at each of part of the Query Editor in more detail in the following pages: 1. Data Explorer 2. Query Window 3. Query Results","title":"Queries are the heart of Dune's magic \ud83d\udc96"},{"location":"getting-started/queries/data-explorer/","text":"The Data Explorer empowers you to search for blockchain and other data to use in your Queries (learn about all the data Dune offers in the Tables section ). To find the data you're looking for, first select which database you want to search in: Then simply enter any keywords, protocol names, contract names, or event names into the search bar at the top. This will bring up a list of Tables containing blockchain data you can use to your Queries! \ud83e\uddd9 Advanced Searching in Dune V2 and V1 \u00b6 If you've selected one of the blockchain databases available in the dropdown, see the v1 tab below. If you've selected the Dune Engine V2 (beta) database, see the V2 tab. Learn about the differences between V1 and V2 data sets here . V2 V1 V2 \u00b6 Warning Like all of Dune V2, the V2 Data Explorer is still in beta. So it might function slightly differently than described here as we're actively testing and iterating ways to make it even better! If you have a suggestion for how to make the V2 Explorer work better, add it on our Feedback board ! If you have a suggestion for how to make these docs better, make an edit and submit a Pull Request on this page's Github ! One of the biggest changes in Dune V2 lies in our data structure - instead of having data siloed into separate databases by blockchain, everything is accessible in one data lake. One way you'll experience the power of this change is in your Queries - with V2 you can incorporate data from multiple chains in one SQL query! You'll also notice this in how searching works in the Data Explorer. To get started exploring Dune V2, select it from the dropdown list instead of one of the individual chains: Here you'll find a search bar that searches all the data inside Dune V2 and 4 different categories of dataset: Raw - just like it sounds, raw data straight from the blockchain Decoded Projects - contract calls and events from projects the Dune Team and Community have organized into nice human-readable tables. Spells - standardized project and sector data (dex.trades, nft.trades, erc20.stablecoins, etc) Community - off-chain data supplied by the web3 community. Searching Dune V2 \u00b6 When you search from the main page using keywords, you'll be searching all of Dune V2 - which is awesome for exploring all available data when you're not quite sure what you're looking for. Note The Dune V2 explorer is not case sensitive. However, since the explorer needs to look through all that data to find matches for your keywords, these searches will also have the most wait time. So if you know you're looking for data in one of the specific categories listed above, click into that category and do your searching there for better speed! Keep in mind, when you search within a specific dataset inside the V2 data explorer, you'll only find data within that set. For example, if we search for Aave at the main level, we'll find all Decoded tables as well as Aave related Spells: However, if we click into the Raw dataset, we won't find anything as this limits our search just to raw blockchain data: This functionality works recursively - when you leave the main V2 search bar, your keyword searches are limited to the specific dataset level you're currently in. To see how this works, let's start by clicking into Decoded projects and searching for Aave there: Here we've found protocols that include Aave in their name, as well as the different versions of Aave itself (just like protocols will publish groups of contracts as v1, v2, etc versions, we group the data in our decoded projects that way to keep things organized like they are on-chain). Clicking one level deeper into aave_v2 , we'll see all the contracts associated with the v2 version of Aave: See the AaveEcosystemReserve contract? Let's back up one level by clicking \"aave v2\" at the top: Then try searching AaveEcosystemReserve here: As you can see, no results were returned since we're one level above the depth that AaveEcosystemReserve is in. The same thing happens if we go further down! Clicking into the AToken contract, we'll find all the functions and events contained within that contract: But if we try searching for AaveEcosystemReserve here... No luck. Likewise, if we click back up to the aave_v2 protocol version level, with all the Aave v2 contracts, and search for the approve function that exists within the AToken contract: We won't get any results. Finally, as you may have noticed above, the Raw, Decoded Projects, and Spells datasets can all be searched by All chains or by specific chains. Just as we've seen, if you limit your search with this dropdown, you'll both get results faster and not be able to find results from other chains. Lastly, you can search within smart contracts for certain Table types, eg function and event either with the dropdown: Or for certain data types within a function or event using the search box: Icons and Labels \u00b6 First, anytime you see a double arrow >> Icon, you can click them to add that Table or data point name to your Query: We've tried to make V2 search more \u2728 with the use of icons and labels for: Which blockchain a Spell, Protocol contract set, or individual contract belongs to. Whether a piece of data/dataset is a Spell, Decoded Contract, or Community Dataset - of of which contain multiple data Tables - or an individual Table. Whether a Table within a Decoded Contract is a Function or Event. Whether a Spell set is for a Project or Sector. What type a piece of data is. Find descriptions of what all the icons and labels mean here ! In cases where the icon is different than a blockchain name next to a contract/data set, the icon is the blockchain that data is on. EG these are Polygon bridge contracts on the Ethereum Chain: V1 \u00b6 Searching by Keyword \u00b6 Searching for just uniswap will bring up all tables that contain the keyword uniswap in some form. Use spaces to create multi-keyword searches. Finding Specific Schemas \u00b6 Searching for uniswap_v2. will bring up all tables related to the uniswap_v2 schema specifically. In Dune's V1 Engine, adding \".\" at the end specifies you're looking for data in this exact schema of tables. Without the \".\" at the end you'll also find a lot of data that includes references to, in this example, uniswap_v2. . Finding Events, Calls, or Contracts \u00b6 Searching for uniswap_v2. evt will bring up only event tables related to the uniswap_v2 schema. Likewise call will bring up calls, and searching for a specific {{contractName}} will bring up all the data for that contract. Finding Specific Contract Data \u00b6 Click a Table name to see a list of all the columns inside that contract's table: Adding References to the Query Window \u00b6 To add references to the contract tables, click the >> next to the Table name: To reference specific data within a Table, click its name:","title":"The Data Explorer"},{"location":"getting-started/queries/data-explorer/#advanced-searching-in-dune-v2-and-v1","text":"If you've selected one of the blockchain databases available in the dropdown, see the v1 tab below. If you've selected the Dune Engine V2 (beta) database, see the V2 tab. Learn about the differences between V1 and V2 data sets here . V2 V1","title":"Advanced Searching in Dune V2 and V1"},{"location":"getting-started/queries/data-explorer/#v2","text":"Warning Like all of Dune V2, the V2 Data Explorer is still in beta. So it might function slightly differently than described here as we're actively testing and iterating ways to make it even better! If you have a suggestion for how to make the V2 Explorer work better, add it on our Feedback board ! If you have a suggestion for how to make these docs better, make an edit and submit a Pull Request on this page's Github ! One of the biggest changes in Dune V2 lies in our data structure - instead of having data siloed into separate databases by blockchain, everything is accessible in one data lake. One way you'll experience the power of this change is in your Queries - with V2 you can incorporate data from multiple chains in one SQL query! You'll also notice this in how searching works in the Data Explorer. To get started exploring Dune V2, select it from the dropdown list instead of one of the individual chains: Here you'll find a search bar that searches all the data inside Dune V2 and 4 different categories of dataset: Raw - just like it sounds, raw data straight from the blockchain Decoded Projects - contract calls and events from projects the Dune Team and Community have organized into nice human-readable tables. Spells - standardized project and sector data (dex.trades, nft.trades, erc20.stablecoins, etc) Community - off-chain data supplied by the web3 community.","title":"V2"},{"location":"getting-started/queries/data-explorer/#searching-dune-v2","text":"When you search from the main page using keywords, you'll be searching all of Dune V2 - which is awesome for exploring all available data when you're not quite sure what you're looking for. Note The Dune V2 explorer is not case sensitive. However, since the explorer needs to look through all that data to find matches for your keywords, these searches will also have the most wait time. So if you know you're looking for data in one of the specific categories listed above, click into that category and do your searching there for better speed! Keep in mind, when you search within a specific dataset inside the V2 data explorer, you'll only find data within that set. For example, if we search for Aave at the main level, we'll find all Decoded tables as well as Aave related Spells: However, if we click into the Raw dataset, we won't find anything as this limits our search just to raw blockchain data: This functionality works recursively - when you leave the main V2 search bar, your keyword searches are limited to the specific dataset level you're currently in. To see how this works, let's start by clicking into Decoded projects and searching for Aave there: Here we've found protocols that include Aave in their name, as well as the different versions of Aave itself (just like protocols will publish groups of contracts as v1, v2, etc versions, we group the data in our decoded projects that way to keep things organized like they are on-chain). Clicking one level deeper into aave_v2 , we'll see all the contracts associated with the v2 version of Aave: See the AaveEcosystemReserve contract? Let's back up one level by clicking \"aave v2\" at the top: Then try searching AaveEcosystemReserve here: As you can see, no results were returned since we're one level above the depth that AaveEcosystemReserve is in. The same thing happens if we go further down! Clicking into the AToken contract, we'll find all the functions and events contained within that contract: But if we try searching for AaveEcosystemReserve here... No luck. Likewise, if we click back up to the aave_v2 protocol version level, with all the Aave v2 contracts, and search for the approve function that exists within the AToken contract: We won't get any results. Finally, as you may have noticed above, the Raw, Decoded Projects, and Spells datasets can all be searched by All chains or by specific chains. Just as we've seen, if you limit your search with this dropdown, you'll both get results faster and not be able to find results from other chains. Lastly, you can search within smart contracts for certain Table types, eg function and event either with the dropdown: Or for certain data types within a function or event using the search box:","title":"Searching Dune V2"},{"location":"getting-started/queries/data-explorer/#icons-and-labels","text":"First, anytime you see a double arrow >> Icon, you can click them to add that Table or data point name to your Query: We've tried to make V2 search more \u2728 with the use of icons and labels for: Which blockchain a Spell, Protocol contract set, or individual contract belongs to. Whether a piece of data/dataset is a Spell, Decoded Contract, or Community Dataset - of of which contain multiple data Tables - or an individual Table. Whether a Table within a Decoded Contract is a Function or Event. Whether a Spell set is for a Project or Sector. What type a piece of data is. Find descriptions of what all the icons and labels mean here ! In cases where the icon is different than a blockchain name next to a contract/data set, the icon is the blockchain that data is on. EG these are Polygon bridge contracts on the Ethereum Chain:","title":"Icons and Labels"},{"location":"getting-started/queries/data-explorer/#v1","text":"","title":"V1"},{"location":"getting-started/queries/data-explorer/#searching-by-keyword","text":"Searching for just uniswap will bring up all tables that contain the keyword uniswap in some form. Use spaces to create multi-keyword searches.","title":"Searching by Keyword"},{"location":"getting-started/queries/data-explorer/#finding-specific-schemas","text":"Searching for uniswap_v2. will bring up all tables related to the uniswap_v2 schema specifically. In Dune's V1 Engine, adding \".\" at the end specifies you're looking for data in this exact schema of tables. Without the \".\" at the end you'll also find a lot of data that includes references to, in this example, uniswap_v2. .","title":"Finding Specific Schemas"},{"location":"getting-started/queries/data-explorer/#finding-events-calls-or-contracts","text":"Searching for uniswap_v2. evt will bring up only event tables related to the uniswap_v2 schema. Likewise call will bring up calls, and searching for a specific {{contractName}} will bring up all the data for that contract.","title":"Finding Events, Calls, or Contracts"},{"location":"getting-started/queries/data-explorer/#finding-specific-contract-data","text":"Click a Table name to see a list of all the columns inside that contract's table:","title":"Finding Specific Contract Data"},{"location":"getting-started/queries/data-explorer/#adding-references-to-the-query-window","text":"To add references to the contract tables, click the >> next to the Table name: To reference specific data within a Table, click its name:","title":"Adding References to the Query Window"},{"location":"getting-started/queries/parameters/","text":"What are Parameters? \u00b6 Parameters are a specialized feature of dune that allow you to implement variables in certain parts of your Query code. This variable can be changed from dashboards and therefore allows you to make an interactive dashboard. Parameters allow you to make changes to certain defined parameters of your code with a few simple clicks. For instance instead of hard coding contract_address , symbol or date ranges you can just use the parameter function to change these aspects of your code using a parameter. This allows you to build an interactive dashboard or customizable Query that the viewer can use to query for exactly the data he needs. Parameters are defined in the Query code as {{parametername}} and will appear below the Query and in any dashboards in which a Query Visualization with parameters is used in. You can pass on input to the parameter below the Query or in the parameter field on a dashboard. Simply run the Query to apply the parameter for a Query within the Query editor. On a dashboard you can either click apply all at the top or change the parameters individually and hit Enter . The Enter submission also works for dropdowns and the date picker. Parameters in a Dashboard can be shared between different Queries, just make sure to use the same name, type and default value between all of them. ![Parameters overview 1](images/parameters-overview-1.png ![Parameters overview 2](images/parameters-overview-2.png ![Parameters overview 3](images/parameters-overview-3.png How do I use Parameters? \u00b6 You can simply add a parameter to your Queries by writing {{parametername}} or using the button below the Query. You can edit the properties of single parameters by clicking on the gear wheel next to the parameter in the Query editor. This allows you to set a default value, define a list of possible parameters or change the type of of the parameter. If you want to share parameters between different Queries on a dashboard make sure they exactly match in regards to name, type and default value. ![Parameters how to use](images/parameters-how-to-use.gif Example Query \u00b6 This Query returns the running total of Gas Paid in USD. The Query Author has chosen to include a parameter for wallet address , start date and end date . with alltransactions AS ( SELECT block_time , success , gas_price / 10 ^ 9 AS gas_prices , gas_used , ( gas_price * gas_used ) / 10 ^ 18 AS eth_paid_for_tx , hash FROM ethereum . transactions WHERE \"from\" = CONCAT ( '\\x' , substring ( '{{1. Eth Address}}' from 3 )):: bytea AND block_time >= '{{2. Start Date}}' AND block_time < '{{3. End Date}}' ) SELECT date_trunc ( 'minute' , block_time ), SUM ( eth_paid_for_tx * price ) over ( ORDER BY date_trunc ( 'minute' , block_time )) AS \"Total Gas Fees Paid in USD\" FROM alltransactions LEFT JOIN ( SELECT minute , price FROM prices . usd WHERE symbol = 'WETH' AND minute > '{{2. Start Date}}' ) AS prices ON date_trunc ( 'minute' , block_time ) = minute ORDER BY block_time DESC Find this Query here Example Dashboards \u00b6 Find interesting stats on Ethereum Wallets with this dashboard: https://dune.com/kevdnlol/Transaction-Breakdown The author has included the parameters wallet address , start date and end date in this Dashboard. Drill down into the single pools of Barnbridge's Smart Yield Product: https://dune.com/0xBoxer/Barnbridge-or-Smart-Yield The Author has chosen to make the parameter poolsymbol into a drop down list here. This allows for easy access to all the relevant pools and detailed statistics on those. Find out how many people are participating in Yearn Vaults: https://dune.com/msilb7/Yearn-How-Many-Addresses-are-Participating https://dune.com/0xrusowsky/KLIMA-Wallet-Activity Find out how your investment in $KLIMA is doing: https://dune.com/0xrusowsky/KLIMA-Wallet-Activity Summary \u00b6 Parameters allow you to make a certain part of your SQL query dynamic and thereby offer you to make Queries and dashboards interactive. That way you can easily display detailed data on your dashboard since it allows the viewer to customize the dashboard for his needs. You could think of parameters like filters, but the possibilities of using this feature go beyond that.","title":"Parameters"},{"location":"getting-started/queries/parameters/#what-are-parameters","text":"Parameters are a specialized feature of dune that allow you to implement variables in certain parts of your Query code. This variable can be changed from dashboards and therefore allows you to make an interactive dashboard. Parameters allow you to make changes to certain defined parameters of your code with a few simple clicks. For instance instead of hard coding contract_address , symbol or date ranges you can just use the parameter function to change these aspects of your code using a parameter. This allows you to build an interactive dashboard or customizable Query that the viewer can use to query for exactly the data he needs. Parameters are defined in the Query code as {{parametername}} and will appear below the Query and in any dashboards in which a Query Visualization with parameters is used in. You can pass on input to the parameter below the Query or in the parameter field on a dashboard. Simply run the Query to apply the parameter for a Query within the Query editor. On a dashboard you can either click apply all at the top or change the parameters individually and hit Enter . The Enter submission also works for dropdowns and the date picker. Parameters in a Dashboard can be shared between different Queries, just make sure to use the same name, type and default value between all of them. ![Parameters overview 1](images/parameters-overview-1.png ![Parameters overview 2](images/parameters-overview-2.png ![Parameters overview 3](images/parameters-overview-3.png","title":"What are Parameters?"},{"location":"getting-started/queries/parameters/#how-do-i-use-parameters","text":"You can simply add a parameter to your Queries by writing {{parametername}} or using the button below the Query. You can edit the properties of single parameters by clicking on the gear wheel next to the parameter in the Query editor. This allows you to set a default value, define a list of possible parameters or change the type of of the parameter. If you want to share parameters between different Queries on a dashboard make sure they exactly match in regards to name, type and default value. ![Parameters how to use](images/parameters-how-to-use.gif","title":"How do I use Parameters?"},{"location":"getting-started/queries/parameters/#example-query","text":"This Query returns the running total of Gas Paid in USD. The Query Author has chosen to include a parameter for wallet address , start date and end date . with alltransactions AS ( SELECT block_time , success , gas_price / 10 ^ 9 AS gas_prices , gas_used , ( gas_price * gas_used ) / 10 ^ 18 AS eth_paid_for_tx , hash FROM ethereum . transactions WHERE \"from\" = CONCAT ( '\\x' , substring ( '{{1. Eth Address}}' from 3 )):: bytea AND block_time >= '{{2. Start Date}}' AND block_time < '{{3. End Date}}' ) SELECT date_trunc ( 'minute' , block_time ), SUM ( eth_paid_for_tx * price ) over ( ORDER BY date_trunc ( 'minute' , block_time )) AS \"Total Gas Fees Paid in USD\" FROM alltransactions LEFT JOIN ( SELECT minute , price FROM prices . usd WHERE symbol = 'WETH' AND minute > '{{2. Start Date}}' ) AS prices ON date_trunc ( 'minute' , block_time ) = minute ORDER BY block_time DESC Find this Query here","title":"Example Query"},{"location":"getting-started/queries/parameters/#example-dashboards","text":"Find interesting stats on Ethereum Wallets with this dashboard: https://dune.com/kevdnlol/Transaction-Breakdown The author has included the parameters wallet address , start date and end date in this Dashboard. Drill down into the single pools of Barnbridge's Smart Yield Product: https://dune.com/0xBoxer/Barnbridge-or-Smart-Yield The Author has chosen to make the parameter poolsymbol into a drop down list here. This allows for easy access to all the relevant pools and detailed statistics on those. Find out how many people are participating in Yearn Vaults: https://dune.com/msilb7/Yearn-How-Many-Addresses-are-Participating https://dune.com/0xrusowsky/KLIMA-Wallet-Activity Find out how your investment in $KLIMA is doing: https://dune.com/0xrusowsky/KLIMA-Wallet-Activity","title":"Example Dashboards"},{"location":"getting-started/queries/parameters/#summary","text":"Parameters allow you to make a certain part of your SQL query dynamic and thereby offer you to make Queries and dashboards interactive. That way you can easily display detailed data on your dashboard since it allows the viewer to customize the dashboard for his needs. You could think of parameters like filters, but the possibilities of using this feature go beyond that.","title":"Summary"},{"location":"getting-started/queries/query-results/","text":"The Query Results section is where you can see the data your Query returns after you run it. To get the results of your Query the first time, smash that Run button in the orange box below the Query Window: After a short time, your Query Results will appear \ud83e\ude84 as a table: Warning Queries run on the database you've currently selected in the Data Explorer dropdown. EG if you run a Query that uses Dune V1 Ethereum data, it will fail to run if you select another V1 Blockchain dataset or Dune V2: Above your Results table you'll find: The \"Query Results\" tab; if you make more Visualizations they'll appear as tabs next to this one. The New Visualization button which lets you make Visualizations out of your Query data. \"Last run\" and \"Last run took\" times so you can know how fresh your Results data is as well as a reference point for how long it might take you to Run again (could be more or less time depending on how you've modified your Query). The Run button (to re-run your Query as you change it or to). Saving Your Query \u00b6 After running your Query at least once, you'll see the Save button appear above the Query Window: Click it to see a pop up that asks you to give your Query a name, with the option to make this Query private ( ~if~ you're a Dune Pro user): After your first Save, above the Query Window you'll find: The Star button which will let you see how many stars your Query has, and star it yourself. The Embed button for you and others to embed this Query on other web pages. The Fork button for you and others to create a copy of this Query to modify to your hearts content \ud83d\udc96 Anytime you make further changes the Run button will become a Save and run button: This does what you might expect (saves your changes and re-runs your Query). Re-Running Your Query \u00b6 When you re-run your Query, your Results table will be blank until the new data is ready and loaded: The Run button will change to show: How long your Query has been running A Cancel button (in case you're tired of waiting!) Formatting Results Tables \u00b6 You can change the formatting and appearance of your Query Results tables with the options below the table. What do we have here? \ud83d\udc40 The Add to dashboard button which won't work until you've saved your Query . Learn more about making Dashboards here . A field to change your table's Title. Settings for each of your columns. Column settings are pretty straightforward: Title let's you change the column's title Align lets you change the column data's text alignment Format lets you change how numbers are formatted, more on that just below \ud83d\udc47. Hide column removes the column from your table's display Colored positive values makes your column's positive values green . Colored negative values makes your column's negative values red . Column number formatting follow this logic: Value Number format Output Description 1256784.3745 left blank 1256784,3745000 Display the full number and 7 decimals 1256784.3745 0 1256784 Only Display whole numbers 1256784.3745 0,0 1,256,784 Only displays whole numbers with comma separator 1256784.3745 0,0.00 1,256,784.38 Displays the value with decimals points according to the count of zeroes after the dot 1256784.3745 0[.]0a 1.2m Displays the value in an abbreviated format. Will display decimals of the abbreviated number according to count of zeroes after the dot. 1256784.3745 $0[.]0a $1.2m Adheres to the same methods as before, but adds a $ prefix. Visualizations \u00b6 To make Visualizations out of your Query Results, start by, you guessed it, smashing that New Visualization button (a normal click will also work if you really prefer). Then check out our Visualizations docs to learn how to make \ud83d\udcca magic!","title":"Query Results"},{"location":"getting-started/queries/query-results/#saving-your-query","text":"After running your Query at least once, you'll see the Save button appear above the Query Window: Click it to see a pop up that asks you to give your Query a name, with the option to make this Query private ( ~if~ you're a Dune Pro user): After your first Save, above the Query Window you'll find: The Star button which will let you see how many stars your Query has, and star it yourself. The Embed button for you and others to embed this Query on other web pages. The Fork button for you and others to create a copy of this Query to modify to your hearts content \ud83d\udc96 Anytime you make further changes the Run button will become a Save and run button: This does what you might expect (saves your changes and re-runs your Query).","title":"Saving Your Query"},{"location":"getting-started/queries/query-results/#re-running-your-query","text":"When you re-run your Query, your Results table will be blank until the new data is ready and loaded: The Run button will change to show: How long your Query has been running A Cancel button (in case you're tired of waiting!)","title":"Re-Running Your Query"},{"location":"getting-started/queries/query-results/#formatting-results-tables","text":"You can change the formatting and appearance of your Query Results tables with the options below the table. What do we have here? \ud83d\udc40 The Add to dashboard button which won't work until you've saved your Query . Learn more about making Dashboards here . A field to change your table's Title. Settings for each of your columns. Column settings are pretty straightforward: Title let's you change the column's title Align lets you change the column data's text alignment Format lets you change how numbers are formatted, more on that just below \ud83d\udc47. Hide column removes the column from your table's display Colored positive values makes your column's positive values green . Colored negative values makes your column's negative values red . Column number formatting follow this logic: Value Number format Output Description 1256784.3745 left blank 1256784,3745000 Display the full number and 7 decimals 1256784.3745 0 1256784 Only Display whole numbers 1256784.3745 0,0 1,256,784 Only displays whole numbers with comma separator 1256784.3745 0,0.00 1,256,784.38 Displays the value with decimals points according to the count of zeroes after the dot 1256784.3745 0[.]0a 1.2m Displays the value in an abbreviated format. Will display decimals of the abbreviated number according to count of zeroes after the dot. 1256784.3745 $0[.]0a $1.2m Adheres to the same methods as before, but adds a $ prefix.","title":"Formatting Results Tables"},{"location":"getting-started/queries/query-results/#visualizations","text":"To make Visualizations out of your Query Results, start by, you guessed it, smashing that New Visualization button (a normal click will also work if you really prefer). Then check out our Visualizations docs to learn how to make \ud83d\udcca magic!","title":"Visualizations"},{"location":"getting-started/queries/query-window/","text":"The Query window is where you work your Dune \ud83e\ude84 by inputting SQL code and running it. Autocomplete \u00b6 You can enable/disable the autocomplete function of the Query editor using the gear wheel in the top right corner: The autocomplete feature will bring up PostgreSQL keywords, as well as tables and aliases you've already included in your Query. Run Selection \u00b6 To save yourself time while testing and debugging your Queries, you can run just a part of your Query. To do this, highlight a part of your Query. You'll then see the Run button turn into a Run selection button. Click and \ud83e\ude84 You'll need to highlight a syntactically complete and correct piece of SQL otherwise you'll get an error: Shortcuts \u00b6 Here are a handful of shortcuts to make crafting Queries a \ud83d\udca8 Shortcut Action ctrl + enter execute the Query ctrl + # or / comments out the selected code ctrl + space brings up a list of keywords crtl + z undoes your last changes ctrl + y redoes your last changes ctrl + f search for keywords ctrl + h search and replace keywords These shortcuts work on US/UK Keyboards and might vary based on the language setting on your machine.","title":"The Query Window"},{"location":"getting-started/queries/query-window/#autocomplete","text":"You can enable/disable the autocomplete function of the Query editor using the gear wheel in the top right corner: The autocomplete feature will bring up PostgreSQL keywords, as well as tables and aliases you've already included in your Query.","title":"Autocomplete"},{"location":"getting-started/queries/query-window/#run-selection","text":"To save yourself time while testing and debugging your Queries, you can run just a part of your Query. To do this, highlight a part of your Query. You'll then see the Run button turn into a Run selection button. Click and \ud83e\ude84 You'll need to highlight a syntactically complete and correct piece of SQL otherwise you'll get an error:","title":"Run Selection"},{"location":"getting-started/queries/query-window/#shortcuts","text":"Here are a handful of shortcuts to make crafting Queries a \ud83d\udca8 Shortcut Action ctrl + enter execute the Query ctrl + # or / comments out the selected code ctrl + space brings up a list of keywords crtl + z undoes your last changes ctrl + y redoes your last changes ctrl + f search for keywords ctrl + h search and replace keywords These shortcuts work on US/UK Keyboards and might vary based on the language setting on your machine.","title":"Shortcuts"},{"location":"getting-started/queries/tips/","text":"Below you'll find a selection of Query related tips to help you become a more powerful \ud83e\uddd9. If you have a tip you think we should add, propose a change on this doc in our GitHub repository ! Use Spells \u00b6 The easiest way to do great analysis with Dune is to use the well-organized data you'll find in Spells (Dune V2) and Abstractions (Dune V1). Theses tables, like dex.trades , are cleaned and contain data/metadata (like human readable token symbols) that make them very straight forward to query. V1 Inline Ethereum Addresses Formatting \u00b6 Warning This feature is only available in the Dune V1 engine. In Dune Ethereum addresses are stored as PostgreSQL byte arrays which are encoded with the \\x prefix. This differs from the customary 0x prefix. If you\u2019d like to use an inline address, say to filter for a given token, you would do WHERE token = '\\x6b175474e89094c44da98b954eedeac495271d0f' which is simply short for WHERE token = '\\x6b175474e89094c44da98b954eedeac495271d0f' :: bytea Quote Column and Table Names in camelCase \u00b6 Warning This feature is only available in the Dune V1 engine. Column and table names are mostly taken directly from smart contract Application Binary Interfaces (ABIs), with no modification. Since most smart contracts are written in Solidity, and written with a camelCased naming convention, so are many of Dune\u2019s table and column names PostgreSQL (Dune V1) requires you to column and table name references are case sensitive: SELECT \u201c columnName \u201d FROM projectname . \u201d contractName_evt_EventName \u201d LIMIT 10 In PostgreSQL, double quotes are reserved for tables and columns, whereas single quotes are reserved for values: SELECT \u201c columnName \u201d FROM projectname . \u201d contratName_evt_eventName \u201d WHERE contract_address = '\\x6B175474E89094C44Da98b954EedeAC495271d0F' LIMIT 10 Schemas are always lowercase in Dune. Remove Decimals \u00b6 Ether transfers and most ERC-20 tokens have 18 decimal places, which most humans agree is too many to read. To transmute these into a more human-friendly form, use the erc20.tokens table and divide the token's transfer_value by 10: PostgreSQL Databricks SQL transfer_value / 10 ^ erc20 . tokens . decimals transfer_value / x * power ( 10 , y ) ` or ` transfer_value / x * 1 e * y Get time with date_trunc \u00b6 We\u2019ve added evt_block_time to decoded event tables for your convenience. A neat way to use it is with the date_trunc function like this: SELECT date_trunc ( 'week' , evt_block_time ) AS time You can use minute , day , week , or month . How to get USD price \u00b6 To get the USD price of on-chain activity, you typically want to JOIN the smart contract event you are looking at with the prices.usd on the minute for a given asset : LEFT JOIN prices . usd p ON p . minute = date_trunc ( 'minute' , evt_block_time ) AND event . \"asset\" = p . contract_address Then you can simply multiply the value or amount from the smart contract event with the USD price in your SELECT statement: * p.price . Token symbols \u00b6 You'll often want to group your results by token address e.g. to see volume on a DEX grouped by token. But a big list of token addresses are abstract and hard to digest! So use the token symbol instead. \ud83e\ude84 To do this, JOIN the table erc20.tokens with your event table where asset = {{token_address}} . You then select symbol in your select statement instead of token address. PostgreSQL Databricks SQL NB The erc20.tokens table contains a selection of popular tokens. If you are working with more obscure tokens you should be careful with joining with this table because tokens that are not in the coincap table might be excluded from your results. NB The tokens_blockchain.erc20 table contains a selection of popular tokens. If you are working with more obscure tokens you should be careful with joining with this table because tokens that are not in the coincap table might be excluded from your results. Filter Queries and Dashboards with Parameters \u00b6 Parameters can turn your Query or Dashboard into an app for blockchain data. Click Add parameter in the bottom right of the SQL editor on the Query editor page Double curly brackets will appear in your Query {{}} . Put the name of your parameter inside, e.g. {{token symbol}} or {{holder address}} . Note that you need to put single quotes if you want to use the parameter in your Query WHERE token = '{{token symbol}}' . Warning The below feature is only available in the Dune V1 engine. To save the user from having to put in \\x for the address a useful formatting of addresses is this one: WHERE contract_address = CONCAT ( '\\x' , substring ( '{{token address}}' from 3 )):: bytea This let\u2019s a user of your Query simply paste in 0xc00e94cb662c3520282e6f5717214004a7f26888 instead of \\xc00e94cb662c3520282e6f5717214004a7f26888 when they filter.","title":"Query Tips"},{"location":"getting-started/queries/tips/#use-spells","text":"The easiest way to do great analysis with Dune is to use the well-organized data you'll find in Spells (Dune V2) and Abstractions (Dune V1). Theses tables, like dex.trades , are cleaned and contain data/metadata (like human readable token symbols) that make them very straight forward to query.","title":"Use Spells"},{"location":"getting-started/queries/tips/#v1-inline-ethereum-addresses-formatting","text":"Warning This feature is only available in the Dune V1 engine. In Dune Ethereum addresses are stored as PostgreSQL byte arrays which are encoded with the \\x prefix. This differs from the customary 0x prefix. If you\u2019d like to use an inline address, say to filter for a given token, you would do WHERE token = '\\x6b175474e89094c44da98b954eedeac495271d0f' which is simply short for WHERE token = '\\x6b175474e89094c44da98b954eedeac495271d0f' :: bytea","title":"V1 Inline Ethereum Addresses Formatting"},{"location":"getting-started/queries/tips/#quote-column-and-table-names-in-camelcase","text":"Warning This feature is only available in the Dune V1 engine. Column and table names are mostly taken directly from smart contract Application Binary Interfaces (ABIs), with no modification. Since most smart contracts are written in Solidity, and written with a camelCased naming convention, so are many of Dune\u2019s table and column names PostgreSQL (Dune V1) requires you to column and table name references are case sensitive: SELECT \u201c columnName \u201d FROM projectname . \u201d contractName_evt_EventName \u201d LIMIT 10 In PostgreSQL, double quotes are reserved for tables and columns, whereas single quotes are reserved for values: SELECT \u201c columnName \u201d FROM projectname . \u201d contratName_evt_eventName \u201d WHERE contract_address = '\\x6B175474E89094C44Da98b954EedeAC495271d0F' LIMIT 10 Schemas are always lowercase in Dune.","title":"Quote Column and Table Names in camelCase"},{"location":"getting-started/queries/tips/#remove-decimals","text":"Ether transfers and most ERC-20 tokens have 18 decimal places, which most humans agree is too many to read. To transmute these into a more human-friendly form, use the erc20.tokens table and divide the token's transfer_value by 10: PostgreSQL Databricks SQL transfer_value / 10 ^ erc20 . tokens . decimals transfer_value / x * power ( 10 , y ) ` or ` transfer_value / x * 1 e * y","title":"Remove Decimals"},{"location":"getting-started/queries/tips/#get-time-with-date_trunc","text":"We\u2019ve added evt_block_time to decoded event tables for your convenience. A neat way to use it is with the date_trunc function like this: SELECT date_trunc ( 'week' , evt_block_time ) AS time You can use minute , day , week , or month .","title":"Get time with date_trunc"},{"location":"getting-started/queries/tips/#how-to-get-usd-price","text":"To get the USD price of on-chain activity, you typically want to JOIN the smart contract event you are looking at with the prices.usd on the minute for a given asset : LEFT JOIN prices . usd p ON p . minute = date_trunc ( 'minute' , evt_block_time ) AND event . \"asset\" = p . contract_address Then you can simply multiply the value or amount from the smart contract event with the USD price in your SELECT statement: * p.price .","title":"How to get USD price"},{"location":"getting-started/queries/tips/#token-symbols","text":"You'll often want to group your results by token address e.g. to see volume on a DEX grouped by token. But a big list of token addresses are abstract and hard to digest! So use the token symbol instead. \ud83e\ude84 To do this, JOIN the table erc20.tokens with your event table where asset = {{token_address}} . You then select symbol in your select statement instead of token address. PostgreSQL Databricks SQL NB The erc20.tokens table contains a selection of popular tokens. If you are working with more obscure tokens you should be careful with joining with this table because tokens that are not in the coincap table might be excluded from your results. NB The tokens_blockchain.erc20 table contains a selection of popular tokens. If you are working with more obscure tokens you should be careful with joining with this table because tokens that are not in the coincap table might be excluded from your results.","title":"Token symbols"},{"location":"getting-started/queries/tips/#filter-queries-and-dashboards-with-parameters","text":"Parameters can turn your Query or Dashboard into an app for blockchain data. Click Add parameter in the bottom right of the SQL editor on the Query editor page Double curly brackets will appear in your Query {{}} . Put the name of your parameter inside, e.g. {{token symbol}} or {{holder address}} . Note that you need to put single quotes if you want to use the parameter in your Query WHERE token = '{{token symbol}}' . Warning The below feature is only available in the Dune V1 engine. To save the user from having to put in \\x for the address a useful formatting of addresses is this one: WHERE contract_address = CONCAT ( '\\x' , substring ( '{{token address}}' from 3 )):: bytea This let\u2019s a user of your Query simply paste in 0xc00e94cb662c3520282e6f5717214004a7f26888 instead of \\xc00e94cb662c3520282e6f5717214004a7f26888 when they filter.","title":"Filter Queries and Dashboards with Parameters"},{"location":"getting-started/query-quick-start/","text":"Here's a short five-step guide to getting familiar with a protocol and figuring out how to query around it using Dune. Thanks to @ilemi for putting this together! Learn more about how Queries work here . 1. Find the main point of entry \u00b6 Using the Dune Data Explorer and the protocol app page/docs and try to figure out what the main user entry point function is. Sometimes this is straightforward, but different contract patterns on more complex protocols will make this confusing. For most of decentralized finance (DeFi), the primary entry point for users is just some variation of Deposit . If the contract isn't Decoded yet, you can start with some raw queries for finding the most common function and event signatures here: Dune Utility Queries If you're having trouble figuring out the tables, see our Table docs here . 2. Explore the contract flow \u00b6 Typically a function call is not as simple as an ETH/token transfer that only involves one contract. Once you figure out the entry point, run a basic LIMIT query on it and look at some example transactions in the relevant blockchain explorer for data hints (i.e. what protocols did the tx interact with besides the main one). SELECT * FROM protocol_name . \"Contractname_evt_EventEmitted\" LIMIT 10 Look at evt_tx_hash and plop it into the blockchain explorer to start getting a sense of what contracts and interactions are involved. 3. Decide what question you want to answer \u00b6 If you already have a question in mind, then skip this step. If you don't have a question yet, think through the flow chart of contract interactions. Here are some starter questions to help you find something interesting to look into: What other protocols did the entry function touch? If there is yield/interest being generated, where and when did the tokens get swapped? How many tokens went in and how many were burned/minted by the end? Is it possible that anything from the last three questions would lead to some sort of imbalance or accumulation? i.e. a DEX pool ending in low liquidity, or depositing so much into one pool that the yield/interest rate falls from supply imbalance. Or if this involves NFTs, what were the effects on future bid/sale behavior (or were there behaviors in bid/sale that led up to this transaction)? 4. Build your Query \u00b6 Now that you have settled on a question, the real fun begins. \ud83e\uddd9 You'll quickly notice that the function call data and event log data don't always have all the parameters you're looking for. The usual culprits that are missing are the transaction signer (found in ethereum.\"transactions\" ) and the ETH value transferred (found in ethereum.\"traces\" ). Typically you'll have to work with the base tables (transactions, traces, logs) and possibly tables from other protocols (like DEX/exchange protocols) to complete the data you need for your query. Figuring out which tables to pull what data from takes a while to learn, and the best way to get started is usually to search for existing dashboards or queries that have attempted something similar. Dune has been around long enough that most query patterns aren't too hard to find somewhere else. \u2728 5. Make your Visualization \u00b6 Lastly, you should visualize the query in a chart by clicking \"New visualization\" next to \"Query Results\". ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8dbf893c-33f5-47cb-8d4f-41ff4b2df8d6/Untitled.png) If you're showing token amounts, you likely have to fix for decimals or multiply by token price (in prices.usd or dex.trades ) to get to a USD value which is more interpretable.","title":"Query Quick Start"},{"location":"getting-started/query-quick-start/#1-find-the-main-point-of-entry","text":"Using the Dune Data Explorer and the protocol app page/docs and try to figure out what the main user entry point function is. Sometimes this is straightforward, but different contract patterns on more complex protocols will make this confusing. For most of decentralized finance (DeFi), the primary entry point for users is just some variation of Deposit . If the contract isn't Decoded yet, you can start with some raw queries for finding the most common function and event signatures here: Dune Utility Queries If you're having trouble figuring out the tables, see our Table docs here .","title":"1. Find the main point of entry"},{"location":"getting-started/query-quick-start/#2-explore-the-contract-flow","text":"Typically a function call is not as simple as an ETH/token transfer that only involves one contract. Once you figure out the entry point, run a basic LIMIT query on it and look at some example transactions in the relevant blockchain explorer for data hints (i.e. what protocols did the tx interact with besides the main one). SELECT * FROM protocol_name . \"Contractname_evt_EventEmitted\" LIMIT 10 Look at evt_tx_hash and plop it into the blockchain explorer to start getting a sense of what contracts and interactions are involved.","title":"2. Explore the contract flow"},{"location":"getting-started/query-quick-start/#3-decide-what-question-you-want-to-answer","text":"If you already have a question in mind, then skip this step. If you don't have a question yet, think through the flow chart of contract interactions. Here are some starter questions to help you find something interesting to look into: What other protocols did the entry function touch? If there is yield/interest being generated, where and when did the tokens get swapped? How many tokens went in and how many were burned/minted by the end? Is it possible that anything from the last three questions would lead to some sort of imbalance or accumulation? i.e. a DEX pool ending in low liquidity, or depositing so much into one pool that the yield/interest rate falls from supply imbalance. Or if this involves NFTs, what were the effects on future bid/sale behavior (or were there behaviors in bid/sale that led up to this transaction)?","title":"3. Decide what question you want to answer"},{"location":"getting-started/query-quick-start/#4-build-your-query","text":"Now that you have settled on a question, the real fun begins. \ud83e\uddd9 You'll quickly notice that the function call data and event log data don't always have all the parameters you're looking for. The usual culprits that are missing are the transaction signer (found in ethereum.\"transactions\" ) and the ETH value transferred (found in ethereum.\"traces\" ). Typically you'll have to work with the base tables (transactions, traces, logs) and possibly tables from other protocols (like DEX/exchange protocols) to complete the data you need for your query. Figuring out which tables to pull what data from takes a while to learn, and the best way to get started is usually to search for existing dashboards or queries that have attempted something similar. Dune has been around long enough that most query patterns aren't too hard to find somewhere else. \u2728","title":"4. Build your Query"},{"location":"getting-started/query-quick-start/#5-make-your-visualization","text":"Lastly, you should visualize the query in a chart by clicking \"New visualization\" next to \"Query Results\". ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8dbf893c-33f5-47cb-8d4f-41ff4b2df8d6/Untitled.png) If you're showing token amounts, you likely have to fix for decimals or multiply by token price (in prices.usd or dex.trades ) to get to a USD value which is more interpretable.","title":"5. Make your Visualization"},{"location":"getting-started/query-quick-start/example/","text":"Let's walk through an example of building a query using OpenSea data to: Figure out who the top buyers and sellers are by USD Visualize this in some sort of distribution, over some sort of time period. If you get stuck, fork and test out the demo query here or ask for help in the #query-questions Discord channel ! 1. Find the main point of entry \u00b6 The easiest place to start your query exploration is the Dune Data Explorer , for two reasons: Searching here will let us discover whether the contracts we'll need have already been decoded; if so we'll have a lot less work to do. If our data is already Decoded , the contracts, calls, events, and data points will probably have obvious labels - eg amount_usd is the US dollar amount an NFT was bought/sold for. So we might not need to dig through the project docs to understand which contracts and data we need to build our Query. So, let's get started by switching to the Dune V2 data set and searching for \"opensea\" to see what we find. By using Dune V2, we get access to Spells which can let us access data from multiple blockchains in one Query - assuming the spell for OpenSea exists (Dune V2 is still in beta so not all data has been migrated yet). Awesome! Looks like we have an \"opesea trades\" Spell (Spells are marked by the \ud83e\ude84 icon in the data explorer). 2. Explore the contract flow \u00b6 There are a couple of ways we can check to see what data we have access to in there. Run a LIMIT query \u00b6 The first way is to run a SELECT * + LIMIT search to see what comes up. SELECT * tells Dune to send us every column avaiable in this table. LIMIT limits the number of rows returned so we don't try to return the whole table (this would take forever to load). Note a couple of things here: Dune has a nice auto complete feature - just start typing to see options and hit enter to paste ( learn more about how the Query Window works here ) You can click the >> next to a table name in the Data Explorer to automatically paste that into your query ( learn more about the Data Explorer here ) Also since this is a pre-made how to guide you can just be lazy and copy this code: SELECT * FROM opensea . trades LIMIT 10 ; From that Query you'll get a table with data to explore like this: [Dune V2 Only] Search the Spellbook Docs \u00b6 Another benefit of Dune V2 is the special set of auto-generated docs that come with it. These can help us quickly figure out what data exists inside of V2 tables AND see how those tables are constructed in case we need it to make more complex queries. Let's head over to https://dune.com/spellbook and search for opensea using the top search bar: Here we find an opensea_trades table that probably has some interesting data! Scrolling through the columns section we can see all the different data columns it has. Since this is a Decoded Table , a lot of the labels - like amount_usd - make sense just by reading them. For those that aren't so obvious - like trade_type - we can click to get a description: The Spellbook Docs also have a Lineage Graph which lets us view the tables that were used to build, as well as the tables built from opensea_trades : We can also click the expand button in the right hand corner to see the complete flow of parent and child tables, then right click on any of them to view their documentation as well: For a complete list of things you can do with the Spellbook Docs see this page . 3. Decide what question you want to answer \u00b6 This step could easily be the first, and is in many cases - a Wizard will hear about a new NFT launch on crypto twitter, be shilled a new project on Discord, or read a story about an emerging market segment and dive into Dune to pull data and learn more. And in this example, to make it easy on you, we've already decided to: Figure out who the top OpenSea buyers and sellers are by USD Visualize this in some sort of distribution, over some sort of time period. Finding inspiration from Dune Dashboards \u00b6 As a beginner, if you don't already have an idea of what you want to build, exploring Dune data from the Query builder is a fun way to dive in. You can also use the Dune Explorer to see what others are building as well! Though this might not lead you to building the next DeFi users over time (created by @richardchen ), it's a great way to learn more about using Dune as you can see what's possible and even fork existing Queries to modify further! As a quick example of this, let's head over to dune.com which will take us to right to the Dashboard Explorer page. By default, the Explorer lists dashboards trending in the last 4 hours. Using the right sidebar, we can also search by: Newest dashboards Dashboards with the most stars (Favorites) By Tags like DeFi and NFT At the time of writing, @niftytable's Trending Contracts dashboard is trending in Dune so let's take a look: Hmm, looks like this shows us top contracts by number of transactions, active wallets, and gas spent. But what about by USD value of transactions? To fork these Queries and add that data we just click the Visualization name (in this case a Table Visualization ), then click the Fork button at the top left: From there we could explore, test, and expand on @niftytable's original Query to make it our own! 4. Build your Query \u00b6 Getting back to our OpenSea Query example, our goal again is to: Figure out who the top OpenSea buyers and sellers are by USD Visualize this in some sort of distribution, over some sort of time period. We've run an initial limit search and found that, among other data points, this Spell has buyer and seller wallet addresses, as well as amount_usd for these transactions. Pulling relevant buyer data \u00b6 So let's refine our Query a bit to step closer to where we want to be, starting by: Only pulling info for buyers Adding all USD amounts for transactions for the same buyer together Limiting our search by Time, say the last day, instead of by number of rows Here's the before and after of how to do that: Note -- lines are SQL comments here for clarification, you don't need these to run this code yourself. We also added more spacing than needed to make it easier to read in the \"After\" Tab, so use the \"Copy/Paste\" tab to run this Query yourself! Before After Copy/Paste SELECT * FROM opensea . trades LIMIT 10 ; -- Returns the `buyer` column from opensea.trades, then adds all the `amount_usd` values together and labels them as `buy_vol` SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades -- limits results to the past 24 hours and removes resuls where amount_usd is empty (for some reason that data is unavailable in the opensea.trades Spell) WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL -- Tells Dune to SUM USD values for each buyer and return the results that way GROUP BY 1 ; SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ; JOIN ing buyer and seller data \ud83e\udd1d \u00b6 Cool, looks like that works for buyers! Now to make this work for buyers and sellers, we're going to have to get fancy. We'll wrap this basic query in a WITH statement, so we can return two separate tables, one for buyers and sellers, then use FULL JOIN to create a new table with buy, sell, and total USD volume for each unique wallet address. Before After Copy/Paste SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ; -- Here we define a new table `buyer_vol` based on the SQL query within the below () WITH buyer_vol AS ( SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 -- The `,` here lets us continue our `WITH` statement with an additional table/query ), seller_vol AS ( SELECT seller , SUM ( amount_usd ) AS sell_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ), all_txn AS ( -- Here we're relabeling the `buyer` column, which contains a wallet address, as `wallet`, returning the buy/sell volumes from each of our above tables, then summing those values as `ttl_vol` SELECT buyer AS wallet , buyer_vol . buy_vol , seller_vol . sell_vol , SUM ( buyer_vol . buy_vol + seller_vol . sell_vol ) AS ttl_vol FROM buyer_vol -- \"JOIN\" is used to combine two tables that share a common column; \"FULL\" ensures our new table includes people who only bought, only sold, and both. Learn more about JOIN in [Dune V2 Apache Spark SQL here](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html#full-join). FULL JOIN seller_vol ON buyer_vol . buyer = seller_vol . seller -- here we're combining our `buyer_vol` and `seller_vol` tables by matching `buyer` and `seller` which we know are both wallet addresses GROUP BY 1 , 2 , 3 -- Here were ensuring our table is ordered by total trade volume, starting with the highest number to the lowest ORDER BY ttl_vol DESC ) -- Finally, below we return all the columns and rows from the combined table we just made; Limit to the TOP 10 wallets by total volume SELECT * FROM all_txn LIMIT 10 ; WITH buyer_vol AS ( SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ), seller_vol AS ( SELECT seller , SUM ( amount_usd ) AS sell_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ), all_txn AS ( SELECT buyer AS wallet , buyer_vol . buy_vol , seller_vol . sell_vol , SUM ( buyer_vol . buy_vol + seller_vol . sell_vol ) AS ttl_vol FROM buyer_vol FULL JOIN seller_vol ON buyer_vol . buyer = seller_vol . seller GROUP BY 1 , 2 , 3 ORDER BY ttl_vol DESC ) SELECT * FROM all_txn LIMIT 10 ; Adding Parameters \u00b6 Now for extra points, let's add a Parameter so people who view our Visualizations can filter by 1 day, 1 week, and 1 month time periods. To do that, let's highlight our 24 hour interval measure, then click the Add parameter button Then click the Gear icon in the field that appears below the Query Window to modify our parameter: Here we'll: Rename unnamed_parameter -> time period Make the Parameter Type \"List\" Add the values \"1 day, 1 week, 1 month\" And save! Lastly, we'll update our other WHERE statements with the parameter. Ready to Visualize \ud83d\udc40 \u00b6 With that, we have the data we need to make the visualizations we're aiming for. \ud83e\ude84 Here are our results, click \"Query results\" below to see the Query and fork it if you'd like! \ud83e\uddd9 5. Make your Visualization \u00b6 Alright, now we want to \"Visualize this in some sort of distribution, over some sort of time period.\" We've got our time period piece in place with our Parameter, and we have total USD bought, sold, and wallet data. So for {{time period}} let's look at our top 10 wallets by total volume, as well as what portion of that volume is buying vs selling. Creating and Formatting Bar Charts \u00b6 To do that we'll use a Bar Chart Visualization : Scrolling down we see Visualization options, it looks like the \"Results Data\" defaulted to making the x column wallet and y column 1 buy_vol , which is a good start. Let's add sell volume to y column 2 and then in the \"Chart options\" Enable Stacking so the values are layered on top of each other: By default, Dune sorted our x-axis alphabetically. Let's uncheck \"Sort Values\" to get it back to being sorted by volume, then \"Reverse Order\" so that our graph is lowest number to highest (the opposite of our data table but the way we're love to see graphs - up and to the right): Our USD amounts are also a bit confusing, so let's update the \"Label Format\" in Y-axis options to $0.0a which will turn a number like 12345.6789 into \"$12.3k\": Then we'll update our title and x/y axis labels to make a nice, easy to understand chart: Learn more about formatting Visualizations here ! Sharing Queries and Visualizations \u00b6 To share your Queries (either embedding them like the above or just to share a lin), click the Embed button above the Query window: Note The embed button works as a stand alone link and as a way to embed your live graphs into websites/apps. If your Query has no Visualizations, the link will be to the Query Results table. If you have multiple Visualizations, the link will be for whichever Visualization you've selected when you clicked the Embed button. With a lot of copy/paste (literally and figuratively), we can make this same graph for just total volume so we don't have to do the mental math of adding up the sell/buy volume: Making a Dashboard \u00b6 Finally, let's add our two visualizations to a starter Dashboard . To do that we: Click the Add to dashboard button above our Visualization. Click New dashboard button in the popup. Give our Dashboard a name and save it. Click the Add button. Then we just add our other visualizations to the Dashboard we created and: To make our Dashboard look nice, we hit the edit button and can drag and resize our Visualizations: And we can add text widgets to explain our dashboards and how they work! And with that, we're ready to hit the share button to get a sharable Dashboard link like this one: https://dune.com/cryptofreedman/query-quick-start Congrats you've made a Dune Query! \u00b6 To go deeper as you're getting started, check out the other pages in our Getting Started section using the left side bar. \ud83d\udc48 Check out the Features section above to learn more about how Dune works or the Tables section to learn more about the data you can query and use with Dune. \ud83d\udc46","title":"Example"},{"location":"getting-started/query-quick-start/example/#1-find-the-main-point-of-entry","text":"The easiest place to start your query exploration is the Dune Data Explorer , for two reasons: Searching here will let us discover whether the contracts we'll need have already been decoded; if so we'll have a lot less work to do. If our data is already Decoded , the contracts, calls, events, and data points will probably have obvious labels - eg amount_usd is the US dollar amount an NFT was bought/sold for. So we might not need to dig through the project docs to understand which contracts and data we need to build our Query. So, let's get started by switching to the Dune V2 data set and searching for \"opensea\" to see what we find. By using Dune V2, we get access to Spells which can let us access data from multiple blockchains in one Query - assuming the spell for OpenSea exists (Dune V2 is still in beta so not all data has been migrated yet). Awesome! Looks like we have an \"opesea trades\" Spell (Spells are marked by the \ud83e\ude84 icon in the data explorer).","title":"1. Find the main point of entry"},{"location":"getting-started/query-quick-start/example/#2-explore-the-contract-flow","text":"There are a couple of ways we can check to see what data we have access to in there.","title":"2. Explore the contract flow"},{"location":"getting-started/query-quick-start/example/#run-a-limit-query","text":"The first way is to run a SELECT * + LIMIT search to see what comes up. SELECT * tells Dune to send us every column avaiable in this table. LIMIT limits the number of rows returned so we don't try to return the whole table (this would take forever to load). Note a couple of things here: Dune has a nice auto complete feature - just start typing to see options and hit enter to paste ( learn more about how the Query Window works here ) You can click the >> next to a table name in the Data Explorer to automatically paste that into your query ( learn more about the Data Explorer here ) Also since this is a pre-made how to guide you can just be lazy and copy this code: SELECT * FROM opensea . trades LIMIT 10 ; From that Query you'll get a table with data to explore like this:","title":"Run a LIMIT query"},{"location":"getting-started/query-quick-start/example/#dune-v2-only-search-the-spellbook-docs","text":"Another benefit of Dune V2 is the special set of auto-generated docs that come with it. These can help us quickly figure out what data exists inside of V2 tables AND see how those tables are constructed in case we need it to make more complex queries. Let's head over to https://dune.com/spellbook and search for opensea using the top search bar: Here we find an opensea_trades table that probably has some interesting data! Scrolling through the columns section we can see all the different data columns it has. Since this is a Decoded Table , a lot of the labels - like amount_usd - make sense just by reading them. For those that aren't so obvious - like trade_type - we can click to get a description: The Spellbook Docs also have a Lineage Graph which lets us view the tables that were used to build, as well as the tables built from opensea_trades : We can also click the expand button in the right hand corner to see the complete flow of parent and child tables, then right click on any of them to view their documentation as well: For a complete list of things you can do with the Spellbook Docs see this page .","title":"[Dune V2 Only] Search the Spellbook Docs"},{"location":"getting-started/query-quick-start/example/#3-decide-what-question-you-want-to-answer","text":"This step could easily be the first, and is in many cases - a Wizard will hear about a new NFT launch on crypto twitter, be shilled a new project on Discord, or read a story about an emerging market segment and dive into Dune to pull data and learn more. And in this example, to make it easy on you, we've already decided to: Figure out who the top OpenSea buyers and sellers are by USD Visualize this in some sort of distribution, over some sort of time period.","title":"3. Decide what question you want to answer"},{"location":"getting-started/query-quick-start/example/#finding-inspiration-from-dune-dashboards","text":"As a beginner, if you don't already have an idea of what you want to build, exploring Dune data from the Query builder is a fun way to dive in. You can also use the Dune Explorer to see what others are building as well! Though this might not lead you to building the next DeFi users over time (created by @richardchen ), it's a great way to learn more about using Dune as you can see what's possible and even fork existing Queries to modify further! As a quick example of this, let's head over to dune.com which will take us to right to the Dashboard Explorer page. By default, the Explorer lists dashboards trending in the last 4 hours. Using the right sidebar, we can also search by: Newest dashboards Dashboards with the most stars (Favorites) By Tags like DeFi and NFT At the time of writing, @niftytable's Trending Contracts dashboard is trending in Dune so let's take a look: Hmm, looks like this shows us top contracts by number of transactions, active wallets, and gas spent. But what about by USD value of transactions? To fork these Queries and add that data we just click the Visualization name (in this case a Table Visualization ), then click the Fork button at the top left: From there we could explore, test, and expand on @niftytable's original Query to make it our own!","title":"Finding inspiration from Dune Dashboards"},{"location":"getting-started/query-quick-start/example/#4-build-your-query","text":"Getting back to our OpenSea Query example, our goal again is to: Figure out who the top OpenSea buyers and sellers are by USD Visualize this in some sort of distribution, over some sort of time period. We've run an initial limit search and found that, among other data points, this Spell has buyer and seller wallet addresses, as well as amount_usd for these transactions.","title":"4. Build your Query"},{"location":"getting-started/query-quick-start/example/#pulling-relevant-buyer-data","text":"So let's refine our Query a bit to step closer to where we want to be, starting by: Only pulling info for buyers Adding all USD amounts for transactions for the same buyer together Limiting our search by Time, say the last day, instead of by number of rows Here's the before and after of how to do that: Note -- lines are SQL comments here for clarification, you don't need these to run this code yourself. We also added more spacing than needed to make it easier to read in the \"After\" Tab, so use the \"Copy/Paste\" tab to run this Query yourself! Before After Copy/Paste SELECT * FROM opensea . trades LIMIT 10 ; -- Returns the `buyer` column from opensea.trades, then adds all the `amount_usd` values together and labels them as `buy_vol` SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades -- limits results to the past 24 hours and removes resuls where amount_usd is empty (for some reason that data is unavailable in the opensea.trades Spell) WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL -- Tells Dune to SUM USD values for each buyer and return the results that way GROUP BY 1 ; SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ;","title":"Pulling relevant buyer data"},{"location":"getting-started/query-quick-start/example/#joining-buyer-and-seller-data","text":"Cool, looks like that works for buyers! Now to make this work for buyers and sellers, we're going to have to get fancy. We'll wrap this basic query in a WITH statement, so we can return two separate tables, one for buyers and sellers, then use FULL JOIN to create a new table with buy, sell, and total USD volume for each unique wallet address. Before After Copy/Paste SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ; -- Here we define a new table `buyer_vol` based on the SQL query within the below () WITH buyer_vol AS ( SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 -- The `,` here lets us continue our `WITH` statement with an additional table/query ), seller_vol AS ( SELECT seller , SUM ( amount_usd ) AS sell_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ), all_txn AS ( -- Here we're relabeling the `buyer` column, which contains a wallet address, as `wallet`, returning the buy/sell volumes from each of our above tables, then summing those values as `ttl_vol` SELECT buyer AS wallet , buyer_vol . buy_vol , seller_vol . sell_vol , SUM ( buyer_vol . buy_vol + seller_vol . sell_vol ) AS ttl_vol FROM buyer_vol -- \"JOIN\" is used to combine two tables that share a common column; \"FULL\" ensures our new table includes people who only bought, only sold, and both. Learn more about JOIN in [Dune V2 Apache Spark SQL here](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-join.html#full-join). FULL JOIN seller_vol ON buyer_vol . buyer = seller_vol . seller -- here we're combining our `buyer_vol` and `seller_vol` tables by matching `buyer` and `seller` which we know are both wallet addresses GROUP BY 1 , 2 , 3 -- Here were ensuring our table is ordered by total trade volume, starting with the highest number to the lowest ORDER BY ttl_vol DESC ) -- Finally, below we return all the columns and rows from the combined table we just made; Limit to the TOP 10 wallets by total volume SELECT * FROM all_txn LIMIT 10 ; WITH buyer_vol AS ( SELECT buyer , SUM ( amount_usd ) AS buy_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ), seller_vol AS ( SELECT seller , SUM ( amount_usd ) AS sell_vol FROM opensea . trades WHERE block_time > now () - interval '24 hours' AND amount_usd IS NOT NULL GROUP BY 1 ), all_txn AS ( SELECT buyer AS wallet , buyer_vol . buy_vol , seller_vol . sell_vol , SUM ( buyer_vol . buy_vol + seller_vol . sell_vol ) AS ttl_vol FROM buyer_vol FULL JOIN seller_vol ON buyer_vol . buyer = seller_vol . seller GROUP BY 1 , 2 , 3 ORDER BY ttl_vol DESC ) SELECT * FROM all_txn LIMIT 10 ;","title":"JOINing buyer and seller data \ud83e\udd1d"},{"location":"getting-started/query-quick-start/example/#adding-parameters","text":"Now for extra points, let's add a Parameter so people who view our Visualizations can filter by 1 day, 1 week, and 1 month time periods. To do that, let's highlight our 24 hour interval measure, then click the Add parameter button Then click the Gear icon in the field that appears below the Query Window to modify our parameter: Here we'll: Rename unnamed_parameter -> time period Make the Parameter Type \"List\" Add the values \"1 day, 1 week, 1 month\" And save! Lastly, we'll update our other WHERE statements with the parameter.","title":"Adding Parameters"},{"location":"getting-started/query-quick-start/example/#ready-to-visualize","text":"With that, we have the data we need to make the visualizations we're aiming for. \ud83e\ude84 Here are our results, click \"Query results\" below to see the Query and fork it if you'd like! \ud83e\uddd9","title":"Ready to Visualize \ud83d\udc40"},{"location":"getting-started/query-quick-start/example/#5-make-your-visualization","text":"Alright, now we want to \"Visualize this in some sort of distribution, over some sort of time period.\" We've got our time period piece in place with our Parameter, and we have total USD bought, sold, and wallet data. So for {{time period}} let's look at our top 10 wallets by total volume, as well as what portion of that volume is buying vs selling.","title":"5. Make your Visualization"},{"location":"getting-started/query-quick-start/example/#creating-and-formatting-bar-charts","text":"To do that we'll use a Bar Chart Visualization : Scrolling down we see Visualization options, it looks like the \"Results Data\" defaulted to making the x column wallet and y column 1 buy_vol , which is a good start. Let's add sell volume to y column 2 and then in the \"Chart options\" Enable Stacking so the values are layered on top of each other: By default, Dune sorted our x-axis alphabetically. Let's uncheck \"Sort Values\" to get it back to being sorted by volume, then \"Reverse Order\" so that our graph is lowest number to highest (the opposite of our data table but the way we're love to see graphs - up and to the right): Our USD amounts are also a bit confusing, so let's update the \"Label Format\" in Y-axis options to $0.0a which will turn a number like 12345.6789 into \"$12.3k\": Then we'll update our title and x/y axis labels to make a nice, easy to understand chart: Learn more about formatting Visualizations here !","title":"Creating and Formatting Bar Charts"},{"location":"getting-started/query-quick-start/example/#sharing-queries-and-visualizations","text":"To share your Queries (either embedding them like the above or just to share a lin), click the Embed button above the Query window: Note The embed button works as a stand alone link and as a way to embed your live graphs into websites/apps. If your Query has no Visualizations, the link will be to the Query Results table. If you have multiple Visualizations, the link will be for whichever Visualization you've selected when you clicked the Embed button. With a lot of copy/paste (literally and figuratively), we can make this same graph for just total volume so we don't have to do the mental math of adding up the sell/buy volume:","title":"Sharing Queries and Visualizations"},{"location":"getting-started/query-quick-start/example/#making-a-dashboard","text":"Finally, let's add our two visualizations to a starter Dashboard . To do that we: Click the Add to dashboard button above our Visualization. Click New dashboard button in the popup. Give our Dashboard a name and save it. Click the Add button. Then we just add our other visualizations to the Dashboard we created and: To make our Dashboard look nice, we hit the edit button and can drag and resize our Visualizations: And we can add text widgets to explain our dashboards and how they work! And with that, we're ready to hit the share button to get a sharable Dashboard link like this one: https://dune.com/cryptofreedman/query-quick-start","title":"Making a Dashboard"},{"location":"getting-started/query-quick-start/example/#congrats-youve-made-a-dune-query","text":"To go deeper as you're getting started, check out the other pages in our Getting Started section using the left side bar. \ud83d\udc48 Check out the Features section above to learn more about how Dune works or the Tables section to learn more about the data you can query and use with Dune. \ud83d\udc46","title":"Congrats you've made a Dune Query!"},{"location":"getting-started/use-cases/","text":"Dune is an open platform \u00b6 Dune is a blockchain data analytics platform that serves a few different groups: Dune Spectators can view a variety of on-chain data made interesting and understandable via Dashboards on Dune.com. Dune Wizards are the magicians who take the raw data the Dune Platform ingests and use it to tell stories via Dashboards , Visualizations and Queries . Dune Patrons including the crypto communities, protocols, and companies who, with the help of Dune Wizards, leverage Dune's powers to solve problems and share insights within their organizations. What is Dune used for? \u00b6 Nearly the entire world of blockchain data analytics can be explored with Dune - there really aren't many limitations to what you can query and visualize with Dune. That being said, to help you get a clearer picture of the type of problems you can explore and solve with done, we can establish a few broad Use Cases: Project Dashboards Sector Dashbaords Ecosystem Dashboards What type of content is generated on Dune? \u00b6 To get more insights into how people use Dune, you can follow our Twitter account where we retweet the most interesting things that happen on Dune. Just click through the last few tweets and see what people are up to on Dune currently! Tweets by Dune","title":"Use Cases"},{"location":"getting-started/use-cases/#dune-is-an-open-platform","text":"Dune is a blockchain data analytics platform that serves a few different groups: Dune Spectators can view a variety of on-chain data made interesting and understandable via Dashboards on Dune.com. Dune Wizards are the magicians who take the raw data the Dune Platform ingests and use it to tell stories via Dashboards , Visualizations and Queries . Dune Patrons including the crypto communities, protocols, and companies who, with the help of Dune Wizards, leverage Dune's powers to solve problems and share insights within their organizations.","title":"Dune is an open platform"},{"location":"getting-started/use-cases/#what-is-dune-used-for","text":"Nearly the entire world of blockchain data analytics can be explored with Dune - there really aren't many limitations to what you can query and visualize with Dune. That being said, to help you get a clearer picture of the type of problems you can explore and solve with done, we can establish a few broad Use Cases: Project Dashboards Sector Dashbaords Ecosystem Dashboards","title":"What is Dune used for?"},{"location":"getting-started/use-cases/#what-type-of-content-is-generated-on-dune","text":"To get more insights into how people use Dune, you can follow our Twitter account where we retweet the most interesting things that happen on Dune. Just click through the last few tweets and see what people are up to on Dune currently! Tweets by Dune","title":"What type of content is generated on Dune?"},{"location":"getting-started/use-cases/ecosystem-dashboards/","text":"Since Dune has almost all blockchain data in one place, we can also discover and explore more general ecosystem-level metrics. Gas Prices by @kroeger0x \u00b6 @kroeger0x easily assembled the data on average gas spent per tx, gas limit, gas spent in total in a block etc. and made it just as easy for us to view using Dune. DeFi Adoption by @rchen8 \u00b6 Richard Chen has compiled the user numbers of all important DeFi Protocols and compiled them in one Dashboard.","title":"Ecosystem Dashboards"},{"location":"getting-started/use-cases/ecosystem-dashboards/#gas-prices-by-kroeger0x","text":"@kroeger0x easily assembled the data on average gas spent per tx, gas limit, gas spent in total in a block etc. and made it just as easy for us to view using Dune.","title":"Gas Prices by @kroeger0x"},{"location":"getting-started/use-cases/ecosystem-dashboards/#defi-adoption-by-rchen8","text":"Richard Chen has compiled the user numbers of all important DeFi Protocols and compiled them in one Dashboard.","title":"DeFi Adoption by @rchen8"},{"location":"getting-started/use-cases/project-dashboards/","text":"Project Dashboards allow you to easily assemble data on your favorite project in one place. Without having to maintain infrastructure or overhead, you can simply query for the data you desire and assemble it on a Dashboard to make a Datahub for your project Reality. Let's explore this with an example: Tornado.Cash by @poma \u00b6 This is the Dashboard shows us many important metrics for one protocol, Tornado Cash, all in one place. We can see that @poma wanted to show us a few \"at a glance\" counters which make important data like total deposits and unique users easy to view and understand by just about anyone. @poma follows this up with some interesting historical data points which show the growth in users and volume. The Dashboard continues, but you can clearly already see how much value these stats and charts can produce for a company, protocol, or community. You can find many other great Dashboards on the Dune Projects Page .","title":"Project Dashboards"},{"location":"getting-started/use-cases/project-dashboards/#tornadocash-by-poma","text":"This is the Dashboard shows us many important metrics for one protocol, Tornado Cash, all in one place. We can see that @poma wanted to show us a few \"at a glance\" counters which make important data like total deposits and unique users easy to view and understand by just about anyone. @poma follows this up with some interesting historical data points which show the growth in users and volume. The Dashboard continues, but you can clearly already see how much value these stats and charts can produce for a company, protocol, or community. You can find many other great Dashboards on the Dune Projects Page .","title":"Tornado.Cash by @poma"},{"location":"getting-started/use-cases/sector-dashboards/","text":"Sector Dashboards allow you to get an overview of an entire sector of the crypto economy and analyze what's happening in real time. Let's explore this with some examples: Dex Metrics by @hagaetc \u00b6 This Dashboard allows you to view data and learn about the entire Decentralized Exchange sector at a glance. Here you can see metrics like Dex Volume, Market Share, and Total Volume across all exchanges, along with many more stats. Together they make it easy to compare the performance of different DEXes against each other and evaluate the performance of the entire sector. All of these stats and charts update in real time and are maintained by the community; if a new DEX wants to be added to this Dashboard, all they have to do is submit their code to our public GitHub. More on this in Spells . Indices by @0xBoxer \u00b6 Another interesting sector-based Dune Dashboard covers Indices. Here again, you can evaluate the entire state of a sector in one glance to compare different products and projects with ease. Lending by @drethereum \u00b6 The same applies for lending protocols, you can easily check the state of lending as a sector and compare protocol performance.","title":"Sector Dashboards"},{"location":"getting-started/use-cases/sector-dashboards/#dex-metrics-by-hagaetc","text":"This Dashboard allows you to view data and learn about the entire Decentralized Exchange sector at a glance. Here you can see metrics like Dex Volume, Market Share, and Total Volume across all exchanges, along with many more stats. Together they make it easy to compare the performance of different DEXes against each other and evaluate the performance of the entire sector. All of these stats and charts update in real time and are maintained by the community; if a new DEX wants to be added to this Dashboard, all they have to do is submit their code to our public GitHub. More on this in Spells .","title":"Dex Metrics by @hagaetc"},{"location":"getting-started/use-cases/sector-dashboards/#indices-by-0xboxer","text":"Another interesting sector-based Dune Dashboard covers Indices. Here again, you can evaluate the entire state of a sector in one glance to compare different products and projects with ease.","title":"Indices by @0xBoxer"},{"location":"getting-started/use-cases/sector-dashboards/#lending-by-drethereum","text":"The same applies for lending protocols, you can easily check the state of lending as a sector and compare protocol performance.","title":"Lending by @drethereum"},{"location":"getting-started/visualizations/","text":"With your blockchain data gathered and organized via Queries , it's time to add a bit more \u2728 with Visualizations. With Dune, you have a variety of ways to transform your Query data into visual data: Chart visualizations Bar charts Area charts Scatter charts Line charts Pie charts Other visualizations Counters Tables Combining these in a thoughtful manner allows you to best communicate your data to your audience via Dashboards and Embeds . Creating Visualizations \u00b6 You can create Visualizations from any Query results in seconds using Dune's Visualization engine. To get started, click the New Visualization button: You'll then see a dropdown appear with a list of the available types of Visualization. Select the one you want then click the Add visualization button to create your Visualization: You can create multiple Visualizations from one Query (to test which works best or reveal different insights) by repeating the steps above: Read more about how each Visualization type works here: Charts & Graphs Counters & Tables Sharing Visualizations \u00b6 Once you've created a Visualization you're happy with, you can share it with others on a Dashboard or be embedded on another website using an Embed . Adding Visualizations to Dashboards \u00b6 To add a Visualization to a Dashboard , click the Add to dashboard button: Then either click the Add button next to one of your existing Dashboards: Or create a new Dashboard and add your Visualization to it like so: Click the New dashboard button Add a name Click the Save dashboard button Click the Add button next to your new Dashboard To view the Dashboard you added your Visualization to, click it's name in the pop up: When you change your Visualization, it will be updated on your Dashboard too: To remove a Visualization from a Dashboard, click the Added button: Creating Visualizations Embeds \u00b6 To create an Embed from your Visualizations, first select the Visualization you want to embed, then click the Embed button above the Data Explorer :","title":"Visualizations"},{"location":"getting-started/visualizations/#creating-visualizations","text":"You can create Visualizations from any Query results in seconds using Dune's Visualization engine. To get started, click the New Visualization button: You'll then see a dropdown appear with a list of the available types of Visualization. Select the one you want then click the Add visualization button to create your Visualization: You can create multiple Visualizations from one Query (to test which works best or reveal different insights) by repeating the steps above: Read more about how each Visualization type works here: Charts & Graphs Counters & Tables","title":"Creating Visualizations"},{"location":"getting-started/visualizations/#sharing-visualizations","text":"Once you've created a Visualization you're happy with, you can share it with others on a Dashboard or be embedded on another website using an Embed .","title":"Sharing Visualizations"},{"location":"getting-started/visualizations/#adding-visualizations-to-dashboards","text":"To add a Visualization to a Dashboard , click the Add to dashboard button: Then either click the Add button next to one of your existing Dashboards: Or create a new Dashboard and add your Visualization to it like so: Click the New dashboard button Add a name Click the Save dashboard button Click the Add button next to your new Dashboard To view the Dashboard you added your Visualization to, click it's name in the pop up: When you change your Visualization, it will be updated on your Dashboard too: To remove a Visualization from a Dashboard, click the Added button:","title":"Adding Visualizations to Dashboards"},{"location":"getting-started/visualizations/#creating-visualizations-embeds","text":"To create an Embed from your Visualizations, first select the Visualization you want to embed, then click the Embed button above the Data Explorer :","title":"Creating Visualizations Embeds"},{"location":"getting-started/visualizations/charts-graphs/","text":"Graphs are great for condensing data points into a Visualization. With Dune, you can create the following types of graphs: Bar charts Area charts Scatter charts Line charts Pie charts Mixed graphs You can mix all of these graph types together in one Visualization, as long as your base graph isn't a Pie chart: All graph Visualizations share a common set of editing options, see the tabs below for how to configure each. Visualization Configuration Options \u00b6 Chart options Result data X-axis options Y-axis options Series options Pie options This section allows you to define how to display your data. Title The title will appear in all instances of this graph prominently at the top. The graph will always keep the name of the Query, even if you edit this. Show chart legend Ticking this box will enable or disable the legend for the chart. Enable stacking If applicable, ticking this box will stack the chart values on top of each other based on the x-axis values. If this is not turned on, the values will be plotted individually on the y-axis. The calculation underpinning this will always group the value corresponding to one value on the x-axis. Make sure your data is clean in able for this to work (avoid gaps in your data). Normalize to percentage data This will normalize the chart to display percentage values of the chosen data table. The calculation underpinning this will always group the value corresponding to one value on the x-axis. Make sure your data is clean in able for this to work (avoid gaps in your data). Show data labels Ticking this box leads to the display of the individual datapoints inside of the graph. This only makes sense in cases where you have few datapoints that are spread out far enough from each other to not overlap. Here you can pick the data points that are to be displayed. You can choose one x-axis and multiple y-axis. Alternatively, you can also choose one data series on the y-axis and choose to group it by a different column of your table (as shown in the example above). Using these options you can influence how your x-axis data gets displayed. Axis title This field allows you to specify a title for your x-axis. Sort Values by ticking this box you can specify if you want the values in your chart to be ordered. If your x-axis is a time series, this will automatically happen. Reverse value Ticking this box will reverse the order of the values on the x-axis. Logarithmic Ticking this box will make your x-axis values display __ logarithmically. With these options you can influence how your x-axis data gets displayed. Axis title This field allows you to specify a title for your y-axis. Logarithmic Ticking this box will make your x-axis values display __ logarithmically. Enable right y-axis Ticking this box will enable an additional y-axis that you can plot values on. You can choose in the chart series section what you want to be displayed on the left and right axis. In this section of the Visualization editor you can finalize your graph. You can rename the \"series\" by simply clicking into the field. You can change the chart type by clicking into the dropdown. You can change the colors by clicking into the color box. Finally you can also change the order of the series. Picking Colors You can pick colors with your browser native color selector. This might look slightly different for you depending on which browser you use. Label format This field allows you to define the tick format of the data labels in your pie chart. X/Y-axis Tick and Label formats \u00b6 Tick formats change how numeric values and axis labels in your graphs are displayed. Here's how to format them: Starting Value Tick/Label format Output Description 1256784.3745 [blank] 1256784.3745000 Displays the number 7 decimal precision. 1256784.3745 0 1256784 Displays only the integer. 1256784.3745 0,0 1,256,784 Only displays the integer with comma separation. 1256784.3745 0,0.00 1,256,784.38 Displays the number with [x] decimal precision, where [x] is the number of 0 you add after the decimal point. 1256784.3745 0.0a 1.2M Displays the number with [x] precision and a letter based on the number's 1e[y] power (eg \"m\" for million, \"b\" for billion) 1256784.3745 $0.0a $1.2M Adds a \"$\" to the number. Works with all formats above though use of the a suffix is recommended. Currently the only \"$\" is the only supported currency symbol.","title":"Charts & Graphs"},{"location":"getting-started/visualizations/charts-graphs/#visualization-configuration-options","text":"Chart options Result data X-axis options Y-axis options Series options Pie options This section allows you to define how to display your data. Title The title will appear in all instances of this graph prominently at the top. The graph will always keep the name of the Query, even if you edit this. Show chart legend Ticking this box will enable or disable the legend for the chart. Enable stacking If applicable, ticking this box will stack the chart values on top of each other based on the x-axis values. If this is not turned on, the values will be plotted individually on the y-axis. The calculation underpinning this will always group the value corresponding to one value on the x-axis. Make sure your data is clean in able for this to work (avoid gaps in your data). Normalize to percentage data This will normalize the chart to display percentage values of the chosen data table. The calculation underpinning this will always group the value corresponding to one value on the x-axis. Make sure your data is clean in able for this to work (avoid gaps in your data). Show data labels Ticking this box leads to the display of the individual datapoints inside of the graph. This only makes sense in cases where you have few datapoints that are spread out far enough from each other to not overlap. Here you can pick the data points that are to be displayed. You can choose one x-axis and multiple y-axis. Alternatively, you can also choose one data series on the y-axis and choose to group it by a different column of your table (as shown in the example above). Using these options you can influence how your x-axis data gets displayed. Axis title This field allows you to specify a title for your x-axis. Sort Values by ticking this box you can specify if you want the values in your chart to be ordered. If your x-axis is a time series, this will automatically happen. Reverse value Ticking this box will reverse the order of the values on the x-axis. Logarithmic Ticking this box will make your x-axis values display __ logarithmically. With these options you can influence how your x-axis data gets displayed. Axis title This field allows you to specify a title for your y-axis. Logarithmic Ticking this box will make your x-axis values display __ logarithmically. Enable right y-axis Ticking this box will enable an additional y-axis that you can plot values on. You can choose in the chart series section what you want to be displayed on the left and right axis. In this section of the Visualization editor you can finalize your graph. You can rename the \"series\" by simply clicking into the field. You can change the chart type by clicking into the dropdown. You can change the colors by clicking into the color box. Finally you can also change the order of the series. Picking Colors You can pick colors with your browser native color selector. This might look slightly different for you depending on which browser you use. Label format This field allows you to define the tick format of the data labels in your pie chart.","title":"Visualization Configuration Options"},{"location":"getting-started/visualizations/charts-graphs/#xy-axis-tick-and-label-formats","text":"Tick formats change how numeric values and axis labels in your graphs are displayed. Here's how to format them: Starting Value Tick/Label format Output Description 1256784.3745 [blank] 1256784.3745000 Displays the number 7 decimal precision. 1256784.3745 0 1256784 Displays only the integer. 1256784.3745 0,0 1,256,784 Only displays the integer with comma separation. 1256784.3745 0,0.00 1,256,784.38 Displays the number with [x] decimal precision, where [x] is the number of 0 you add after the decimal point. 1256784.3745 0.0a 1.2M Displays the number with [x] precision and a letter based on the number's 1e[y] power (eg \"m\" for million, \"b\" for billion) 1256784.3745 $0.0a $1.2M Adds a \"$\" to the number. Works with all formats above though use of the a suffix is recommended. Currently the only \"$\" is the only supported currency symbol.","title":"X/Y-axis Tick and Label formats"},{"location":"getting-started/visualizations/other-visualizations/","text":"Here are a few non-graph visualizations you can make with Dune! Tables \u00b6 Tables are the default Visualization you'll find labeled Query results whenever you create and run a Query: You can also make more Tables to display your data differently using the New visualization button and drop down menu: Configuring your Table \u00b6 Table options \u00b6 Title The Title appears at the top of your Table. Leaving default value ( Table ) or making this blank makes your Table title the same as your Query's title/name. Adding any other value to this field will add that value first, followed by your Query Name: Note: the default value for \"Query Results\" is treated like an added value. \"Column [x]:\" options \u00b6 You can configure the following options for each column in your Table Title Align Format Hide Column The Title appears at the top of your Table. Leaving this blank makes your column title the same as your it's Dune database name. This changes the text alignment for the column data and title. Allows you to adjust the numerical format of your data following the X/Y-axis Tick and Label formats here . Hides this column from your table. Numerical Column options \u00b6 Columns that return numerical data have these additional options: Type Colored Values Normal simply displays the column's numerical data. Progress bar shows the column's numerical data with a progress bar visual that is \"full\" for the columns highest value and \"nearly empty\" for the columns lowest value, with the rest of the data ranging in between: Check these boxes to color Positive Values Green and Negative Values Red . Counters \u00b6 Counters are a great way to provide your audience with immediate \"on a glance\" stats. Configuring your Counter \u00b6 Counter options \u00b6 In this section you can define what kind of data the counter should display: Title Column Row The Title will appear in all instances of this graph prominently at the top If left blank the Query name will be the only thing that is left standing In this field you can define which column the counter should show. This field can be used to define which row of the underlying data table you want displayed e.g. row 1 Usually this requires you to sort or limit your Query results in order for row 1 to show the wanted results. Formatting \u00b6 This section is where you can adjust how your numerical data is displayed. Prefix Suffix Label Decimals This field allows you to define a prefix for your counter value. e.g.: $ , \u20ac , \u039e , \u0e3f This field allows you to define a suffix for your counter value. This field allows you to define a label for your counter value. The label will appear beneath the counter value as text. In this field you can choose how many decimals you want displayed for your counter This is currently limited to 3 decimal places.","title":"Other Visualizations"},{"location":"getting-started/visualizations/other-visualizations/#tables","text":"Tables are the default Visualization you'll find labeled Query results whenever you create and run a Query: You can also make more Tables to display your data differently using the New visualization button and drop down menu:","title":"Tables"},{"location":"getting-started/visualizations/other-visualizations/#configuring-your-table","text":"","title":"Configuring your Table"},{"location":"getting-started/visualizations/other-visualizations/#table-options","text":"Title The Title appears at the top of your Table. Leaving default value ( Table ) or making this blank makes your Table title the same as your Query's title/name. Adding any other value to this field will add that value first, followed by your Query Name: Note: the default value for \"Query Results\" is treated like an added value.","title":"Table options"},{"location":"getting-started/visualizations/other-visualizations/#column-x-options","text":"You can configure the following options for each column in your Table Title Align Format Hide Column The Title appears at the top of your Table. Leaving this blank makes your column title the same as your it's Dune database name. This changes the text alignment for the column data and title. Allows you to adjust the numerical format of your data following the X/Y-axis Tick and Label formats here . Hides this column from your table.","title":"\"Column [x]:\" options"},{"location":"getting-started/visualizations/other-visualizations/#numerical-column-options","text":"Columns that return numerical data have these additional options: Type Colored Values Normal simply displays the column's numerical data. Progress bar shows the column's numerical data with a progress bar visual that is \"full\" for the columns highest value and \"nearly empty\" for the columns lowest value, with the rest of the data ranging in between: Check these boxes to color Positive Values Green and Negative Values Red .","title":"Numerical Column options"},{"location":"getting-started/visualizations/other-visualizations/#counters","text":"Counters are a great way to provide your audience with immediate \"on a glance\" stats.","title":"Counters"},{"location":"getting-started/visualizations/other-visualizations/#configuring-your-counter","text":"","title":"Configuring your Counter"},{"location":"getting-started/visualizations/other-visualizations/#counter-options","text":"In this section you can define what kind of data the counter should display: Title Column Row The Title will appear in all instances of this graph prominently at the top If left blank the Query name will be the only thing that is left standing In this field you can define which column the counter should show. This field can be used to define which row of the underlying data table you want displayed e.g. row 1 Usually this requires you to sort or limit your Query results in order for row 1 to show the wanted results.","title":"Counter options"},{"location":"getting-started/visualizations/other-visualizations/#formatting","text":"This section is where you can adjust how your numerical data is displayed. Prefix Suffix Label Decimals This field allows you to define a prefix for your counter value. e.g.: $ , \u20ac , \u039e , \u0e3f This field allows you to define a suffix for your counter value. This field allows you to define a label for your counter value. The label will appear beneath the counter value as text. In this field you can choose how many decimals you want displayed for your counter This is currently limited to 3 decimal places.","title":"Formatting"},{"location":"reference/","text":"Reference is the place to find answers to your \"who, what, where\" questions eg: What data exists in Dune Tables? What tools are helpful for building dashboards? Where do I find Dune events and support? We've also included a couple of supplemental resources like Wizard Tools that we think you'll find helpful in your work as a Wizard! \ud83e\uddd9","title":"Reference"},{"location":"reference/dune-bounties/","text":"How many Holders does BAYC have? Wanna do some whale watching? How is the UST depeg going? How many users are using Uniswap V3 in a day? Over the past 3 years, Dune has become the go-to solution for answering questions like these via on-demand crypto analytics. When it comes to blockchain data analysis, Dune has it all and can do it all, but there's always a frontier left unexplored. While Dune has created the tools to make \u2728 from the \ud83c\udf00 of crypto data, it's through the skills and bravery of our community of Wizards that data is surfaced and made understandable for projects and the public alike. Many of the Dashboards and Queries you see on Dune today were made out of sheer curiosity and enjoyment. Some were done for clout and stars. But there's nothing like cold, hard, cash to make sure top tier Wizards come to your aid. The problem: Wizards + Projects != \u2728\ud83d\udcca \u00b6 Projects need data, Wizards want jobs. It's hard for one to find the other. Until now, analytics tasks have been spread out between Twitter, Discord, Gitcoin, Layer3, Notion boards and a few other dark and hidden places. It's hard for Wizards to keep track of all these locations they should look for jobs and there's unnecessary friction in the application and work-delivery process. For Projects , the lack of organization makes it exceedingly hard to find the right Wizard for the job and introduces unnecessary overhead in the management process. Fundamentally, this problem is solved through the creation of one marketplace for helping Wizards and Projects make the data flow. This one solution should be able to handle: task creation the application process communication reputation payments and it should be web3 native. Thankfully this marketplace now exists! The solution: Dune Bounties \u00b6 Leveraging dework.xyz , we've set up a bounty board to manage our Dune internal bounty programs - and we've set it up so it's accessible for other projects and organizations to make use of this infrastructure! Dework describes itself as a \u201cweb3 native Trello with payments and credentialing\". What exactly does that mean? Let's hear what Lonis, co-founder of Dework, has to say: Dework offers infrastructure for us to write out tasks on our own board and allows other organizations to seamlessly connect with Dune Wizards by simply specifying \u201cDune Analytics\u201d as Skill. All open tasks that have the Dune Analytics Skill attached will appear in the Dune hub , making it easy for Wizards to find tasks that need their skills. For projects in need of Wizards \u00b6 If you are an organization in need of web3 analytics, it\u2019s now easier than ever to connect with Dune Wizards! You can either do all of this by yourself or ask for our help via the typeform in the \"supported approach\" tab in the section below, we are happy to help! Supported approach Self Serve Fill in this Typeform We know setting up Dework, defining bounties, and managing your contributors can be quite a challenge. Reach out to us by clicking the button below and filling out our Typeform and we'll happily assist you in onboarding and running successful bounties. Typeform Dework's Documentation is the best in-depth resource for getting started, but here's a short breakdown for convenience: Create an organization If you don\u2019t have a Dework organization yet, the first step is to create one. At a minimum, we recommend including a description, an icon and a link to your socials so Wizards know who they are working with. We also strongly recommend setting up the Discord integrations to allow for easy communication with the chosen Applicants. Define tasks After completing the initial setup, you can start creating tasks for anything, but to work with Dune Wizards, you'll want to define tasks with the \"Dune Analytics\" Skill\" . Once you have created a task, the task will be: in your board and in the Dune hub . From there on out, people can find your open task and apply or compete. Choose an applicant If you have defined a task that needs to be assigned to someone, you will get notifications within Dework and can vet the applicant using their work history, GitHub profile and any other attached information on their profile. We strongly recommend spending time and effort vetting your applicants to have a smooth bounty process. Once you choose an applicant to work with, they'll become a Dework \"contributor\" to your project. Review the work Once the Wizard applicant you have chosen has submitted their work, you can start reviewing the completed task. If it is satisfactory, you mark the task as done and initiate the payment process. Pay your contributor Dework integrates with Metamask, Gnosis Safe, Utopia Labs and even Phantom wallet for Solana based payments. You can choose whatever works best for you and your Wizard contributor here. For Wizards looking for work \u00b6 If you are a Wizard or a aspiring Wizard, join the Dune Analytics organization on Dework and start looking for tasks: On our board On the Dune hub Be sure to complete your profile with relevant links to your socials, GitHub and Dune profile so organizations can easily assess your skills and decide you're a perfect fit (the easier it is for them to see what you can do, the more likely they'll hire you!). Some notes about working on Dune \u00b6 Dune can roughly be separated into two parts: The App Layer The Data Layer . In the App Layer , you can find Queries , Visualizations and Dashboards . Everything in the App Layer is public by default and can be utilized by other Wizards, but the work produced in the App Layer is not very persistent and most importantly doesn't enable other Wizard analysts to easily build on top of this work. Dune's Data Layer allows you and the Wizards you work with to produce scalable and persistent work by standardizing and normalizing data. We call this data layer Spellbook . A good example of the power of working in the Dune Data Layer is OpenSea's standardization and normalization of all NFT trades across all chains and versions . By transforming their raw data and adding it to the opensea.trades table inside of Spellbook, 2 things can happen: Every Dune Wizard can easily work with OpenSea's data as it's cleaned and standardized. The data can be referenced in other Spellbook tables (\"Spells\") like nft.trades ; this makes it even more likely that Wizards will incorporate OpenSea data into their work as the nft.trades table contains data from all marketplaces across all chains. In this way, a project like yours working in the Spellbook Data Layer can get a lot of leverage not only from whatever data analysis projects and visualizations you commission, but by making it more likely other Wizards will find and use your data, build it into their Dashboards, and generate interest in your project. Though not necessarily required for your project, getting your data normalized, standardized and possibly inserted into one of our sector level tables like nft.trades is definitely recommended! Once that is done, people working in the App layer will have a much easier time building good queries, visualizations and dashboards since the hardest data engineering parts are already taken care of. If this all sounds confusing to you, don't worry we can advise you in this process! Reach out via the Typeform in the \"Supported approach\" tab above, or ask about building your project in Spellbook in our #spellbook Discord channel ! TL;DR We suggest working in this order so your data flows efficiently: Build Spells in the Data Layer to take care of the data engineering Build cool stuff in the App Layer to surface findings Dune Bounties FAQ \u00b6 I want to create a requests but how much should I pay/offer? \u00b6 Dune is an open platform on which you can build all kinds of analysis, Dashboards and Spells, so the official answer here is the dreaded \"it depends.\" Going rates for Freelance Dune Wizards seem to be anywhere between $30-$100 per hour. And you can always ask in our #bounty-questions Discord or fill out the Typeform in the \"Supported approach\" tab above to get some help on this! Does Dune take a cut of bounty payments? \u00b6 Dune does not take a cut in any bounty payments. I don't have time/capacity to do this myself, can you run this for me? \u00b6 In some cases, the Dune team can actually run entire bounty campaigns/contests for you, but we can't offer this for every organization. Fill out the Typeform in the \"Supported approach\" tab above and we'll let you know what we can do to help! How do Wizards get paid for bounties? \u00b6 Dework has a native payment feature. Wizards simply connect their wallet in their Dework profile and will get paid as soon as the bounty is paid out by the Project that created the task. Can I run contests on Dework? \u00b6 You can indeed run contests on Dework, they are called \"multiple submissions.\" You can learn more in Dework's documentation . After running contests it often makes sense to give one of the winners a follow-up task to reconcile the best ideas of all submitted dashboards into one final version. Is all of this public? \u00b6 Dework allows you to define tasks privately and publicly, if you wanted to you could for example limit tasks for only members of your Discord. How do I choose the right applicant? \u00b6 You can click on any profile in Dework to see what the Credentials of the person are. For example: https://app.dework.xyz/profile/hamzat_iii What's your recommend approach to organizing bounties for my Project and needs? \u00b6 Our advice is to first run specific tasks for adding your data to Spellbook first and run a Dashboard design contest afterwards. This way, your data will be easily accessible to any Dune Wizard in any Queries and Dashboards they imagine Then, by running a contest for Dashboard designs using Dework's \"multiple submissions\" feature, you'll ensure specific Dashboards you want are created while also leaving room to be surprised by our Wizard's creativity.","title":"Dune Bounties"},{"location":"reference/dune-bounties/#the-problem-wizards-projects","text":"Projects need data, Wizards want jobs. It's hard for one to find the other. Until now, analytics tasks have been spread out between Twitter, Discord, Gitcoin, Layer3, Notion boards and a few other dark and hidden places. It's hard for Wizards to keep track of all these locations they should look for jobs and there's unnecessary friction in the application and work-delivery process. For Projects , the lack of organization makes it exceedingly hard to find the right Wizard for the job and introduces unnecessary overhead in the management process. Fundamentally, this problem is solved through the creation of one marketplace for helping Wizards and Projects make the data flow. This one solution should be able to handle: task creation the application process communication reputation payments and it should be web3 native. Thankfully this marketplace now exists!","title":"The problem: Wizards + Projects != \u2728\ud83d\udcca"},{"location":"reference/dune-bounties/#the-solution-dune-bounties","text":"Leveraging dework.xyz , we've set up a bounty board to manage our Dune internal bounty programs - and we've set it up so it's accessible for other projects and organizations to make use of this infrastructure! Dework describes itself as a \u201cweb3 native Trello with payments and credentialing\". What exactly does that mean? Let's hear what Lonis, co-founder of Dework, has to say: Dework offers infrastructure for us to write out tasks on our own board and allows other organizations to seamlessly connect with Dune Wizards by simply specifying \u201cDune Analytics\u201d as Skill. All open tasks that have the Dune Analytics Skill attached will appear in the Dune hub , making it easy for Wizards to find tasks that need their skills.","title":"The solution: Dune Bounties"},{"location":"reference/dune-bounties/#for-projects-in-need-of-wizards","text":"If you are an organization in need of web3 analytics, it\u2019s now easier than ever to connect with Dune Wizards! You can either do all of this by yourself or ask for our help via the typeform in the \"supported approach\" tab in the section below, we are happy to help! Supported approach Self Serve Fill in this Typeform We know setting up Dework, defining bounties, and managing your contributors can be quite a challenge. Reach out to us by clicking the button below and filling out our Typeform and we'll happily assist you in onboarding and running successful bounties. Typeform Dework's Documentation is the best in-depth resource for getting started, but here's a short breakdown for convenience: Create an organization If you don\u2019t have a Dework organization yet, the first step is to create one. At a minimum, we recommend including a description, an icon and a link to your socials so Wizards know who they are working with. We also strongly recommend setting up the Discord integrations to allow for easy communication with the chosen Applicants. Define tasks After completing the initial setup, you can start creating tasks for anything, but to work with Dune Wizards, you'll want to define tasks with the \"Dune Analytics\" Skill\" . Once you have created a task, the task will be: in your board and in the Dune hub . From there on out, people can find your open task and apply or compete. Choose an applicant If you have defined a task that needs to be assigned to someone, you will get notifications within Dework and can vet the applicant using their work history, GitHub profile and any other attached information on their profile. We strongly recommend spending time and effort vetting your applicants to have a smooth bounty process. Once you choose an applicant to work with, they'll become a Dework \"contributor\" to your project. Review the work Once the Wizard applicant you have chosen has submitted their work, you can start reviewing the completed task. If it is satisfactory, you mark the task as done and initiate the payment process. Pay your contributor Dework integrates with Metamask, Gnosis Safe, Utopia Labs and even Phantom wallet for Solana based payments. You can choose whatever works best for you and your Wizard contributor here.","title":"For projects in need of Wizards"},{"location":"reference/dune-bounties/#for-wizards-looking-for-work","text":"If you are a Wizard or a aspiring Wizard, join the Dune Analytics organization on Dework and start looking for tasks: On our board On the Dune hub Be sure to complete your profile with relevant links to your socials, GitHub and Dune profile so organizations can easily assess your skills and decide you're a perfect fit (the easier it is for them to see what you can do, the more likely they'll hire you!).","title":"For Wizards looking for work"},{"location":"reference/dune-bounties/#some-notes-about-working-on-dune","text":"Dune can roughly be separated into two parts: The App Layer The Data Layer . In the App Layer , you can find Queries , Visualizations and Dashboards . Everything in the App Layer is public by default and can be utilized by other Wizards, but the work produced in the App Layer is not very persistent and most importantly doesn't enable other Wizard analysts to easily build on top of this work. Dune's Data Layer allows you and the Wizards you work with to produce scalable and persistent work by standardizing and normalizing data. We call this data layer Spellbook . A good example of the power of working in the Dune Data Layer is OpenSea's standardization and normalization of all NFT trades across all chains and versions . By transforming their raw data and adding it to the opensea.trades table inside of Spellbook, 2 things can happen: Every Dune Wizard can easily work with OpenSea's data as it's cleaned and standardized. The data can be referenced in other Spellbook tables (\"Spells\") like nft.trades ; this makes it even more likely that Wizards will incorporate OpenSea data into their work as the nft.trades table contains data from all marketplaces across all chains. In this way, a project like yours working in the Spellbook Data Layer can get a lot of leverage not only from whatever data analysis projects and visualizations you commission, but by making it more likely other Wizards will find and use your data, build it into their Dashboards, and generate interest in your project. Though not necessarily required for your project, getting your data normalized, standardized and possibly inserted into one of our sector level tables like nft.trades is definitely recommended! Once that is done, people working in the App layer will have a much easier time building good queries, visualizations and dashboards since the hardest data engineering parts are already taken care of. If this all sounds confusing to you, don't worry we can advise you in this process! Reach out via the Typeform in the \"Supported approach\" tab above, or ask about building your project in Spellbook in our #spellbook Discord channel ! TL;DR We suggest working in this order so your data flows efficiently: Build Spells in the Data Layer to take care of the data engineering Build cool stuff in the App Layer to surface findings","title":"Some notes about working on Dune"},{"location":"reference/dune-bounties/#dune-bounties-faq","text":"","title":"Dune Bounties FAQ"},{"location":"reference/dune-bounties/#i-want-to-create-a-requests-but-how-much-should-i-payoffer","text":"Dune is an open platform on which you can build all kinds of analysis, Dashboards and Spells, so the official answer here is the dreaded \"it depends.\" Going rates for Freelance Dune Wizards seem to be anywhere between $30-$100 per hour. And you can always ask in our #bounty-questions Discord or fill out the Typeform in the \"Supported approach\" tab above to get some help on this!","title":"I want to create a requests but how much should I pay/offer?"},{"location":"reference/dune-bounties/#does-dune-take-a-cut-of-bounty-payments","text":"Dune does not take a cut in any bounty payments.","title":"Does Dune take a cut of bounty payments?"},{"location":"reference/dune-bounties/#i-dont-have-timecapacity-to-do-this-myself-can-you-run-this-for-me","text":"In some cases, the Dune team can actually run entire bounty campaigns/contests for you, but we can't offer this for every organization. Fill out the Typeform in the \"Supported approach\" tab above and we'll let you know what we can do to help!","title":"I don't have time/capacity to do this myself, can you run this for me?"},{"location":"reference/dune-bounties/#how-do-wizards-get-paid-for-bounties","text":"Dework has a native payment feature. Wizards simply connect their wallet in their Dework profile and will get paid as soon as the bounty is paid out by the Project that created the task.","title":"How do Wizards get paid for bounties?"},{"location":"reference/dune-bounties/#can-i-run-contests-on-dework","text":"You can indeed run contests on Dework, they are called \"multiple submissions.\" You can learn more in Dework's documentation . After running contests it often makes sense to give one of the winners a follow-up task to reconcile the best ideas of all submitted dashboards into one final version.","title":"Can I run contests on Dework?"},{"location":"reference/dune-bounties/#is-all-of-this-public","text":"Dework allows you to define tasks privately and publicly, if you wanted to you could for example limit tasks for only members of your Discord.","title":"Is all of this public?"},{"location":"reference/dune-bounties/#how-do-i-choose-the-right-applicant","text":"You can click on any profile in Dework to see what the Credentials of the person are. For example: https://app.dework.xyz/profile/hamzat_iii","title":"How do I choose the right applicant?"},{"location":"reference/dune-bounties/#whats-your-recommend-approach-to-organizing-bounties-for-my-project-and-needs","text":"Our advice is to first run specific tasks for adding your data to Spellbook first and run a Dashboard design contest afterwards. This way, your data will be easily accessible to any Dune Wizard in any Queries and Dashboards they imagine Then, by running a contest for Dashboard designs using Dework's \"multiple submissions\" feature, you'll ensure specific Dashboards you want are created while also leaving room to be surprised by our Wizard's creativity.","title":"What's your recommend approach to organizing bounties for my Project and needs?"},{"location":"reference/dune-explorer/","text":"Here's a quick overview of the Dune Explorer! We'll quickly review the different ways you can search for Dashboards, Queries, and Wizards to find inspiration... and forkable Queries to help you get to making \ud83d\udcab faster. When you head to dune.com , you'll land on the Dashboards section of the Dune Explorer. You'll also see tabs at the top for Queries, Wizards, and Teams. Dashboards \u00b6 By default, Dashboard Explorer shows Dashboards that have been Trending (getting a lot of stars) in the last 4 hours. Looking to the right sidebar, we can sort this list by: Favorites - Dashboards that have gotten the most stars in the last 1/7/30 days as well as all time. Trending - Dashboards that have been getting more attention in the last 1/4/24 hours. New - Dashboards sorted by publish time. And we can filter this list by: Using the \"Search for dashboards\" text field to find dashboards by keyword. Click on one of the \"Popular dashboard tags\" to see dashboards with that tag. Learn how to build your own Dashboards here . Queries \u00b6 By default, Queries Explorer shows Queries that have gotten the most stars in the last 7 days. Looking to the right sidebar, we can sort this list by: Favorites - Queries that have gotten the most stars in the last 1/7/30 days as well as all time. New - Dashboards sorted by publish time. And we can filter this list by: Using the \"Search for queries\" text field to find dashboards by keyword. Click on one of the \"Popular query tags\" to see dashboards with that tag. Learn more about making Queries here . Wizards \u00b6 By default, Wizards Explorer shows Wizards that have the most stars. Use the right sidebar to search for a specific Wizard. And become a Wizard yourself with our Query Quick Start here ! \ud83e\uddd9 Teams \u00b6 By default, Teams Explorer shows Teams that have the most stars. Use the right sidebar to search for a specific Team. Learn more about Teams here !","title":"The Dune Explorer"},{"location":"reference/dune-explorer/#dashboards","text":"By default, Dashboard Explorer shows Dashboards that have been Trending (getting a lot of stars) in the last 4 hours. Looking to the right sidebar, we can sort this list by: Favorites - Dashboards that have gotten the most stars in the last 1/7/30 days as well as all time. Trending - Dashboards that have been getting more attention in the last 1/4/24 hours. New - Dashboards sorted by publish time. And we can filter this list by: Using the \"Search for dashboards\" text field to find dashboards by keyword. Click on one of the \"Popular dashboard tags\" to see dashboards with that tag. Learn how to build your own Dashboards here .","title":"Dashboards"},{"location":"reference/dune-explorer/#queries","text":"By default, Queries Explorer shows Queries that have gotten the most stars in the last 7 days. Looking to the right sidebar, we can sort this list by: Favorites - Queries that have gotten the most stars in the last 1/7/30 days as well as all time. New - Dashboards sorted by publish time. And we can filter this list by: Using the \"Search for queries\" text field to find dashboards by keyword. Click on one of the \"Popular query tags\" to see dashboards with that tag. Learn more about making Queries here .","title":"Queries"},{"location":"reference/dune-explorer/#wizards","text":"By default, Wizards Explorer shows Wizards that have the most stars. Use the right sidebar to search for a specific Wizard. And become a Wizard yourself with our Query Quick Start here ! \ud83e\uddd9","title":"Wizards"},{"location":"reference/dune-explorer/#teams","text":"By default, Teams Explorer shows Teams that have the most stars. Use the right sidebar to search for a specific Team. Learn more about Teams here !","title":"Teams"},{"location":"reference/events/","text":"Dune is all about the community and communities all have get togethers! Hang out anytime in our Community Discord to get to know (and learn from) your fellow wizards. \ud83e\uddd9 To keep track of our live (streamed and IRL events), check out our events Google calendar @ events.dune.com . Here's how to subscribe to that calendar and add it to yours:","title":"Events"},{"location":"reference/press-kit/","text":"Press Kit \u00b6 How to reference Dune data! \u00b6 Content on Dune is first and foremost the product of the creators (\"Wizards\") who build the Queries, Visualizations, and Dashboards we reference and share. Any mention or use of data or Visualizations you find on our platform should credit the creators of the Specific Query, Dashboard, or Data specific - not just \"Dune\". Crediting should be done as follows: \" @rchen8 via OpenSea monthly volume (Ethereum) \". If you're referencing a Dashboard, you can find the creator's User Name as well as the Dashboard's name at the top left of the page. The link to the users profile can be found by clicking their User Name: Likewise, you can find their User Name/Profile link and the Query/Visualization name at the top left side of a Query page: For more info on how to get Embed links for Visualizations you want to share, see this page ! Primary - Standard Logo \u00b6 This is the primary Dune logo. It should be used in this format whenever possible. dune-standard-logo.svg dune-standard-logo@2x.png dune-standard-logo-dark.svg dune-standard-logo-dark@2x.png Vertical logo \u00b6 dune-vertical-logo.svg dune-vertical-logo@2x.png dune-vertical-logo-dark.svg dune-vertical-logo-dark@2x.png Icon only \u00b6 dune-icon-only.svg dune-icon-only@2x.png Complete set of all logos \u00b6 Dune-logo-Full.zip","title":"Press Kit"},{"location":"reference/press-kit/#press-kit","text":"","title":"Press Kit"},{"location":"reference/press-kit/#how-to-reference-dune-data","text":"Content on Dune is first and foremost the product of the creators (\"Wizards\") who build the Queries, Visualizations, and Dashboards we reference and share. Any mention or use of data or Visualizations you find on our platform should credit the creators of the Specific Query, Dashboard, or Data specific - not just \"Dune\". Crediting should be done as follows: \" @rchen8 via OpenSea monthly volume (Ethereum) \". If you're referencing a Dashboard, you can find the creator's User Name as well as the Dashboard's name at the top left of the page. The link to the users profile can be found by clicking their User Name: Likewise, you can find their User Name/Profile link and the Query/Visualization name at the top left side of a Query page: For more info on how to get Embed links for Visualizations you want to share, see this page !","title":"How to reference Dune data!"},{"location":"reference/press-kit/#primary-standard-logo","text":"This is the primary Dune logo. It should be used in this format whenever possible. dune-standard-logo.svg dune-standard-logo@2x.png dune-standard-logo-dark.svg dune-standard-logo-dark@2x.png","title":"Primary - Standard Logo"},{"location":"reference/press-kit/#vertical-logo","text":"dune-vertical-logo.svg dune-vertical-logo@2x.png dune-vertical-logo-dark.svg dune-vertical-logo-dark@2x.png","title":"Vertical logo"},{"location":"reference/press-kit/#icon-only","text":"dune-icon-only.svg dune-icon-only@2x.png","title":"Icon only"},{"location":"reference/press-kit/#complete-set-of-all-logos","text":"Dune-logo-Full.zip","title":"Complete set of all logos"},{"location":"reference/pro/","text":"Dune pro let's you access a host of premium features including: Private Queries and Dashboards CSV Exports Higher Priority in the Query processing queue No Dune watermarks Processing for up to 6 Queries at once Learn more and get started on our Pricing page !","title":"Dune Pro"},{"location":"reference/recommended-reading/","text":"Here are a few reading resources to help you in your web3 data analytics journey! Have a resource you think we should include? Propose your changes here ! Wizard Content \u00b6 A few members of our Wizard community are also amazing content creators, check out their work here: Journey to the Centre of Arakis by @1chioku Andrew Hong's Mirror.xyz by @ilemi NiftyTable by @niftytable General Blockchain Info \u00b6 Immersion Den - a giant collection of web3 information sources. DeFi \u00b6 Biggest DeFi Hacks DAOs \u00b6 DAOs: Absorbing the Internet Bridges/L2 \u00b6 The ultimate guide to L2s on Ethereum EIPs \u00b6 https://eips.ethereum.org/ News and Info: \u00b6 21Shares Blockworks Decrypt CoinDesk Week in Ethereum News Bankless The Daily Ape Finematics (helpful youtube videos) Gaby's Web3 reading list Newsletters \u00b6 THe Dune Digest CryptoPragmatist Messari The Daily Gwei OurNetwork data newsletter NotBoring Cool Blockchain Data Websites \u00b6 cryptofees.info ultrasound.money fees.wtf liquidated.live","title":"Recommended Reading"},{"location":"reference/recommended-reading/#wizard-content","text":"A few members of our Wizard community are also amazing content creators, check out their work here: Journey to the Centre of Arakis by @1chioku Andrew Hong's Mirror.xyz by @ilemi NiftyTable by @niftytable","title":"Wizard Content"},{"location":"reference/recommended-reading/#general-blockchain-info","text":"Immersion Den - a giant collection of web3 information sources.","title":"General Blockchain Info"},{"location":"reference/recommended-reading/#defi","text":"Biggest DeFi Hacks","title":"DeFi"},{"location":"reference/recommended-reading/#daos","text":"DAOs: Absorbing the Internet","title":"DAOs"},{"location":"reference/recommended-reading/#bridgesl2","text":"The ultimate guide to L2s on Ethereum","title":"Bridges/L2"},{"location":"reference/recommended-reading/#eips","text":"https://eips.ethereum.org/","title":"EIPs"},{"location":"reference/recommended-reading/#news-and-info","text":"21Shares Blockworks Decrypt CoinDesk Week in Ethereum News Bankless The Daily Ape Finematics (helpful youtube videos) Gaby's Web3 reading list","title":"News and Info:"},{"location":"reference/recommended-reading/#newsletters","text":"THe Dune Digest CryptoPragmatist Messari The Daily Gwei OurNetwork data newsletter NotBoring","title":"Newsletters"},{"location":"reference/recommended-reading/#cool-blockchain-data-websites","text":"cryptofees.info ultrasound.money fees.wtf liquidated.live","title":"Cool Blockchain Data Websites"},{"location":"reference/support-feedback/","text":"How to Get Support \u00b6 If you can't seem to find the help you're looking for, here's the best way to get it: 1. Try searching using the Search bar above \u00b6 We know you probably did that already but just in case :) 2. Discord! \u00b6 Here are a few relevant support channels: #beginners - if you're just getting going and have what feels like a stupid question (note: it's probably not) #query-questions is great for getting help on building amazing queries. #data-tables is the place for questions related to the data you'll find in Dune. #decoding is for smart contract decoding related questions. #dashboards-viz is for Dashboard and Visualization questions. #spellbook is for Spellbook related questions. #dune-api is for API related questions. 3. Weekly Office Hours \u00b6 Lastly, you can join our weekly office hours to get live help - find the next one on our Events Calendar here ! How to Leave Feedback \u00b6 For general Dune app feedback, leave a suggestion on our Canny board here . We also have 3 feedback channels in Discord: #general-feedback for general feedback. #bugs bug reports. #feature-requests for any cool ideas you have. If you're working with any of our public repositories (e.g. Spellbook), you can also open a Github issue. For Docs feedback fixes or additions, click the pencil icon to the right of the page title and open a GitHub issue or submit a pull request: Account issues \u00b6 For any issues related to your account, billing or any other administrative problems, please send an email to support@dune.com .","title":"Support & Feedback"},{"location":"reference/support-feedback/#how-to-get-support","text":"If you can't seem to find the help you're looking for, here's the best way to get it:","title":"How to Get Support"},{"location":"reference/support-feedback/#1-try-searching-using-the-search-bar-above","text":"We know you probably did that already but just in case :)","title":"1. Try searching using the Search bar above"},{"location":"reference/support-feedback/#2-discord","text":"Here are a few relevant support channels: #beginners - if you're just getting going and have what feels like a stupid question (note: it's probably not) #query-questions is great for getting help on building amazing queries. #data-tables is the place for questions related to the data you'll find in Dune. #decoding is for smart contract decoding related questions. #dashboards-viz is for Dashboard and Visualization questions. #spellbook is for Spellbook related questions. #dune-api is for API related questions.","title":"2. Discord!"},{"location":"reference/support-feedback/#3-weekly-office-hours","text":"Lastly, you can join our weekly office hours to get live help - find the next one on our Events Calendar here !","title":"3. Weekly Office Hours"},{"location":"reference/support-feedback/#how-to-leave-feedback","text":"For general Dune app feedback, leave a suggestion on our Canny board here . We also have 3 feedback channels in Discord: #general-feedback for general feedback. #bugs bug reports. #feature-requests for any cool ideas you have. If you're working with any of our public repositories (e.g. Spellbook), you can also open a Github issue. For Docs feedback fixes or additions, click the pencil icon to the right of the page title and open a GitHub issue or submit a pull request:","title":"How to Leave Feedback"},{"location":"reference/support-feedback/#account-issues","text":"For any issues related to your account, billing or any other administrative problems, please send an email to support@dune.com .","title":"Account issues"},{"location":"reference/types-labels/","text":"While using the Data Explorer (and in reading these docs), you'll encounter a variety of labels. Here's what they all mean. Blockchain Icons \u00b6 Some data sets will have multiple blockchain icons - meaning data from each of those blockchains is available within that data set! Icon Description Ethereum blockchain Raw Data, Decoded Project, or Spell Gnosis Chain Raw Data, Decoded Project, or Spell Polygon blockchain Raw Data, Decoded Project, or Spell Optimism blockchain Raw Data, Decoded Project, or Spell Optimism (legacy) blockchain Raw Data, Decoded Project, or Spell BNB Chain Raw Data, Decoded Project, or Spell Solana blockchain Raw Data, Decoded Project, or Spell Arbitrum blockchain Raw Data, Decoded Project, or Spell Avalanche C-Chain Raw Data, Decoded Project, or Spell Dataset Icons \u00b6 Icon Description Data Table (Raw Data, Spell, or smart contract Event or Function) Decoded Project (protocol or protocol version eg \"opensea\" or \"aave_v2\") Spell set (eg cow_protocol contains \"batches\" and \"solvers\" Spells) Community Data Set Dataset Labels \u00b6 Label Description project A Spell set for a specific project eg aave sector A Spell set for a sector eg dex event A smart contract event dataset function A smart contract function dataset Data Type Labels \u00b6 You can find the full Databricks SQL data types documentation here . For types not found there, see Apache Spark SQL data types here . For V1 Data here are the official PostgreSQL data types . Label Description string Character sequences of any length greater or equal to 0. long Represents 8-byte signed integer numbers. The range of numbers is from -9223372036854775808 to 9223372036854775807. integer Represents 4-byte signed integer numbers. The range of numbers is from -2147483648 to 2147483647. double Represents 8-byte double-precision floating point numbers. boolean Represents boolean values. (TRUE date Represents values comprising values of fields year, month and day, without a time-zone. timestamp Represents values comprising values of fields year, month, day, hour, minute, and second, with the session local time-zone. The timestamp value represents an absolute point in time. decimal({{p}},{{s}}) Represents numbers with a specified maximum precision (p, 1 - 38) and fixed scale (s, the number of digits to the right of the decimal, 0 to p). array<{{xx}}> An array of {{xx}} data. ( long , string , etc)","title":"Data Types and Labels"},{"location":"reference/types-labels/#blockchain-icons","text":"Some data sets will have multiple blockchain icons - meaning data from each of those blockchains is available within that data set! Icon Description Ethereum blockchain Raw Data, Decoded Project, or Spell Gnosis Chain Raw Data, Decoded Project, or Spell Polygon blockchain Raw Data, Decoded Project, or Spell Optimism blockchain Raw Data, Decoded Project, or Spell Optimism (legacy) blockchain Raw Data, Decoded Project, or Spell BNB Chain Raw Data, Decoded Project, or Spell Solana blockchain Raw Data, Decoded Project, or Spell Arbitrum blockchain Raw Data, Decoded Project, or Spell Avalanche C-Chain Raw Data, Decoded Project, or Spell","title":"Blockchain Icons"},{"location":"reference/types-labels/#dataset-icons","text":"Icon Description Data Table (Raw Data, Spell, or smart contract Event or Function) Decoded Project (protocol or protocol version eg \"opensea\" or \"aave_v2\") Spell set (eg cow_protocol contains \"batches\" and \"solvers\" Spells) Community Data Set","title":"Dataset Icons"},{"location":"reference/types-labels/#dataset-labels","text":"Label Description project A Spell set for a specific project eg aave sector A Spell set for a sector eg dex event A smart contract event dataset function A smart contract function dataset","title":"Dataset Labels"},{"location":"reference/types-labels/#data-type-labels","text":"You can find the full Databricks SQL data types documentation here . For types not found there, see Apache Spark SQL data types here . For V1 Data here are the official PostgreSQL data types . Label Description string Character sequences of any length greater or equal to 0. long Represents 8-byte signed integer numbers. The range of numbers is from -9223372036854775808 to 9223372036854775807. integer Represents 4-byte signed integer numbers. The range of numbers is from -2147483648 to 2147483647. double Represents 8-byte double-precision floating point numbers. boolean Represents boolean values. (TRUE date Represents values comprising values of fields year, month and day, without a time-zone. timestamp Represents values comprising values of fields year, month, day, hour, minute, and second, with the session local time-zone. The timestamp value represents an absolute point in time. decimal({{p}},{{s}}) Represents numbers with a specified maximum precision (p, 1 - 38) and fixed scale (s, the number of digits to the right of the decimal, 0 to p). array<{{xx}}> An array of {{xx}} data. ( long , string , etc)","title":"Data Type Labels"},{"location":"reference/dune-v2/","text":"Dune Engine V2 is an update to our Query engine that brings a new level of performance, scalability and functionality to the core tools that enable Wizards to query, extract, and visualize blockchain data. It leverages Apache Spark SQL to increase performance for complex Queries, handle data scale, and enable cross-chain Queries all within the Query Editing UI you're used to. All of the data sources contained in this section are available for querying with the new Query engine today. We currently have the following data available in V2: Raw tables Decoded projects Spells Community Tables New Query engine \u00b6 Dune V2 changes our entire database architecture. We are transitioning away from a PostgresQL database to an Instance of Apache Spark hosted on Databricks. The difference between the two systems can be summarized as follows: Instead of PostgresQL, we will now use Databricks SQL. The change in SQL keywords is minimal but might be relevant for some of your querying habits. Spark is a column oriented database in contrast to PostgresQL\u2019s row oriented approach. traditional indexes are replaced by column chunk level min/max values You can read more about the changes here: Query Engine Or start getting your wand dirty by following along here: @springzhang 's Tips and Tricks for Dune V2 Queries and Visualizations Spellbook \u00b6 Abstractions have been upgraded to Spells stored in the Spellbook in Dune V2. They run on data build tool (dbt) . dbt enables analytics engineers to transform data in their warehouses by simply writing select statements. dbt handles turning these select statements into tables and views . This will makes the data abstractions built as Spells more robust, scalable and easier to work with. Spells Feedback \u00b6 One final note, as the Query engine is still in in beta you may run into bugs or have feedback on how it can be improved, feel free to share it with us on in our #general-feedback Discord channel or on our Canny board .","title":"Dune V2"},{"location":"reference/dune-v2/#new-query-engine","text":"Dune V2 changes our entire database architecture. We are transitioning away from a PostgresQL database to an Instance of Apache Spark hosted on Databricks. The difference between the two systems can be summarized as follows: Instead of PostgresQL, we will now use Databricks SQL. The change in SQL keywords is minimal but might be relevant for some of your querying habits. Spark is a column oriented database in contrast to PostgresQL\u2019s row oriented approach. traditional indexes are replaced by column chunk level min/max values You can read more about the changes here: Query Engine Or start getting your wand dirty by following along here: @springzhang 's Tips and Tricks for Dune V2 Queries and Visualizations","title":"New Query engine"},{"location":"reference/dune-v2/#spellbook","text":"Abstractions have been upgraded to Spells stored in the Spellbook in Dune V2. They run on data build tool (dbt) . dbt enables analytics engineers to transform data in their warehouses by simply writing select statements. dbt handles turning these select statements into tables and views . This will makes the data abstractions built as Spells more robust, scalable and easier to work with. Spells","title":"Spellbook"},{"location":"reference/dune-v2/#feedback","text":"One final note, as the Query engine is still in in beta you may run into bugs or have feedback on how it can be improved, feel free to share it with us on in our #general-feedback Discord channel or on our Canny board .","title":"Feedback"},{"location":"reference/dune-v2/query-engine/","text":"Welcome to DuneV2 \u00b6 DuneV2 changes our whole database architecture. We are transitioning away from a PostgreSQL database to an Instance of Apache Spark hosted on Databricks . The difference between the two systems can be summarized as follows: Instead of PostgreSQL, we will now use Databricks SQL. The change in SQL keywords is minimal but might be relevant for some of your querying habits. Spark is a column oriented database in contrast to PostgreSQL\u2019s row oriented approach. traditional indexes are replaced by column chunk level min/max values Note Find a detailed wakthrough of the changes V2 brings to building Queries below. Or start getting your wand dirty by following along with @springzhang 's Tips and Tricks for Dune V2 Queries and Visualizations Databricks SQL <> PostgresSQL operator changes \u00b6 The changes between the 2 coding languages syntax and the keyword operators are quite minimal, however there is some differences you should be mindful of: Description DuneV1 DuneV2 bytea2numeric does not exist in Spark. bytea2numeric(bytea) bytea2numeric_v2(string) 0 vs 1 based indexing 1 indexed 0 indexed bytea vs string for address, tx hash, etc\u2026 \\x2a7d.. (bytea) 0x2a7d... (string) Addresses (strings) are lower case in dune v2 \\x2A7D... (bytea) Works in Postgres 0x2a7d... (string) Has to be lowercase in Spark. Can be done via lower('0x2A7D...') . Selecting keyword columns is is different \"from\" `from` Alias naming is different as \"daily active users \" as `daily active user ` Exponentiation notation x/10^y x*power(10,y) or x*1e*y Interval arguments need a space in between the number and time units Interval '1day' Interval '1 day' Generate_series () is now sequence () generate_series('2022-05-15', CURRENT_DATE, '1 day') explode(sequence(to_date('2022-01-01'), to_date('2022-02-01'), interval 1 day)) Decimals are no longer in prices.usd Don\u2019t use prices.usd decimals Replace by blockchain.erc20_tokens.decimals Define NULL array NULL::integer[] CAST(NULL AS ARRAY<int>)) encoding strings to hex encode(string, 'hex') hex(string) Get json object differences (\"takerOutputUpdate\"-> 'deltaWei'->'value' ) decode(substring((\"addressSet\"->'baseAsset')::TEXT, 4,40), 'hex') get_json_object(get_json_object(takerOutputUpdate,' \\(.deltaWei'),'\\) .value') '0x' Using double quotes is not recommended in DuneV2, even when the engine can run the query and does not return an error. This is because the parser sometimes treats words in double quotes as a string and sometimes it treats them as an object (column name for example). For example, referencing a column name in the where clause using double quotes works as expected. However, the same query inside a CTE treats the column name as a string, as can be seen here . If you have found any other changes that are important to note, please feel free to submit a PR to our docs or leave us feedback in our #general-feedback Discord channel ! When googling for SQL questions, instead of googling PGSQL median , you should now google for Databricks SQL median . Databricks has a well documented index of built in functions on their website. Databricks - Databricks SQL Language Reference Changes in how the database works \u00b6 How does a database work? \u00b6 On a very high level, databases read data from storage into memory in order to return your Query results. A database is often times limited by the speed of the database being able to read data into it's memory. This is a classic computer science problem that's commonly referred to as being I/O bound . Row oriented database \u00b6 Databases store their data in pages. Pages traditionally contain rows of information. Multiple pages will make up one data file. A table in a database will sometimes consist of multiple data files. When retrieving data from the database, the database will read data into memory/cache in the size of pages. This is the smallest amount of data the database will read at once and is a common bottleneck while reading data from any database. After reading the data into memory, the database will either create temporary files or is able to read the data from memory again to finally arrive at the desired Query output. In any database system, we want to reduce the amount of pages we read when retrieving any amount of data from the database. Since traditional databases store rows in one page, they are best suited for retrieving all columns of one row in a Query. The database will always have to read the entire page in which a specific row is stored, therefore it is quite simple for the database to return the data also stored in this same page. The same is true for querying for rows which are stored closely together in the database. So querying for rows 500-600 is very efficient, querying for rows 5, 87, 789 and 1050 is not really all that efficient, but still alright. In contrast, querying for data which is stored in many different logical rows and therefore different pages is a hugely expensive operation. Most of the Queries we run on Dune today are aggregation of data points in a column over thousands if not millions of rows. In these cases, the database will read the entire pages in which this column data is stored, even though it only needs the data of one column. This means that on average, we are reading large amounts of data that is not needed to return the Query results, just because it is also contained on a page and the database is not able to read \"just\" the one column, but has to read the entire row in which the column is contained. In PostgreSQL, we can use indexes to not force the database to read through the entire table (and therefore a lot of pages), but rather only look at a structured subset. This will lead to very fast and efficient Queries, but is limited to the columns which are indexed. Since every new index that is created for a specific table will be a new file in the database and make it harder to update and maintain that table, this is not a sustainable approach to scale a database. We can't possibly create an index on every column or combination of columns in our database without running into trouble down the line. Therefore, Dune V2 will not run on row-oriented database, but rather on a column-oriented database. Column oriented database \u00b6 Instead of storing rows in pages, we store columns in pages. In this way, we reduce the amount of pages the database needs to read while aggregating or reading through a specific column. Specifically, in Dune V2 we are using the parquet file format for our new database. Parquet is sometimes described as a hybrid approach between row-oriented databases and column-oriented databases since a table in the database will still consist of multiple parquet files which are partitioned by rows of the dataset. Inside of the parquet file the pages which actually contain the data will contain columns instead of rows, but are still stored within row groups which further partition the data by rows. The database is still roughly stored in a row oriented format, but the individual values are stored on pages in column orientation. This means, that even though the database at large is somewhat oriented in a row oriented manner, should we actually want to read data, we will always read from a page which is column oriented. In this way, we can easily aggregate data in one column over a large amounts of logical rows, as in this layout the amount of pages we have to load into memory to actually read the data is minimized. In contrast, should we try to query for all columns of specific logical rows, we have to access lots of different pages as the data of one logical row is no longer stored in one page, but rather distributed across lots of different pages. This video does a pretty good job of explaining the differences in row vs column oriented database systems. In essence , storing columns in pages instead of rows minimizes the amount of not needed data that is read by the database when retrieving data for one column over a large amount of logical rows. We were sometimes able to mimic this in PostgreSQL by creating large amounts of structured subset data in the form of indexes, but this doesn't scale. Indexes or rather no Indexes \u00b6 Indexes don't exist in a traditional sense in a parquet based system. However, they are basically created on the fly with each parquet file having a footer that contains min/max values for every column stored in that parquet file. This pattern is then repeated on a column chunk level, which stores this metadata for the columns within a specific row group within the parquet file. Using these min/max values, both on a file level and on a column chunk level, allows the database to efficiently skip over entire parquet files or column chunks within parquet files while scanning through the table. Unfortunately, the min/max values of strings are often times not very useful. Especially tx_hash strings and address strings in blockchain systems are not suited well for this kind of min/max data gathering since they are randomly generated. That means the database won't be able to skip files or column chunks based on these strings and Queries will therefore be quite inefficient since it requires the database to actually load all the pages into memory. That said, since the Query engine at large is still able to read through individual columns in which these strings are stored very efficiently, most of the time this won't make a big difference in your Query execution speed. This is mostly relevant for base tables like ethereum.transactions , bnb.logs , erc20_ethereum.erc20_evt_transfer , etc. which contain very large datasets which are not pre-filtered. A notable exception from this is the Solana dataset account_activity , which instead of being ordered by block_time like the rest of our datasets, is ordered by account_keys . This allows us to actually reasonably utilize the min/max values for the account keys which were used and therefore run efficient Queries based on the account_keys values. Query examples \u00b6 Equipped with this knowledge, let's look at some Queries on the new Dune V2 engine. Querying for transaction hashes Select * from ethereum . transactions where hash = '0xce1f1a2dd0c10fcf9385d14bc92c686c210e4accf00a3fe7ec2b5db7a5499cff' If you think about this for second with all the knowledge we have learned earlier, you will hopefully understand that this Query is very inefficient. Our only filter condition here is a hash string, therefore we basically force the Query engine to read all pages which store the data of the tx_hash column in full. We probably can skip a few column chunks where the min/max value stored in the footer of each parquet file is 0xa0 - 0xcd , but those will be a rare exception. Given that we are basically doing a full scan over the entire history of Ethereum Mainnet (1.6B entries at time of writing) while searching for one hash , it's pretty impressive that this Query runs in about 6 minutes. Given that querying for hash is a very common occurrence in the workflow of an analyst on Dune, let's think about how we can make this faster. We simply have to use a column that actually has useful min/max values in order to be able to not read all pages in full, but rather be able to skip over a lot of files and column chunks. Both block_time and block_number are useful for this purpose. Select * from ethereum . transactions where block_number = 14854616 and hash = '0xce1f1a2dd0c10fcf9385d14bc92c686c210e4accf00a3fe7ec2b5db7a5499cff' This Query is still not as fast as in PostgreSQL, where we can make use of B-tree indexes, but with a runtime of 13 seconds, we are getting pretty close. What happens during the Query execution in this case is that the database engine is able to read the footer of the parquet files, is able to determine that the min/max values of a lot of parquet files is not meeting the defined criteria and skip over them efficiently. Once we have found a parquet file that actually meets our conditions, we can simply drill down into the column chunk min/max values, find the right column chunks, load the few pages of column data that are left into memory and find the match for the hash condition as well. Since we are selecting all entries from the logical row in this Query, we actually need to access a few other pages as well, but this is a reasonably efficient operation if we only do this for a few rows. Lesson: Define your conditions in a way in which the database is able to reasonably work with min/max values of files and columns chunks so it can efficiently find the logical row you need. Aggregating data over a large amount of logical rows This is mainly a case study to illustrate how efficient DuneV2 is in aggregating data over a large set of logical rows. Select avg ( gas_used ) from ethereum . transactions This Query runs in an amazing 7 seconds. This is mainly due to the fact that instead of having to read literally the entire table, we are now able to able to majorly reduce the amount of pages we have to read, since all this data is stored together in pages across parquet files. In PostgreSQL, each page that we would have to read would have contained a lot of not needed data, in Dune V2, we just read the data that we actually need. Lesson: Querying for data across a large amount of logical rows is now much more efficient and a lot of Queries that were formerly sheer impossible due to timing out are now able to be executed. A good example to illustrate this is hildobby's Ethereum Overview Dashboard. This is simply a level of data processing that was not possible before. Closing remarks \u00b6 Some Queries that were heavily indexed on our v1 database might feel a bit awkward in DuneV2. This is especially the case for erc20 event transfer tables, ethereum.transactions and ethereum.logs and their counterparts on other blockchains. This is a tradeoff we were willing to take to enable blockchain analytics on a large scale basis. We will continue to keep innovating on these datasets and our database architecture to make every Query run as fast as possible on DuneV2, but things like Queries for tx_hash being slow is just in the nature of this new database system. That said, we think we have done a pretty damn good job of enabling a lot of new usecases and speeding up a large amount of already existing Queries. If you have any feedback or run into trouble with the new system, we are all ears and await your feedback on Canny and Discord .","title":"Query Engine"},{"location":"reference/dune-v2/query-engine/#welcome-to-dunev2","text":"DuneV2 changes our whole database architecture. We are transitioning away from a PostgreSQL database to an Instance of Apache Spark hosted on Databricks . The difference between the two systems can be summarized as follows: Instead of PostgreSQL, we will now use Databricks SQL. The change in SQL keywords is minimal but might be relevant for some of your querying habits. Spark is a column oriented database in contrast to PostgreSQL\u2019s row oriented approach. traditional indexes are replaced by column chunk level min/max values Note Find a detailed wakthrough of the changes V2 brings to building Queries below. Or start getting your wand dirty by following along with @springzhang 's Tips and Tricks for Dune V2 Queries and Visualizations","title":"Welcome to DuneV2"},{"location":"reference/dune-v2/query-engine/#databricks-sql-postgressql-operator-changes","text":"The changes between the 2 coding languages syntax and the keyword operators are quite minimal, however there is some differences you should be mindful of: Description DuneV1 DuneV2 bytea2numeric does not exist in Spark. bytea2numeric(bytea) bytea2numeric_v2(string) 0 vs 1 based indexing 1 indexed 0 indexed bytea vs string for address, tx hash, etc\u2026 \\x2a7d.. (bytea) 0x2a7d... (string) Addresses (strings) are lower case in dune v2 \\x2A7D... (bytea) Works in Postgres 0x2a7d... (string) Has to be lowercase in Spark. Can be done via lower('0x2A7D...') . Selecting keyword columns is is different \"from\" `from` Alias naming is different as \"daily active users \" as `daily active user ` Exponentiation notation x/10^y x*power(10,y) or x*1e*y Interval arguments need a space in between the number and time units Interval '1day' Interval '1 day' Generate_series () is now sequence () generate_series('2022-05-15', CURRENT_DATE, '1 day') explode(sequence(to_date('2022-01-01'), to_date('2022-02-01'), interval 1 day)) Decimals are no longer in prices.usd Don\u2019t use prices.usd decimals Replace by blockchain.erc20_tokens.decimals Define NULL array NULL::integer[] CAST(NULL AS ARRAY<int>)) encoding strings to hex encode(string, 'hex') hex(string) Get json object differences (\"takerOutputUpdate\"-> 'deltaWei'->'value' ) decode(substring((\"addressSet\"->'baseAsset')::TEXT, 4,40), 'hex') get_json_object(get_json_object(takerOutputUpdate,' \\(.deltaWei'),'\\) .value') '0x' Using double quotes is not recommended in DuneV2, even when the engine can run the query and does not return an error. This is because the parser sometimes treats words in double quotes as a string and sometimes it treats them as an object (column name for example). For example, referencing a column name in the where clause using double quotes works as expected. However, the same query inside a CTE treats the column name as a string, as can be seen here . If you have found any other changes that are important to note, please feel free to submit a PR to our docs or leave us feedback in our #general-feedback Discord channel ! When googling for SQL questions, instead of googling PGSQL median , you should now google for Databricks SQL median . Databricks has a well documented index of built in functions on their website. Databricks - Databricks SQL Language Reference","title":"Databricks SQL &lt;&gt; PostgresSQL operator changes"},{"location":"reference/dune-v2/query-engine/#changes-in-how-the-database-works","text":"","title":"Changes in how the database works"},{"location":"reference/dune-v2/query-engine/#how-does-a-database-work","text":"On a very high level, databases read data from storage into memory in order to return your Query results. A database is often times limited by the speed of the database being able to read data into it's memory. This is a classic computer science problem that's commonly referred to as being I/O bound .","title":"How does a database work?"},{"location":"reference/dune-v2/query-engine/#row-oriented-database","text":"Databases store their data in pages. Pages traditionally contain rows of information. Multiple pages will make up one data file. A table in a database will sometimes consist of multiple data files. When retrieving data from the database, the database will read data into memory/cache in the size of pages. This is the smallest amount of data the database will read at once and is a common bottleneck while reading data from any database. After reading the data into memory, the database will either create temporary files or is able to read the data from memory again to finally arrive at the desired Query output. In any database system, we want to reduce the amount of pages we read when retrieving any amount of data from the database. Since traditional databases store rows in one page, they are best suited for retrieving all columns of one row in a Query. The database will always have to read the entire page in which a specific row is stored, therefore it is quite simple for the database to return the data also stored in this same page. The same is true for querying for rows which are stored closely together in the database. So querying for rows 500-600 is very efficient, querying for rows 5, 87, 789 and 1050 is not really all that efficient, but still alright. In contrast, querying for data which is stored in many different logical rows and therefore different pages is a hugely expensive operation. Most of the Queries we run on Dune today are aggregation of data points in a column over thousands if not millions of rows. In these cases, the database will read the entire pages in which this column data is stored, even though it only needs the data of one column. This means that on average, we are reading large amounts of data that is not needed to return the Query results, just because it is also contained on a page and the database is not able to read \"just\" the one column, but has to read the entire row in which the column is contained. In PostgreSQL, we can use indexes to not force the database to read through the entire table (and therefore a lot of pages), but rather only look at a structured subset. This will lead to very fast and efficient Queries, but is limited to the columns which are indexed. Since every new index that is created for a specific table will be a new file in the database and make it harder to update and maintain that table, this is not a sustainable approach to scale a database. We can't possibly create an index on every column or combination of columns in our database without running into trouble down the line. Therefore, Dune V2 will not run on row-oriented database, but rather on a column-oriented database.","title":"Row oriented database"},{"location":"reference/dune-v2/query-engine/#column-oriented-database","text":"Instead of storing rows in pages, we store columns in pages. In this way, we reduce the amount of pages the database needs to read while aggregating or reading through a specific column. Specifically, in Dune V2 we are using the parquet file format for our new database. Parquet is sometimes described as a hybrid approach between row-oriented databases and column-oriented databases since a table in the database will still consist of multiple parquet files which are partitioned by rows of the dataset. Inside of the parquet file the pages which actually contain the data will contain columns instead of rows, but are still stored within row groups which further partition the data by rows. The database is still roughly stored in a row oriented format, but the individual values are stored on pages in column orientation. This means, that even though the database at large is somewhat oriented in a row oriented manner, should we actually want to read data, we will always read from a page which is column oriented. In this way, we can easily aggregate data in one column over a large amounts of logical rows, as in this layout the amount of pages we have to load into memory to actually read the data is minimized. In contrast, should we try to query for all columns of specific logical rows, we have to access lots of different pages as the data of one logical row is no longer stored in one page, but rather distributed across lots of different pages. This video does a pretty good job of explaining the differences in row vs column oriented database systems. In essence , storing columns in pages instead of rows minimizes the amount of not needed data that is read by the database when retrieving data for one column over a large amount of logical rows. We were sometimes able to mimic this in PostgreSQL by creating large amounts of structured subset data in the form of indexes, but this doesn't scale.","title":"Column oriented database"},{"location":"reference/dune-v2/query-engine/#indexes-or-rather-no-indexes","text":"Indexes don't exist in a traditional sense in a parquet based system. However, they are basically created on the fly with each parquet file having a footer that contains min/max values for every column stored in that parquet file. This pattern is then repeated on a column chunk level, which stores this metadata for the columns within a specific row group within the parquet file. Using these min/max values, both on a file level and on a column chunk level, allows the database to efficiently skip over entire parquet files or column chunks within parquet files while scanning through the table. Unfortunately, the min/max values of strings are often times not very useful. Especially tx_hash strings and address strings in blockchain systems are not suited well for this kind of min/max data gathering since they are randomly generated. That means the database won't be able to skip files or column chunks based on these strings and Queries will therefore be quite inefficient since it requires the database to actually load all the pages into memory. That said, since the Query engine at large is still able to read through individual columns in which these strings are stored very efficiently, most of the time this won't make a big difference in your Query execution speed. This is mostly relevant for base tables like ethereum.transactions , bnb.logs , erc20_ethereum.erc20_evt_transfer , etc. which contain very large datasets which are not pre-filtered. A notable exception from this is the Solana dataset account_activity , which instead of being ordered by block_time like the rest of our datasets, is ordered by account_keys . This allows us to actually reasonably utilize the min/max values for the account keys which were used and therefore run efficient Queries based on the account_keys values.","title":"Indexes or rather no Indexes"},{"location":"reference/dune-v2/query-engine/#query-examples","text":"Equipped with this knowledge, let's look at some Queries on the new Dune V2 engine. Querying for transaction hashes Select * from ethereum . transactions where hash = '0xce1f1a2dd0c10fcf9385d14bc92c686c210e4accf00a3fe7ec2b5db7a5499cff' If you think about this for second with all the knowledge we have learned earlier, you will hopefully understand that this Query is very inefficient. Our only filter condition here is a hash string, therefore we basically force the Query engine to read all pages which store the data of the tx_hash column in full. We probably can skip a few column chunks where the min/max value stored in the footer of each parquet file is 0xa0 - 0xcd , but those will be a rare exception. Given that we are basically doing a full scan over the entire history of Ethereum Mainnet (1.6B entries at time of writing) while searching for one hash , it's pretty impressive that this Query runs in about 6 minutes. Given that querying for hash is a very common occurrence in the workflow of an analyst on Dune, let's think about how we can make this faster. We simply have to use a column that actually has useful min/max values in order to be able to not read all pages in full, but rather be able to skip over a lot of files and column chunks. Both block_time and block_number are useful for this purpose. Select * from ethereum . transactions where block_number = 14854616 and hash = '0xce1f1a2dd0c10fcf9385d14bc92c686c210e4accf00a3fe7ec2b5db7a5499cff' This Query is still not as fast as in PostgreSQL, where we can make use of B-tree indexes, but with a runtime of 13 seconds, we are getting pretty close. What happens during the Query execution in this case is that the database engine is able to read the footer of the parquet files, is able to determine that the min/max values of a lot of parquet files is not meeting the defined criteria and skip over them efficiently. Once we have found a parquet file that actually meets our conditions, we can simply drill down into the column chunk min/max values, find the right column chunks, load the few pages of column data that are left into memory and find the match for the hash condition as well. Since we are selecting all entries from the logical row in this Query, we actually need to access a few other pages as well, but this is a reasonably efficient operation if we only do this for a few rows. Lesson: Define your conditions in a way in which the database is able to reasonably work with min/max values of files and columns chunks so it can efficiently find the logical row you need. Aggregating data over a large amount of logical rows This is mainly a case study to illustrate how efficient DuneV2 is in aggregating data over a large set of logical rows. Select avg ( gas_used ) from ethereum . transactions This Query runs in an amazing 7 seconds. This is mainly due to the fact that instead of having to read literally the entire table, we are now able to able to majorly reduce the amount of pages we have to read, since all this data is stored together in pages across parquet files. In PostgreSQL, each page that we would have to read would have contained a lot of not needed data, in Dune V2, we just read the data that we actually need. Lesson: Querying for data across a large amount of logical rows is now much more efficient and a lot of Queries that were formerly sheer impossible due to timing out are now able to be executed. A good example to illustrate this is hildobby's Ethereum Overview Dashboard. This is simply a level of data processing that was not possible before.","title":"Query examples"},{"location":"reference/dune-v2/query-engine/#closing-remarks","text":"Some Queries that were heavily indexed on our v1 database might feel a bit awkward in DuneV2. This is especially the case for erc20 event transfer tables, ethereum.transactions and ethereum.logs and their counterparts on other blockchains. This is a tradeoff we were willing to take to enable blockchain analytics on a large scale basis. We will continue to keep innovating on these datasets and our database architecture to make every Query run as fast as possible on DuneV2, but things like Queries for tx_hash being slow is just in the nature of this new database system. That said, we think we have done a pretty damn good job of enabling a lot of new usecases and speeding up a large amount of already existing Queries. If you have any feedback or run into trouble with the new system, we are all ears and await your feedback on Canny and Discord .","title":"Closing remarks"},{"location":"reference/faq/does-dune-have-a-token/","text":"Does Dune have a Token? \u00b6 Dune does not have token.","title":"Does Dune have a token?"},{"location":"reference/faq/does-dune-have-a-token/#does-dune-have-a-token","text":"Dune does not have token.","title":"Does Dune have a Token?"},{"location":"reference/faq/does-dune-have-an-api/","text":"Does Dune have an API? \u00b6 Dune does have an API! It's currently in private beta with a closed set of users, but will be publicly available in November! If you don't have API access, paid plan users can export results as CSV and anyone can embed Dune charts on their page for free by clicking \" Embed \" on any query Visualization.","title":"Does Dune have an API?"},{"location":"reference/faq/does-dune-have-an-api/#does-dune-have-an-api","text":"Dune does have an API! It's currently in private beta with a closed set of users, but will be publicly available in November! If you don't have API access, paid plan users can export results as CSV and anyone can embed Dune charts on their page for free by clicking \" Embed \" on any query Visualization.","title":"Does Dune have an API?"},{"location":"reference/faq/how-are-results-refreshing/","text":"How do results get refreshed? \u00b6 When a Visualization is viewed, either on a query page or on a dashboard, the Dune backend will inspect the age of the most recent result. If the result is stale (currently defined as >3 hours old), Dune will automatically queue an execution for this query and run it in the background. This means that dashboards will always be kept up to date when they are being viewed and the query creator does not need to set a refresh schedule. Note that the query execution queue is separate from each individual user's queue when they create and run queries in the query editor.","title":"How do results get refreshed?"},{"location":"reference/faq/how-are-results-refreshing/#how-do-results-get-refreshed","text":"When a Visualization is viewed, either on a query page or on a dashboard, the Dune backend will inspect the age of the most recent result. If the result is stale (currently defined as >3 hours old), Dune will automatically queue an execution for this query and run it in the background. This means that dashboards will always be kept up to date when they are being viewed and the query creator does not need to set a refresh schedule. Note that the query execution queue is separate from each individual user's queue when they create and run queries in the query editor.","title":"How do results get refreshed?"},{"location":"reference/faq/how-does-dune-get-its-data/","text":"How does Dune get its data ? \u00b6 We are working together with node providers across the Industry to ingest data into our database. This includes all raw historical data from these blockchains. It does not include state data. We don't discriminate against any smart contract, you can submit any contract via our decoding page and work with it in a matter of hours. The only prerequisite for this is an ABI so we can actually decode the contract (usually the ABI is already on Etherscan, but there are edge cases).","title":"How does Dune get data?"},{"location":"reference/faq/how-does-dune-get-its-data/#how-does-dune-get-its-data","text":"We are working together with node providers across the Industry to ingest data into our database. This includes all raw historical data from these blockchains. It does not include state data. We don't discriminate against any smart contract, you can submit any contract via our decoding page and work with it in a matter of hours. The only prerequisite for this is an ABI so we can actually decode the contract (usually the ABI is already on Etherscan, but there are edge cases).","title":"How does Dune get its data ?"},{"location":"reference/tables/","text":"Dune enables you to query data on our supported chains at different levels of abstraction. To start with, you have access to the raw tables for each supported blockchain, with tables like blocks and transactions . This is the most flexible option. To make it easier to work with smart contracts, Dune also provides decoded data as individual, human readable tables. We use the ABI for smart contracts and the interface standard for standardized token smart contracts (ERC20, ERC721 etc.). We've indexed over 280k contracts as of writing, and you can submit new contracts . On top of that, we're building a set of Spells for common use cases (e.g. NFTs or DEX) and third party datasets. Here is an overview what is available: Raw data : unedited, raw and encoded blockchain data Decoded data : ( most popular ) view the decoded calls and events made to smart contracts Spells : custom tables that are maintained by Dune and our community Community : partnering with select organizations for off-chain data Prices : prices from third party data providers User generated : construct your own view, function or table inside of our database Available Chains \u00b6 Learn about the chains we have available for Querying here: Available Chains How Tables are Generated from Raw Ethereum Data \u00b6 Note Table generation is similar to this for all EVM based blockchains. The labels below follow Dune V1 naming conventions and are slightly different in Dune V2 Tables. Learn about the differences in V2's data structure here . And learn how to search V2 data here . There are three main tables in Dune generated from raw Ethereum data, which are the source truth for everything else on the platform. The rest of Dune's tables are built from these three: If you can't figure out the table to query from, you'll need to dig through an example transaction hash on the relevant chain's blockchain explorer to figure out the call/evt Contract. Essentially go tx_hash \u2192 contract in \"to\" \u2192 code and pull the contract name from there. If it's a proxy, the blockchain explorer should link you to an implementation address. Dune V2 data also has a handy Lineage Graph that lets you explore how all of Dune's data tables are related to each other - learn more about that in the Lineage Graph section here . You should always try to use the Decoded or Spell tables when you can as these are human-readable and pre-organized so they're much easier to use.","title":"Tables"},{"location":"reference/tables/#available-chains","text":"Learn about the chains we have available for Querying here: Available Chains","title":"Available Chains"},{"location":"reference/tables/#how-tables-are-generated-from-raw-ethereum-data","text":"Note Table generation is similar to this for all EVM based blockchains. The labels below follow Dune V1 naming conventions and are slightly different in Dune V2 Tables. Learn about the differences in V2's data structure here . And learn how to search V2 data here . There are three main tables in Dune generated from raw Ethereum data, which are the source truth for everything else on the platform. The rest of Dune's tables are built from these three: If you can't figure out the table to query from, you'll need to dig through an example transaction hash on the relevant chain's blockchain explorer to figure out the call/evt Contract. Essentially go tx_hash \u2192 contract in \"to\" \u2192 code and pull the contract name from there. If it's a proxy, the blockchain explorer should link you to an implementation address. Dune V2 data also has a handy Lineage Graph that lets you explore how all of Dune's data tables are related to each other - learn more about that in the Lineage Graph section here . You should always try to use the Decoded or Spell tables when you can as these are human-readable and pre-organized so they're much easier to use.","title":"How Tables are Generated from Raw Ethereum Data"},{"location":"reference/tables/available-chains/","text":"Here are the chains we have available to Query in Dune. Ethereum Mainnet \u00b6 Ethereum was first launched in 2015 and is the original Blockchain that innovated and implemented the Ethereum Virtual Machine. Ethereum to this day remains a \"truly\" decentralized platform with many node operators all over the world securing the Blockchain. Ethereum is maintained and developed by independent developers all over the world. Since it's inception in 2015, Ethereum has undergone a range of updates. Explaining the Ethereum history, Roadmap and technical details goes beyond the scope of this documentation, therefore we encourage you to read the Documentation on Ethereum.org . Ethereum Developer Docs Gnosis Chain (xDai) \u00b6 Gnosis Chain is the predecessor of xDAI. It's a unique system in which the native fee currency is a bridged version of the stablecoin $DAI. The chain uses a unique dual-token model; $xDai is a stable token used for transactions, payments, and fees; Proof of Stake protection will be provided by $GNO with the consensus-layer Gnosis Beacon Chain. Gnosis Chain is yet to complete it's transition to an open proof of stake system, in the meanwhile the chain is being maintained by the POSDAO. You can read more about this transitional state here . Gnosis Chain will continue xDai\u2019s intent to follow the Ethereum roadmap as closely as possible. Future goals include: Offer the highest degree of compatibility between Gnosis Chain and Ethereum Set up a Gnosis Beacon Chain (in preparation of a later merge) Develop over time a role similar to what Kusama is to Polkadot Gnosis Chain follows all standards and upgrades of Ethereum Mainnet, querying on Dune is exactly the same. Polygon POS \u00b6 Polygon(formerly MATIC) is an Ethereum sidechain hosted and maintained by by Polygon Technology. Polygon PoS is a solution that achieves transaction speed and cost savings by utilizing a POS network. Polygon node requirements are significantly higher than Mainnet requirements as Polygon has a higher gas limit and shorter block time. You can read more about Polygon and their approach to scaling an EVM in their documentation . Polygon follows all the rules of ETH mainnet and querying on Dune works exactly the same. Optimism \u00b6 Note We've included Optimism's OVM 1.0 base tables (blocks, logs, traces, transactions) in Dune V2, which can be found in the optimism_legacy_ovm1 database. Data from these tables are labeled \"Optimism (Legacy)\" in the dropdown menu and use this icon: These tables are no longer updated as Optimism made significant changes with their OVM 2.0 update . Data for the current version of Optimism's blockchain (November 11th, 2021 to present), is contained in the optimism database, are labeled \"Optimism\" in the dropdown menu, and use this icon: Optimism is a Layer 2 Optimistic Rollup network designed to utilize the strong security guarantees of Ethereum while reducing its cost and latency. Optimism processes transactions outside of Ethereum Mainnet, reducing congestion on the base layer and improving scalability. For a Deep Dive into Optimism, we recommend reading through their Documentation . Optimism differs in it's EVM implementation in the calculation of gas costs, since it also needs to pay for L1 resources. Gas costs on Optimism \u00b6 Optimism settles it's transactions on Ethereum Mainnet. This happens via a sequencer contract on L1 that submits the raw calldata of any given transaction on Optimism's Network on L1 in batches. These costs for the batch settlement transactions needs to be accounted for in the Calculation of gas costs on Optimism. The transaction fees on Optimism are calculated with the following formula: \\[ Transaction\\ Fees = L1\\ fee + (L2\\ gas\\ price * L2\\ gas\\ used) \\] The L1 Fee consists of: \\[ L1\\ Fee = Fee\\ Scalar * L1\\ Gas\\ Price * L1\\ Gas\\ Used \\] L1 Fee Scalar is a variable that can be increased/decreased by the Optimism Team. It ensures that the gas costs for L1 are sufficiently covered and provides income to the Optimism Team. L1 Gas Price is an estimation of the gas price on Mainnet. L1 Gas used breaks down to: \\[ L1\\ Gas\\ Used = Calldata\\ gas + a\\ fixed\\ overhead\\ gas\\ cost \\] So the full calculation of gas costs on optimism consists of: \\[ Transaction\\ Fee = Fee\\ Scalar * L1\\ Gas\\ Price * (L1 Calldata\\ gas + a\\ fixed\\ overhead\\ gas\\ cost) \\] You can read more about Optimism Gas Costs and the approach to minimize them in this article . TL;DR \u00b6 To calculate gas costs for a transaction on Optimism you need to follow this formula: L1\\_Fee + (gas_price*gas_used) Additionally, Optimism hasn't implemented EIP1559, so it follows the \"old\" gas auction model. BNB Chain (BSC) \u00b6 BNB Chain(formerly Binance Smart Chain, BSC) is an instance of the Ethereum Virtual Machine built and maintained by a team from the popular Crypto Exchange Binance . BNB Chain follows most of the rules of Ethereum Mainnet, but has not implemented EIP1559. Instead it relies on BEP-95 to burn fees that accrue during usage of the platform. Furthermore, the gas limit per block is set to 100 mio, enabling more transactions to be processed in a given block. Transactions fees are paid in $BNB instead of $ETH. You can read more about BNB Chain in the documentation . On Dune, that means that the gas fields for EIP1559 transactions stay empty, everything else is the same. BNB Chain Documentation Solana \u00b6 Solana is a non-EVM blockchain that aims to have high transaction speeds without sacrificing decentralization. The chain employs a bunch of novel approaches, including the \u201cproof of history\u201d mechanism, which is why you'll find their data is quite different from EVM chains. Learn more about Solana's approach and details here . Arbitrum \u00b6 Arbitrum is an optimistic rollup that settles it's transactions on Ethereum Mainnet. You can read all about Arbitrum's approach to scaling and building a rollup in their docs . Arbitrum's execution environment differs from the Mainnet EVM implementation in it's calculation of gas costs. Since Arbitrum is an optimistic rollup that publishes it's transaction on Ethereum Mainnet, the gas calculations have to account for additional factors. Gas costs for L1 \u00b6 Transactions on Arbitrum have to pay for both L1 and L2 resources. The L1 resources are essentially just Ethereum calldata; i.e., you pay the size in raw data of your transaction times Arbitrum's view of the L1 calldata price. This already factors in L1's fluctuating gas prices. The L2 resources in a given transaction are the natively emerging costs for computation and storage that you invoke with your transaction, similar to any other EVM. Since the \"normal\" implementation of the Ethereum Virtual Machine does not possess multiple ways to pay gas fees, Arbitrum solved this by including the cost for L1 resources inside of the computational gas units used ( gas_limit or gas_used ). Whenever somebody tries to transact on Arbitrum, the RPC endpoint that is called for estimating a sufficient \"gas limit\" returns an amount of computational gas units that exceeds the standard costs that would incur in a \"normal\" EVM. P = \"Gas Price\" on Arbitrum = Arbitrum's native gas price G = traditionally \"Gas Limit\" = L2 gas used + L1 resources used Arbitrum's transactions costs are calculated with the following formula: \\[ P*G = Costs \\] G consists of: \\[ (L2 \\ gas \\ used) +\\frac{(L1 \\ gas \\ price * L1\\ calldata)}{P} = G \\] The complete calculation therefore consists of: \\[ P * ((L2 \\ gas \\ used) +\\frac{(L1 \\ gas \\ price * L1\\ calldata)}{P}) = Costs \\] Looking at this formula, we can now understand that an increasing gas price on Arbitrum leads to cheaper transactions. This is due to the fact that Arbitrum will always publish it's transactions on Ethereum mainnet, whether there is one transaction that needs to be published on mainnet or a thousand. Fees on Arbitrum will only rise if there is an increased demand for blockspace. The increased demand for blockspace in return means that more Arbitrum transactions get published on mainnet in one \"batch publish\" transaction, significantly reducing the costs for individual transactions. We are reflecting the L1 resources used for Arbitrum Transactions in the column gas_used_for_l1 . Gas Price Mechanism \u00b6 Arbitrum does not follow the EIP1559 standard and instead has its own mechanism for determining gas costs. A transaction will have a gas_price , which is treated as the maximum price that the user is willing to pay for this transaction. The actual gas_price that is paid by the user will be determined by an algorithm within the Arbitrum EVM. This actual gas_price that the user pays is reflected in the column effective_gas_price . Calculation of Gas Usage \u00b6 Arbitrum does not follow the standard EVM standards and rules to calculate how much gas is being used by a transaction. That is including and excluding the L1 Resource costs, the costs within Arbitrum's EVM are just calculated differently all together. You can look at the specifications of Arbitrum's AVM opcode costs in their documentation . You can't compare Arbitrum gas costs to other EVM chains gas costs! TL;DR \u00b6 Use effective_gas_price for calculating gas costs on Arbitrum. To look at only the gas costs that occur within the Arbitrum EVM, subtract the gas_used_for_l1 amount from the effective_gas_price field. That being said, you can't compare Arbitrum's gas consumption to other EVM chains gas costs. Avalanche C-Chain \u00b6 C-Chain is an instance of the Ethereum Virtual Machine powered by the Avalanche network. It follows the rules of Ethereum Mainnet and only differs in it's consensus mechanism, all other technical specification are exactly the same. Gas is paid in $AVAX instead of $ETH. You can read more about Avalanche Network and C-Chain in this article . Working with the C-Chain on Dune works exactly like querying Ethereum mainnet data. Only avalanche_c.blocks has slightly different properties as Avalanche C-Chain is already in a proof of stake(POS) consensus algorithm.","title":"Available Chains"},{"location":"reference/tables/available-chains/#ethereum-mainnet","text":"Ethereum was first launched in 2015 and is the original Blockchain that innovated and implemented the Ethereum Virtual Machine. Ethereum to this day remains a \"truly\" decentralized platform with many node operators all over the world securing the Blockchain. Ethereum is maintained and developed by independent developers all over the world. Since it's inception in 2015, Ethereum has undergone a range of updates. Explaining the Ethereum history, Roadmap and technical details goes beyond the scope of this documentation, therefore we encourage you to read the Documentation on Ethereum.org . Ethereum Developer Docs","title":"Ethereum Mainnet"},{"location":"reference/tables/available-chains/#gnosis-chain-xdai","text":"Gnosis Chain is the predecessor of xDAI. It's a unique system in which the native fee currency is a bridged version of the stablecoin $DAI. The chain uses a unique dual-token model; $xDai is a stable token used for transactions, payments, and fees; Proof of Stake protection will be provided by $GNO with the consensus-layer Gnosis Beacon Chain. Gnosis Chain is yet to complete it's transition to an open proof of stake system, in the meanwhile the chain is being maintained by the POSDAO. You can read more about this transitional state here . Gnosis Chain will continue xDai\u2019s intent to follow the Ethereum roadmap as closely as possible. Future goals include: Offer the highest degree of compatibility between Gnosis Chain and Ethereum Set up a Gnosis Beacon Chain (in preparation of a later merge) Develop over time a role similar to what Kusama is to Polkadot Gnosis Chain follows all standards and upgrades of Ethereum Mainnet, querying on Dune is exactly the same.","title":"Gnosis Chain (xDai)"},{"location":"reference/tables/available-chains/#polygon-pos","text":"Polygon(formerly MATIC) is an Ethereum sidechain hosted and maintained by by Polygon Technology. Polygon PoS is a solution that achieves transaction speed and cost savings by utilizing a POS network. Polygon node requirements are significantly higher than Mainnet requirements as Polygon has a higher gas limit and shorter block time. You can read more about Polygon and their approach to scaling an EVM in their documentation . Polygon follows all the rules of ETH mainnet and querying on Dune works exactly the same.","title":"Polygon POS"},{"location":"reference/tables/available-chains/#optimism","text":"Note We've included Optimism's OVM 1.0 base tables (blocks, logs, traces, transactions) in Dune V2, which can be found in the optimism_legacy_ovm1 database. Data from these tables are labeled \"Optimism (Legacy)\" in the dropdown menu and use this icon: These tables are no longer updated as Optimism made significant changes with their OVM 2.0 update . Data for the current version of Optimism's blockchain (November 11th, 2021 to present), is contained in the optimism database, are labeled \"Optimism\" in the dropdown menu, and use this icon: Optimism is a Layer 2 Optimistic Rollup network designed to utilize the strong security guarantees of Ethereum while reducing its cost and latency. Optimism processes transactions outside of Ethereum Mainnet, reducing congestion on the base layer and improving scalability. For a Deep Dive into Optimism, we recommend reading through their Documentation . Optimism differs in it's EVM implementation in the calculation of gas costs, since it also needs to pay for L1 resources.","title":"Optimism"},{"location":"reference/tables/available-chains/#gas-costs-on-optimism","text":"Optimism settles it's transactions on Ethereum Mainnet. This happens via a sequencer contract on L1 that submits the raw calldata of any given transaction on Optimism's Network on L1 in batches. These costs for the batch settlement transactions needs to be accounted for in the Calculation of gas costs on Optimism. The transaction fees on Optimism are calculated with the following formula: \\[ Transaction\\ Fees = L1\\ fee + (L2\\ gas\\ price * L2\\ gas\\ used) \\] The L1 Fee consists of: \\[ L1\\ Fee = Fee\\ Scalar * L1\\ Gas\\ Price * L1\\ Gas\\ Used \\] L1 Fee Scalar is a variable that can be increased/decreased by the Optimism Team. It ensures that the gas costs for L1 are sufficiently covered and provides income to the Optimism Team. L1 Gas Price is an estimation of the gas price on Mainnet. L1 Gas used breaks down to: \\[ L1\\ Gas\\ Used = Calldata\\ gas + a\\ fixed\\ overhead\\ gas\\ cost \\] So the full calculation of gas costs on optimism consists of: \\[ Transaction\\ Fee = Fee\\ Scalar * L1\\ Gas\\ Price * (L1 Calldata\\ gas + a\\ fixed\\ overhead\\ gas\\ cost) \\] You can read more about Optimism Gas Costs and the approach to minimize them in this article .","title":"Gas costs on Optimism"},{"location":"reference/tables/available-chains/#tldr","text":"To calculate gas costs for a transaction on Optimism you need to follow this formula: L1\\_Fee + (gas_price*gas_used) Additionally, Optimism hasn't implemented EIP1559, so it follows the \"old\" gas auction model.","title":"TL;DR"},{"location":"reference/tables/available-chains/#bnb-chain-bsc","text":"BNB Chain(formerly Binance Smart Chain, BSC) is an instance of the Ethereum Virtual Machine built and maintained by a team from the popular Crypto Exchange Binance . BNB Chain follows most of the rules of Ethereum Mainnet, but has not implemented EIP1559. Instead it relies on BEP-95 to burn fees that accrue during usage of the platform. Furthermore, the gas limit per block is set to 100 mio, enabling more transactions to be processed in a given block. Transactions fees are paid in $BNB instead of $ETH. You can read more about BNB Chain in the documentation . On Dune, that means that the gas fields for EIP1559 transactions stay empty, everything else is the same. BNB Chain Documentation","title":"BNB Chain (BSC)"},{"location":"reference/tables/available-chains/#solana","text":"Solana is a non-EVM blockchain that aims to have high transaction speeds without sacrificing decentralization. The chain employs a bunch of novel approaches, including the \u201cproof of history\u201d mechanism, which is why you'll find their data is quite different from EVM chains. Learn more about Solana's approach and details here .","title":"Solana"},{"location":"reference/tables/available-chains/#arbitrum","text":"Arbitrum is an optimistic rollup that settles it's transactions on Ethereum Mainnet. You can read all about Arbitrum's approach to scaling and building a rollup in their docs . Arbitrum's execution environment differs from the Mainnet EVM implementation in it's calculation of gas costs. Since Arbitrum is an optimistic rollup that publishes it's transaction on Ethereum Mainnet, the gas calculations have to account for additional factors.","title":"Arbitrum"},{"location":"reference/tables/available-chains/#gas-costs-for-l1","text":"Transactions on Arbitrum have to pay for both L1 and L2 resources. The L1 resources are essentially just Ethereum calldata; i.e., you pay the size in raw data of your transaction times Arbitrum's view of the L1 calldata price. This already factors in L1's fluctuating gas prices. The L2 resources in a given transaction are the natively emerging costs for computation and storage that you invoke with your transaction, similar to any other EVM. Since the \"normal\" implementation of the Ethereum Virtual Machine does not possess multiple ways to pay gas fees, Arbitrum solved this by including the cost for L1 resources inside of the computational gas units used ( gas_limit or gas_used ). Whenever somebody tries to transact on Arbitrum, the RPC endpoint that is called for estimating a sufficient \"gas limit\" returns an amount of computational gas units that exceeds the standard costs that would incur in a \"normal\" EVM. P = \"Gas Price\" on Arbitrum = Arbitrum's native gas price G = traditionally \"Gas Limit\" = L2 gas used + L1 resources used Arbitrum's transactions costs are calculated with the following formula: \\[ P*G = Costs \\] G consists of: \\[ (L2 \\ gas \\ used) +\\frac{(L1 \\ gas \\ price * L1\\ calldata)}{P} = G \\] The complete calculation therefore consists of: \\[ P * ((L2 \\ gas \\ used) +\\frac{(L1 \\ gas \\ price * L1\\ calldata)}{P}) = Costs \\] Looking at this formula, we can now understand that an increasing gas price on Arbitrum leads to cheaper transactions. This is due to the fact that Arbitrum will always publish it's transactions on Ethereum mainnet, whether there is one transaction that needs to be published on mainnet or a thousand. Fees on Arbitrum will only rise if there is an increased demand for blockspace. The increased demand for blockspace in return means that more Arbitrum transactions get published on mainnet in one \"batch publish\" transaction, significantly reducing the costs for individual transactions. We are reflecting the L1 resources used for Arbitrum Transactions in the column gas_used_for_l1 .","title":"Gas costs for L1"},{"location":"reference/tables/available-chains/#gas-price-mechanism","text":"Arbitrum does not follow the EIP1559 standard and instead has its own mechanism for determining gas costs. A transaction will have a gas_price , which is treated as the maximum price that the user is willing to pay for this transaction. The actual gas_price that is paid by the user will be determined by an algorithm within the Arbitrum EVM. This actual gas_price that the user pays is reflected in the column effective_gas_price .","title":"Gas Price Mechanism"},{"location":"reference/tables/available-chains/#calculation-of-gas-usage","text":"Arbitrum does not follow the standard EVM standards and rules to calculate how much gas is being used by a transaction. That is including and excluding the L1 Resource costs, the costs within Arbitrum's EVM are just calculated differently all together. You can look at the specifications of Arbitrum's AVM opcode costs in their documentation . You can't compare Arbitrum gas costs to other EVM chains gas costs!","title":"Calculation of Gas Usage"},{"location":"reference/tables/available-chains/#tldr_1","text":"Use effective_gas_price for calculating gas costs on Arbitrum. To look at only the gas costs that occur within the Arbitrum EVM, subtract the gas_used_for_l1 amount from the effective_gas_price field. That being said, you can't compare Arbitrum's gas consumption to other EVM chains gas costs.","title":"TL;DR"},{"location":"reference/tables/available-chains/#avalanche-c-chain","text":"C-Chain is an instance of the Ethereum Virtual Machine powered by the Avalanche network. It follows the rules of Ethereum Mainnet and only differs in it's consensus mechanism, all other technical specification are exactly the same. Gas is paid in $AVAX instead of $ETH. You can read more about Avalanche Network and C-Chain in this article . Working with the C-Chain on Dune works exactly like querying Ethereum mainnet data. Only avalanche_c.blocks has slightly different properties as Avalanche C-Chain is already in a proof of stake(POS) consensus algorithm.","title":"Avalanche C-Chain"},{"location":"reference/tables/user-generated/","text":"Note User generated tables are not yet available on V2. Note that these tables are not guaranteed to contain correct data, please use these with caution if you haven't created them yourself. Always save the constructor arguments for your views. Sometimes we have to drop views in order to be able to change some decoding tables or proxy dependencies and you might have to redeploy your view. Use Cases \u00b6 There is several ways in which you can utilize your own views and tables inside of Dune to make working with your data on Dune even easier. Your own tables, views and function all have an important part to play in creating content on Dune and make maintenance of your dashboards and queries easier if used correctly. If you are unfamiliar with tables, views, materialized views and functions please consult the pgSQL documentation or check out our getting started guide . Storing Information \u00b6 Sometimes certain references or information that you do need for your data extraction are not properly stored in available tables or stored in many different tables, which would makes the query quite hard to work with. In these cases, you are best off storing the necessary information in a view and referencing that view. An example of this would be the mapping of certain vault or lending tokens to their respective underlying tokens. CREATE OR REPLACE VIEW dune_user_generated . view_test ( symbol , contract_address , decimals , underlying_token_address ) AS VALUES ( 'iETH' :: text , '\\x9Dde7cdd09dbed542fC422d18d89A589fA9fD4C0' :: bytea , 18 :: numeric , '\\xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2' :: bytea ), ( 'yDAI' :: text , '\\x9D25057e62939D3408406975aD75Ffe834DA4cDd' :: bytea , 18 :: numeric , '\\x6B175474E89094C44Da98b954EedeAC495271d0F' :: bytea ), ( 'yUSDC' :: text , '\\xa2609b2b43ac0f5ebe27deb944d2a399c201e3da' :: bytea , 6 :: numeric , '\\xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48' :: bytea ), ( 'ySUSD' :: text , '\\x36324b8168f960A12a8fD01406C9C78143d41380' :: bytea , 18 :: numeric , '\\x57Ab1ec28D129707052df4dF418D58a2D46d5f51' :: bytea ), ( 'yUSDT' :: text , '\\xa1787206d5b1bE0f432C4c4f96Dc4D1257A1Dd14' :: bytea , 6 :: numeric , '\\xdAC17F958D2ee523a2206206994597C13D831ec7' :: bytea ), ( 'yCRV' :: text , '\\x9Ce551A9D2B1A4Ec0cc6eB0E0CC12977F6ED306C' :: bytea , 18 :: numeric , '\\x6B175474E89094C44Da98b954EedeAC495271d0F' :: bytea ), ( 'yBTC' :: text , '\\x04EF8121aD039ff41d10029c91EA1694432514e9' :: bytea , 8 :: numeric , '\\x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599' :: bytea ) This table generates a view that you can use to join on your query. Look at the table . Aggregating Data \u00b6 Views can also be used to aggregate the actions of multiple smart contracts into one view that contains all the necessary data. This is especially useful if you are working with the same dataset over and over and only change the way you display or aggregate the data. That way, instead of having to query for your dataset again and again, you just put it into a view once and then can start referencing that view. This will allow you to change the base query that constructs your dataset without having to go through multiple different instances of your query. Think about it like splitting your data collection and the actual work/display you do with that data into two different parts that function independently of each other. Utilizing this will make the maintenance of your dashboards much easier since you can just change the dune_user_generated view instead of having to go through all queries individually. A great example of this in action is almost all queries on this dashboard . The Creator made one base dataset in the dune_user_generated schema and uses that to base all of his queries on. Please do note that while this approach works for most cases, views can get very computationally expensive and you might be better off constructing a materialized view or table in our Spells . This example takes the data from Uniswap_v3 and standardizes the data for the dex.trades table. CREATE OR REPLACE view dune_user_generated . uniswap_v3 as SELECT dexs . block_time , erc20a . symbol AS token_a_symbol , erc20b . symbol AS token_b_symbol , token_a_amount_raw / 10 ^ erc20a . decimals AS token_a_amount , token_b_amount_raw / 10 ^ erc20b . decimals AS token_b_amount , project , version , category , coalesce ( trader_a , tx . \"from\" ) as trader_a , -- subqueries rely on this COALESCE to avoid redundant joins with the transactions table trader_b , token_a_amount_raw , token_b_amount_raw , coalesce ( usd_amount , token_a_amount_raw / 10 ^ erc20a . decimals * pa . price , token_b_amount_raw / 10 ^ erc20b . decimals * pb . price ) as usd_amount , token_a_address , token_b_address , exchange_contract_address , tx_hash , tx . \"from\" as tx_from , tx . \"to\" as tx_to , trace_address , evt_index , row_number () OVER ( PARTITION BY tx_hash , evt_index , trace_address ) AS trade_id FROM ( --Uniswap v3 SELECT t . evt_block_time AS block_time , 'Uniswap' AS project , '3' AS version , 'DEX' AS category , t . \"recipient\" AS trader_a , NULL :: bytea AS trader_b , abs ( amount0 ) AS token_a_amount_raw , abs ( amount1 ) AS token_b_amount_raw , NULL :: numeric AS usd_amount , f . token0 AS token_a_address , f . token1 AS token_b_address , t . contract_address as exchange_contract_address , t . evt_tx_hash AS tx_hash , NULL :: integer [] AS trace_address , t . evt_index FROM uniswap_v3 . \"Pair_evt_Swap\" t INNER JOIN uniswap_v3 . \"Factory_evt_PoolCreated\" f ON f . pool = t . contract_address ) dexs INNER JOIN ethereum . transactions tx ON dexs . tx_hash = tx . hash LEFT JOIN erc20 . tokens erc20a ON erc20a . contract_address = dexs . token_a_address LEFT JOIN erc20 . tokens erc20b ON erc20b . contract_address = dexs . token_b_address LEFT JOIN prices . usd pa ON pa . minute = date_trunc ( 'minute' , dexs . block_time ) AND pa . contract_address = dexs . token_a_address LEFT JOIN prices . usd pb ON pb . minute = date_trunc ( 'minute' , dexs . block_time ) AND pb . contract_address = dexs . token_b_address https://dune.com/queries/42779 Testing Abstractions \u00b6 Another great use case of utilizing the \"create\" function is to test out if the Pull Request you are making to our abstractions GitHub actually produce the intended results. Simply try running the query with the schema dune_user_generated instead of the actual schema that you want in GitHub. If the test succeeds, you can proceed in making the Pull Request. If you can please attach the \"Test Table/View\" into the Pull Request. View Definition \u00b6 To find out how a particular view got created you can run queries against PostgreSQL base tables. A particular view select definition from pg_views where viewname = 'view_name_here' All views select * from pg_views where schemaname = 'dune_user_generated' View dependencies \u00b6 If you build multiple views that are dependent on each other it might sometimes happen that you can't change view1 because view2 depends on view1 . This can be remedied by running the query below to fix any dependency issues. -- source: https://stackoverflow.com/a/48770535/1838257 --CREATE OR REPLACE VIEW dune_user_generated.gosuto_view_dependencies AS SELECT DISTINCT srcobj . oid AS src_oid , srcnsp . nspname AS src_schemaname , srcobj . relname AS src_objectname , tgtobj . oid AS dependent_viewoid , tgtnsp . nspname AS dependant_schemaname , tgtobj . relname AS dependant_objectname FROM pg_class srcobj JOIN pg_depend srcdep ON srcobj . oid = srcdep . refobjid JOIN pg_depend tgtdep ON srcdep . objid = tgtdep . objid JOIN pg_class tgtobj ON tgtdep . refobjid = tgtobj . oid AND srcobj . oid <> tgtobj . oid LEFT JOIN pg_namespace srcnsp ON srcobj . relnamespace = srcnsp . oid LEFT JOIN pg_namespace tgtnsp ON tgtobj . relnamespace = tgtnsp . oid WHERE tgtdep . deptype = 'i' :: \"char\" AND tgtobj . relkind = 'v' :: \"char\" -- filter like so: -- SELECT * FROM dune_user_generated.gosuto_view_dependencies -- WHERE src_objectname LIKE '%filter_word%' You need to temporarily break the dependencies in order to be able to change view1 . Find the query here . Big thanks to @gosuto for uncovering this.","title":"User Generated Tables"},{"location":"reference/tables/user-generated/#use-cases","text":"There is several ways in which you can utilize your own views and tables inside of Dune to make working with your data on Dune even easier. Your own tables, views and function all have an important part to play in creating content on Dune and make maintenance of your dashboards and queries easier if used correctly. If you are unfamiliar with tables, views, materialized views and functions please consult the pgSQL documentation or check out our getting started guide .","title":"Use Cases"},{"location":"reference/tables/user-generated/#storing-information","text":"Sometimes certain references or information that you do need for your data extraction are not properly stored in available tables or stored in many different tables, which would makes the query quite hard to work with. In these cases, you are best off storing the necessary information in a view and referencing that view. An example of this would be the mapping of certain vault or lending tokens to their respective underlying tokens. CREATE OR REPLACE VIEW dune_user_generated . view_test ( symbol , contract_address , decimals , underlying_token_address ) AS VALUES ( 'iETH' :: text , '\\x9Dde7cdd09dbed542fC422d18d89A589fA9fD4C0' :: bytea , 18 :: numeric , '\\xc02aaa39b223fe8d0a0e5c4f27ead9083c756cc2' :: bytea ), ( 'yDAI' :: text , '\\x9D25057e62939D3408406975aD75Ffe834DA4cDd' :: bytea , 18 :: numeric , '\\x6B175474E89094C44Da98b954EedeAC495271d0F' :: bytea ), ( 'yUSDC' :: text , '\\xa2609b2b43ac0f5ebe27deb944d2a399c201e3da' :: bytea , 6 :: numeric , '\\xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48' :: bytea ), ( 'ySUSD' :: text , '\\x36324b8168f960A12a8fD01406C9C78143d41380' :: bytea , 18 :: numeric , '\\x57Ab1ec28D129707052df4dF418D58a2D46d5f51' :: bytea ), ( 'yUSDT' :: text , '\\xa1787206d5b1bE0f432C4c4f96Dc4D1257A1Dd14' :: bytea , 6 :: numeric , '\\xdAC17F958D2ee523a2206206994597C13D831ec7' :: bytea ), ( 'yCRV' :: text , '\\x9Ce551A9D2B1A4Ec0cc6eB0E0CC12977F6ED306C' :: bytea , 18 :: numeric , '\\x6B175474E89094C44Da98b954EedeAC495271d0F' :: bytea ), ( 'yBTC' :: text , '\\x04EF8121aD039ff41d10029c91EA1694432514e9' :: bytea , 8 :: numeric , '\\x2260FAC5E5542a773Aa44fBCfeDf7C193bc2C599' :: bytea ) This table generates a view that you can use to join on your query. Look at the table .","title":"Storing Information"},{"location":"reference/tables/user-generated/#aggregating-data","text":"Views can also be used to aggregate the actions of multiple smart contracts into one view that contains all the necessary data. This is especially useful if you are working with the same dataset over and over and only change the way you display or aggregate the data. That way, instead of having to query for your dataset again and again, you just put it into a view once and then can start referencing that view. This will allow you to change the base query that constructs your dataset without having to go through multiple different instances of your query. Think about it like splitting your data collection and the actual work/display you do with that data into two different parts that function independently of each other. Utilizing this will make the maintenance of your dashboards much easier since you can just change the dune_user_generated view instead of having to go through all queries individually. A great example of this in action is almost all queries on this dashboard . The Creator made one base dataset in the dune_user_generated schema and uses that to base all of his queries on. Please do note that while this approach works for most cases, views can get very computationally expensive and you might be better off constructing a materialized view or table in our Spells . This example takes the data from Uniswap_v3 and standardizes the data for the dex.trades table. CREATE OR REPLACE view dune_user_generated . uniswap_v3 as SELECT dexs . block_time , erc20a . symbol AS token_a_symbol , erc20b . symbol AS token_b_symbol , token_a_amount_raw / 10 ^ erc20a . decimals AS token_a_amount , token_b_amount_raw / 10 ^ erc20b . decimals AS token_b_amount , project , version , category , coalesce ( trader_a , tx . \"from\" ) as trader_a , -- subqueries rely on this COALESCE to avoid redundant joins with the transactions table trader_b , token_a_amount_raw , token_b_amount_raw , coalesce ( usd_amount , token_a_amount_raw / 10 ^ erc20a . decimals * pa . price , token_b_amount_raw / 10 ^ erc20b . decimals * pb . price ) as usd_amount , token_a_address , token_b_address , exchange_contract_address , tx_hash , tx . \"from\" as tx_from , tx . \"to\" as tx_to , trace_address , evt_index , row_number () OVER ( PARTITION BY tx_hash , evt_index , trace_address ) AS trade_id FROM ( --Uniswap v3 SELECT t . evt_block_time AS block_time , 'Uniswap' AS project , '3' AS version , 'DEX' AS category , t . \"recipient\" AS trader_a , NULL :: bytea AS trader_b , abs ( amount0 ) AS token_a_amount_raw , abs ( amount1 ) AS token_b_amount_raw , NULL :: numeric AS usd_amount , f . token0 AS token_a_address , f . token1 AS token_b_address , t . contract_address as exchange_contract_address , t . evt_tx_hash AS tx_hash , NULL :: integer [] AS trace_address , t . evt_index FROM uniswap_v3 . \"Pair_evt_Swap\" t INNER JOIN uniswap_v3 . \"Factory_evt_PoolCreated\" f ON f . pool = t . contract_address ) dexs INNER JOIN ethereum . transactions tx ON dexs . tx_hash = tx . hash LEFT JOIN erc20 . tokens erc20a ON erc20a . contract_address = dexs . token_a_address LEFT JOIN erc20 . tokens erc20b ON erc20b . contract_address = dexs . token_b_address LEFT JOIN prices . usd pa ON pa . minute = date_trunc ( 'minute' , dexs . block_time ) AND pa . contract_address = dexs . token_a_address LEFT JOIN prices . usd pb ON pb . minute = date_trunc ( 'minute' , dexs . block_time ) AND pb . contract_address = dexs . token_b_address https://dune.com/queries/42779","title":"Aggregating Data"},{"location":"reference/tables/user-generated/#testing-abstractions","text":"Another great use case of utilizing the \"create\" function is to test out if the Pull Request you are making to our abstractions GitHub actually produce the intended results. Simply try running the query with the schema dune_user_generated instead of the actual schema that you want in GitHub. If the test succeeds, you can proceed in making the Pull Request. If you can please attach the \"Test Table/View\" into the Pull Request.","title":"Testing Abstractions"},{"location":"reference/tables/user-generated/#view-definition","text":"To find out how a particular view got created you can run queries against PostgreSQL base tables. A particular view select definition from pg_views where viewname = 'view_name_here' All views select * from pg_views where schemaname = 'dune_user_generated'","title":"View Definition"},{"location":"reference/tables/user-generated/#view-dependencies","text":"If you build multiple views that are dependent on each other it might sometimes happen that you can't change view1 because view2 depends on view1 . This can be remedied by running the query below to fix any dependency issues. -- source: https://stackoverflow.com/a/48770535/1838257 --CREATE OR REPLACE VIEW dune_user_generated.gosuto_view_dependencies AS SELECT DISTINCT srcobj . oid AS src_oid , srcnsp . nspname AS src_schemaname , srcobj . relname AS src_objectname , tgtobj . oid AS dependent_viewoid , tgtnsp . nspname AS dependant_schemaname , tgtobj . relname AS dependant_objectname FROM pg_class srcobj JOIN pg_depend srcdep ON srcobj . oid = srcdep . refobjid JOIN pg_depend tgtdep ON srcdep . objid = tgtdep . objid JOIN pg_class tgtobj ON tgtdep . refobjid = tgtobj . oid AND srcobj . oid <> tgtobj . oid LEFT JOIN pg_namespace srcnsp ON srcobj . relnamespace = srcnsp . oid LEFT JOIN pg_namespace tgtnsp ON tgtobj . relnamespace = tgtnsp . oid WHERE tgtdep . deptype = 'i' :: \"char\" AND tgtobj . relkind = 'v' :: \"char\" -- filter like so: -- SELECT * FROM dune_user_generated.gosuto_view_dependencies -- WHERE src_objectname LIKE '%filter_word%' You need to temporarily break the dependencies in order to be able to change view1 . Find the query here . Big thanks to @gosuto for uncovering this.","title":"View dependencies"},{"location":"reference/tables/community/","text":"Note Community is only available on V2 Engine. While Blockchain data is cool on it's own and we do our best to prepare, standardize and work with that data, sometimes a bit of off-chain data or augmented on-chain data is needed as well. Therefore, we are starting to partner with selected organizations that stream their data directly to Dune. We are still building out the infrastructure for this endeavor and can therefore not support more datasets at the current moment. Flashbots Reservoir","title":"Community"},{"location":"reference/tables/community/flashbots/","text":"Flashbots \u00b6 Note: mev-inspect-py, Flashbots\u2019 open source engine for generating MEV data, is used to power dashboards such as mev-explore and Dune\u2019s Flashbots integration. We\u2019re always looking to improve, fix bugs, cover edge cases, and add protocol coverage to the best of our ability with the help of our community and contributors. We encourage researchers and developers to report and help correct any found bugs, or implement any new features! Feel free to consult the documentation and join the Flashbots discord for more information and updates on our data and mev-inspect. Docs: https://docs.flashbots.net/ Discord: https://discord.gg/7hvTycdNcK","title":"Flashbots"},{"location":"reference/tables/community/flashbots/#flashbots","text":"Note: mev-inspect-py, Flashbots\u2019 open source engine for generating MEV data, is used to power dashboards such as mev-explore and Dune\u2019s Flashbots integration. We\u2019re always looking to improve, fix bugs, cover edge cases, and add protocol coverage to the best of our ability with the help of our community and contributors. We encourage researchers and developers to report and help correct any found bugs, or implement any new features! Feel free to consult the documentation and join the Flashbots discord for more information and updates on our data and mev-inspect. Docs: https://docs.flashbots.net/ Discord: https://discord.gg/7hvTycdNcK","title":"Flashbots"},{"location":"reference/tables/community/flashbots/arbitrages/","text":"arbitrages \u00b6 flashbots.arbitrages \u00b6 This table contains records with additional information about each arbitrage trade. Query examples can be found here: Total Arb Protocols Column name Type Description block_number bigint Block number account_address string Address of the searcher created_at string Time of the record creation end_amount bigint Available amount after the arbitrage error string Available amount after the arbitrage id string Internal id of the arbitrage profit_amount bigint Profit amount after the arbitrage profit_token_address string Address of the profit asset protocols string List of protocols involved in the transaction start_amount bigint Available amount before the arbitrage transaction_hash string Hash of the transaction timestamp timestamp Timestamp of the latest update of the file","title":"arbitrages"},{"location":"reference/tables/community/flashbots/arbitrages/#arbitrages","text":"","title":"arbitrages"},{"location":"reference/tables/community/flashbots/arbitrages/#flashbotsarbitrages","text":"This table contains records with additional information about each arbitrage trade. Query examples can be found here: Total Arb Protocols Column name Type Description block_number bigint Block number account_address string Address of the searcher created_at string Time of the record creation end_amount bigint Available amount after the arbitrage error string Available amount after the arbitrage id string Internal id of the arbitrage profit_amount bigint Profit amount after the arbitrage profit_token_address string Address of the profit asset protocols string List of protocols involved in the transaction start_amount bigint Available amount before the arbitrage transaction_hash string Hash of the transaction timestamp timestamp Timestamp of the latest update of the file","title":"flashbots.arbitrages"},{"location":"reference/tables/community/flashbots/liquidations/","text":"liquidations \u00b6 flashbots.liquidations \u00b6 Liquidation is another MEV strategy. This table contains details related to executed liquidations. Query examples can be found here: Liquidations by Protocol Column name Type Description created_at string Time of the records creation transaction_hash string Transaction hash trace_address string Trace pattern related to the position of the transaction in the chain of all transactions related to the MEV trade. debt_token_address string Underlying token address of the debt to pay received_amount bigint Amount received from the liquidation protocol string Protocol name liquidated_user string Address of the liquidated user liquidator_user string Address of the liquidator user received_token_address string Address of the received asset block_number bigint Block number debt_purchase_amount bigint Amount of purchased debt timestamp timestamp Timestamp of the latest update of the file","title":"liquidations"},{"location":"reference/tables/community/flashbots/liquidations/#liquidations","text":"","title":"liquidations"},{"location":"reference/tables/community/flashbots/liquidations/#flashbotsliquidations","text":"Liquidation is another MEV strategy. This table contains details related to executed liquidations. Query examples can be found here: Liquidations by Protocol Column name Type Description created_at string Time of the records creation transaction_hash string Transaction hash trace_address string Trace pattern related to the position of the transaction in the chain of all transactions related to the MEV trade. debt_token_address string Underlying token address of the debt to pay received_amount bigint Amount received from the liquidation protocol string Protocol name liquidated_user string Address of the liquidated user liquidator_user string Address of the liquidator user received_token_address string Address of the received asset block_number bigint Block number debt_purchase_amount bigint Amount of purchased debt timestamp timestamp Timestamp of the latest update of the file","title":"flashbots.liquidations"},{"location":"reference/tables/community/flashbots/mev_summary/","text":"mev_summary \u00b6 flashbots.mev_summary \u00b6 This table contains summary of all the classified transactions Query examples can be found here: Miner Revenue from Liquidations and Arbitrages Column name Type Description block_timestamp timestamp Block timestamp block_number bigint Block number base_fee_per_gas bigint Base fee per gas coinbase_transfer bigint Direct transfer to miner\u2019s address error string Error if exists gas_price bigint Price of the gas gas_price_with_coinbase_transfer bigint Amount of gas spent + direct transfer to miner address gas_used bigint Amount of gas used gross_profit_usd double Total profit from the transaction in usd miner_address string Address of the miner miner_payment_usd double Payment received by the miner in usd protocol string Main interacted protocol protocols string List of protocols involved in the transaction transaction_hash string Hash of the transaction type string Type of the MEV (e.g. arbitrage) timestamp timestamp Timestamp of the latest update of the file","title":"mev\\_summary"},{"location":"reference/tables/community/flashbots/mev_summary/#mev_summary","text":"","title":"mev_summary"},{"location":"reference/tables/community/flashbots/mev_summary/#flashbotsmev_summary","text":"This table contains summary of all the classified transactions Query examples can be found here: Miner Revenue from Liquidations and Arbitrages Column name Type Description block_timestamp timestamp Block timestamp block_number bigint Block number base_fee_per_gas bigint Base fee per gas coinbase_transfer bigint Direct transfer to miner\u2019s address error string Error if exists gas_price bigint Price of the gas gas_price_with_coinbase_transfer bigint Amount of gas spent + direct transfer to miner address gas_used bigint Amount of gas used gross_profit_usd double Total profit from the transaction in usd miner_address string Address of the miner miner_payment_usd double Payment received by the miner in usd protocol string Main interacted protocol protocols string List of protocols involved in the transaction transaction_hash string Hash of the transaction type string Type of the MEV (e.g. arbitrage) timestamp timestamp Timestamp of the latest update of the file","title":"flashbots.mev_summary"},{"location":"reference/tables/community/flashbots/sandwiched-swaps/","text":"sandwiched swaps \u00b6 flashbots.sandwiched_swaps \u00b6 The sandwiched_swaps table contains additional data about one or more swaps that were sandwiched with a corresponding sandwich in the database. Query examples can be found here: Column name Type Description created_at string Time of the records creation block_number bigint Block number sandwich_id string Internal id of the sandwiched swap trace_address string Trace pattern related to the position of the swap in the chain of all swaps related to the arbitrage trade. transaction_hash string Transaction hash timestamp timestamp Timestamp of the latest update of the file","title":"sandwiched swaps"},{"location":"reference/tables/community/flashbots/sandwiched-swaps/#sandwiched-swaps","text":"","title":"sandwiched swaps"},{"location":"reference/tables/community/flashbots/sandwiched-swaps/#flashbotssandwiched_swaps","text":"The sandwiched_swaps table contains additional data about one or more swaps that were sandwiched with a corresponding sandwich in the database. Query examples can be found here: Column name Type Description created_at string Time of the records creation block_number bigint Block number sandwich_id string Internal id of the sandwiched swap trace_address string Trace pattern related to the position of the swap in the chain of all swaps related to the arbitrage trade. transaction_hash string Transaction hash timestamp timestamp Timestamp of the latest update of the file","title":"flashbots.sandwiched_swaps"},{"location":"reference/tables/community/flashbots/sandwiches/","text":"sandwiches \u00b6 sandwiches \u00b6 This table contains detailed information about executed sandwiches Column name Type Description created_at datetime Time of the records creation block_number bigint Block number backrun_swap_trace_address string address of the swap in the backrun transaction backrun_swap_transaction_hash string transaction_hash of backrun transaction of specified sandwich frontrun_swap_trace_address string address of the swap in the frontrun transaction frontrun_swap_transaction_hash string transaction_hash of frontrun transaction of specified sandwich id string Internal id of the sandwich profit_amount bigint Profit amount after the arbitrage profit_token_address string Address of the profit asset sandwicher_address string Address of the sandwicher timestamp timestamp Timestamp of the latest update of the file **** \u00b6","title":"sandwiches"},{"location":"reference/tables/community/flashbots/sandwiches/#sandwiches","text":"","title":"sandwiches"},{"location":"reference/tables/community/flashbots/sandwiches/#sandwiches_1","text":"This table contains detailed information about executed sandwiches Column name Type Description created_at datetime Time of the records creation block_number bigint Block number backrun_swap_trace_address string address of the swap in the backrun transaction backrun_swap_transaction_hash string transaction_hash of backrun transaction of specified sandwich frontrun_swap_trace_address string address of the swap in the frontrun transaction frontrun_swap_transaction_hash string transaction_hash of frontrun transaction of specified sandwich id string Internal id of the sandwich profit_amount bigint Profit amount after the arbitrage profit_token_address string Address of the profit asset sandwicher_address string Address of the sandwicher timestamp timestamp Timestamp of the latest update of the file","title":"sandwiches"},{"location":"reference/tables/community/flashbots/sandwiches/#_1","text":"","title":"****"},{"location":"reference/tables/community/reservoir/","text":"Reservoir \u00b6 Dashboard: https://dune.com/reservoir0x/reservoir-dune-docs-dashboard Docs: https://docs.reservoir.tools/docs Discord: https://discord.gg/jbEUwrVx","title":"Reservoir"},{"location":"reference/tables/community/reservoir/#reservoir","text":"Dashboard: https://dune.com/reservoir0x/reservoir-dune-docs-dashboard Docs: https://docs.reservoir.tools/docs Discord: https://discord.gg/jbEUwrVx","title":"Reservoir"},{"location":"reference/tables/community/reservoir/ask-events/","text":"ask events \u00b6 reservoir.ask_events \u00b6 This table contains records with information about each ask change. Query examples can be found here: https://dune.com/queries/1302858/2232178 https://dune.com/queries/1302863/2232189 Column name Type Description id bigint Internal event id kind string Event type (new-order, expiry, sale, cancel, balance-change, approval-change, bootstrap, revalidation, reprice) contract string Contract address token_id string Id of the token in the collection order_id string Associated ask id maker string Associated ask maker wallet address price decimal Associated ask price (native currency) quantity_remaining bigint Associated ask tokens remaining valid_from bigint Associated ask validity start valid_until bigint Associated ask validity expiration source string Source of the order (e.g. opensea.io) tx_hash string Associated transaction hash tx_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the event was recorded","title":"ask events"},{"location":"reference/tables/community/reservoir/ask-events/#ask-events","text":"","title":"ask events"},{"location":"reference/tables/community/reservoir/ask-events/#reservoirask_events","text":"This table contains records with information about each ask change. Query examples can be found here: https://dune.com/queries/1302858/2232178 https://dune.com/queries/1302863/2232189 Column name Type Description id bigint Internal event id kind string Event type (new-order, expiry, sale, cancel, balance-change, approval-change, bootstrap, revalidation, reprice) contract string Contract address token_id string Id of the token in the collection order_id string Associated ask id maker string Associated ask maker wallet address price decimal Associated ask price (native currency) quantity_remaining bigint Associated ask tokens remaining valid_from bigint Associated ask validity start valid_until bigint Associated ask validity expiration source string Source of the order (e.g. opensea.io) tx_hash string Associated transaction hash tx_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the event was recorded","title":"reservoir.ask_events"},{"location":"reference/tables/community/reservoir/asks/","text":"asks \u00b6 reservoir.asks \u00b6 This table contains records with information about each listing. Query examples can be found here: https://dune.com/queries/1302885/2232229 https://dune.com/queries/1302904/2232257 Column name Type Description id string Internal order id kind string Protocol name (e.g. seaport) status string Order status (active, inactive) contract string Contract address token_id string Id of the token in the collection maker string Maker wallet address taker string Taker wallet address price decimal The current price (native currency) start_price bigint Start Listing price (for dutch auctions) end_price bigint End Listing price (for dutch auctions) currency_address string Currency address currency_symbol string Currency symbol currency_price decimal Currency price dynamic boolean Is dutch auction? quantity bigint Amount of tokens that is listed quantity_filled bigint Amount of tokens that was filled quantity_remaining bigint Amount of tokens remaining valid_from bigint Listing start time valid_until bigint Listing end time nonce string The order nonce of the maker source string Source of the listing (e.g. opensea.io) fee_bps bigint Listing fee expiration bigint Associated transaction hash raw_data string Raw order data (format will vary per source) created_at timestamp Timestamp the listing was created updated_at timestamp Timestamp the listing was updated","title":"asks"},{"location":"reference/tables/community/reservoir/asks/#asks","text":"","title":"asks"},{"location":"reference/tables/community/reservoir/asks/#reservoirasks","text":"This table contains records with information about each listing. Query examples can be found here: https://dune.com/queries/1302885/2232229 https://dune.com/queries/1302904/2232257 Column name Type Description id string Internal order id kind string Protocol name (e.g. seaport) status string Order status (active, inactive) contract string Contract address token_id string Id of the token in the collection maker string Maker wallet address taker string Taker wallet address price decimal The current price (native currency) start_price bigint Start Listing price (for dutch auctions) end_price bigint End Listing price (for dutch auctions) currency_address string Currency address currency_symbol string Currency symbol currency_price decimal Currency price dynamic boolean Is dutch auction? quantity bigint Amount of tokens that is listed quantity_filled bigint Amount of tokens that was filled quantity_remaining bigint Amount of tokens remaining valid_from bigint Listing start time valid_until bigint Listing end time nonce string The order nonce of the maker source string Source of the listing (e.g. opensea.io) fee_bps bigint Listing fee expiration bigint Associated transaction hash raw_data string Raw order data (format will vary per source) created_at timestamp Timestamp the listing was created updated_at timestamp Timestamp the listing was updated","title":"reservoir.asks"},{"location":"reference/tables/community/reservoir/attribute-keys/","text":"attribute keys \u00b6 reservoir.attribute_keys \u00b6 This table contains records with information about each attribute key. Query examples can be found here: https://dune.com/queries/1302930/2232305 Column name Type Description id string Internal attribute key id collection_id string Associated collection id key string The name of the attribute kind string Value type (string, number, date, range) rank string Sort order created_at timestamp Timestamp the attribute key was created updated_at timestamp Timestamp the attribute key was updated","title":"attribute keys"},{"location":"reference/tables/community/reservoir/attribute-keys/#attribute-keys","text":"","title":"attribute keys"},{"location":"reference/tables/community/reservoir/attribute-keys/#reservoirattribute_keys","text":"This table contains records with information about each attribute key. Query examples can be found here: https://dune.com/queries/1302930/2232305 Column name Type Description id string Internal attribute key id collection_id string Associated collection id key string The name of the attribute kind string Value type (string, number, date, range) rank string Sort order created_at timestamp Timestamp the attribute key was created updated_at timestamp Timestamp the attribute key was updated","title":"reservoir.attribute_keys"},{"location":"reference/tables/community/reservoir/attributes/","text":"attributes \u00b6 reservoir.attributes \u00b6 This table contains records with information about each attribute. Query examples can be found here: https://dune.com/queries/1302927/2232298 https://dune.com/queries/1302966/2232361 Column name Type Description id bigint Internal attribute id attribute_key_id bigint Internal attribute key id value string Attribute value token_count bigint Amount of tokens that have the attribute on_sale_count bigint Amount of tokens that have the attribute which are on sale floor_sell_value decimal Current floor ask price sell_updated_at timestamp Timestamp the floor sale was last updated collection_id string Associated collection id kind string Value type (string, number, date, range) key string Associated key name created_at timestamp Timestamp the attribute was created updated_at timestamp Timestamp the attribute was updated","title":"attributes"},{"location":"reference/tables/community/reservoir/attributes/#attributes","text":"","title":"attributes"},{"location":"reference/tables/community/reservoir/attributes/#reservoirattributes","text":"This table contains records with information about each attribute. Query examples can be found here: https://dune.com/queries/1302927/2232298 https://dune.com/queries/1302966/2232361 Column name Type Description id bigint Internal attribute id attribute_key_id bigint Internal attribute key id value string Attribute value token_count bigint Amount of tokens that have the attribute on_sale_count bigint Amount of tokens that have the attribute which are on sale floor_sell_value decimal Current floor ask price sell_updated_at timestamp Timestamp the floor sale was last updated collection_id string Associated collection id kind string Value type (string, number, date, range) key string Associated key name created_at timestamp Timestamp the attribute was created updated_at timestamp Timestamp the attribute was updated","title":"reservoir.attributes"},{"location":"reference/tables/community/reservoir/collection-floor-ask-events/","text":"collection floor ask events \u00b6 reservoir.collection_floor_ask_events \u00b6 This table contains records with information about each collection floor ask change. Query examples can be found here: https://dune.com/queries/1302799/2232083 https://dune.com/queries/1302841/2232151 Column name Type Description id bigint Internal event id kind string Event type (new-order, expiry, sale, cancel, balance-change, approval-change, bootstrap, revalidation, reprice) collection_id string Collection id contract string Contract address token_id string Id of the token in the collection order_id string Associated ask id maker string Associated ask maker wallet address price decimal Associated ask price (native currency) previous_price decimal previous floor ask price (native currency) valid_until bigint Associated ask validity expiration source string Source of the order (e.g. opensea.io) tx_hash string Associated transaction hash tx_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the event was recorded","title":"collection floor ask events"},{"location":"reference/tables/community/reservoir/collection-floor-ask-events/#collection-floor-ask-events","text":"","title":"collection floor ask events"},{"location":"reference/tables/community/reservoir/collection-floor-ask-events/#reservoircollection_floor_ask_events","text":"This table contains records with information about each collection floor ask change. Query examples can be found here: https://dune.com/queries/1302799/2232083 https://dune.com/queries/1302841/2232151 Column name Type Description id bigint Internal event id kind string Event type (new-order, expiry, sale, cancel, balance-change, approval-change, bootstrap, revalidation, reprice) collection_id string Collection id contract string Contract address token_id string Id of the token in the collection order_id string Associated ask id maker string Associated ask maker wallet address price decimal Associated ask price (native currency) previous_price decimal previous floor ask price (native currency) valid_until bigint Associated ask validity expiration source string Source of the order (e.g. opensea.io) tx_hash string Associated transaction hash tx_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the event was recorded","title":"reservoir.collection_floor_ask_events"},{"location":"reference/tables/community/reservoir/collections/","text":"collections \u00b6 reservoir.collections \u00b6 This table contains records with information about each NFT collection. Query examples can be found here: https://dune.com/queries/1302781/2232054 https://dune.com/queries/1302788/2232065 Column name Type Description id string Internal collection id slug string Collection slug name string Collection name description string Collection description token_count bigint Id of the token in the collection contract string Contract address day1_rank bigint Ranking in the previous day day7_rank bigint Ranking in the previous 7 days day30_rank bigint Ranking in the previous 30 days all_time_rank bigint All time ranking day1_volume decimal Trade volume in the previous day day7_volume decimal Trade volume in the previous 7 days day30_volume decimal Trade volume in the previous 30 days all_time_volume decimal All time trade volume day1_volume_change double Trade volume change in the previous day day7_volume_change double Trade volume change in the previous 7 days day30_volume_change double Trade volume change in the previous 30 days floor\\ask_value decimal Current floor sale price (native currency) day1_floor_sale_value decimal Floor sale price in the previous day day7_floor_sale_value decimal Floor sale price 7 days ago day30_floor_sale_value decimal Floor sale price 30 days ago day1_floor_sale_change double Floor sale price change from previous day day7_floor_sale_change double Floor sale price change from 7 days ago day30_floor_sale_change double Floor sale price change from 30 days ago created_at timestamp Timestamp the collection was created updated_at timestamp Timestamp the collection was updated","title":"collections"},{"location":"reference/tables/community/reservoir/collections/#collections","text":"","title":"collections"},{"location":"reference/tables/community/reservoir/collections/#reservoircollections","text":"This table contains records with information about each NFT collection. Query examples can be found here: https://dune.com/queries/1302781/2232054 https://dune.com/queries/1302788/2232065 Column name Type Description id string Internal collection id slug string Collection slug name string Collection name description string Collection description token_count bigint Id of the token in the collection contract string Contract address day1_rank bigint Ranking in the previous day day7_rank bigint Ranking in the previous 7 days day30_rank bigint Ranking in the previous 30 days all_time_rank bigint All time ranking day1_volume decimal Trade volume in the previous day day7_volume decimal Trade volume in the previous 7 days day30_volume decimal Trade volume in the previous 30 days all_time_volume decimal All time trade volume day1_volume_change double Trade volume change in the previous day day7_volume_change double Trade volume change in the previous 7 days day30_volume_change double Trade volume change in the previous 30 days floor\\ask_value decimal Current floor sale price (native currency) day1_floor_sale_value decimal Floor sale price in the previous day day7_floor_sale_value decimal Floor sale price 7 days ago day30_floor_sale_value decimal Floor sale price 30 days ago day1_floor_sale_change double Floor sale price change from previous day day7_floor_sale_change double Floor sale price change from 7 days ago day30_floor_sale_change double Floor sale price change from 30 days ago created_at timestamp Timestamp the collection was created updated_at timestamp Timestamp the collection was updated","title":"reservoir.collections"},{"location":"reference/tables/community/reservoir/sales/","text":"sales \u00b6 reservoir.sales \u00b6 This table contains records with information about each sale. Query examples can be found here: https://dune.com/queries/1302771/2232036 https://dune.com/queries/1302775/2232040 Column name Type Description id string Internal sale id contract string Contract address token_id string Id of the token in the collection order_id string Associated order id order_kind string Protocol name (e.g. seaport) order_side string Order type (ask / bid) order_source string Source of the listing (e.g. opensea.io) from string Maker wallet address to string Taker wallet address price decimal Sale price (native currency) usd_price string Sale price in USD currency_address string The currency address used for this sale currency_symbol string The currency symbol used for this sale currency_price decimal Sale price amount string Amount of tokens sold fill_source string Where the order was filled aggregator_source string aggregator source (e.g. reservoir) wash_trading_score int Internal wash trading score (based on past sales) is_primary boolean Is paid mint? tx_hash string Associated transaction hash tx_log_index int Associated transaction log index tx_batch_index int Associated transaction batch index tx_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the sale was recorded updated_at timestamp Timestamp the sale was updated","title":"sales"},{"location":"reference/tables/community/reservoir/sales/#sales","text":"","title":"sales"},{"location":"reference/tables/community/reservoir/sales/#reservoirsales","text":"This table contains records with information about each sale. Query examples can be found here: https://dune.com/queries/1302771/2232036 https://dune.com/queries/1302775/2232040 Column name Type Description id string Internal sale id contract string Contract address token_id string Id of the token in the collection order_id string Associated order id order_kind string Protocol name (e.g. seaport) order_side string Order type (ask / bid) order_source string Source of the listing (e.g. opensea.io) from string Maker wallet address to string Taker wallet address price decimal Sale price (native currency) usd_price string Sale price in USD currency_address string The currency address used for this sale currency_symbol string The currency symbol used for this sale currency_price decimal Sale price amount string Amount of tokens sold fill_source string Where the order was filled aggregator_source string aggregator source (e.g. reservoir) wash_trading_score int Internal wash trading score (based on past sales) is_primary boolean Is paid mint? tx_hash string Associated transaction hash tx_log_index int Associated transaction log index tx_batch_index int Associated transaction batch index tx_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the sale was recorded updated_at timestamp Timestamp the sale was updated","title":"reservoir.sales"},{"location":"reference/tables/community/reservoir/token-attributes/","text":"token attributes \u00b6 reservoir.token_attributes \u00b6 This table contains records with information about each NFT token attribute. Query examples can be found here: https://dune.com/queries/1302940/2232326 Column name Type Description id bigint Internal token attribute id contract string Contract address token_id string Id of the token in the collection attribute_id bigint Internal attribute id collection_id string Internal collection id key string Attribute name value string Attribute value created_at timestamp Timestamp the token attribute was created updated_at timestamp Timestamp the token attribute was updated","title":"token attributes"},{"location":"reference/tables/community/reservoir/token-attributes/#token-attributes","text":"","title":"token attributes"},{"location":"reference/tables/community/reservoir/token-attributes/#reservoirtoken_attributes","text":"This table contains records with information about each NFT token attribute. Query examples can be found here: https://dune.com/queries/1302940/2232326 Column name Type Description id bigint Internal token attribute id contract string Contract address token_id string Id of the token in the collection attribute_id bigint Internal attribute id collection_id string Internal collection id key string Attribute name value string Attribute value created_at timestamp Timestamp the token attribute was created updated_at timestamp Timestamp the token attribute was updated","title":"reservoir.token_attributes"},{"location":"reference/tables/community/reservoir/token-floor-ask-events/","text":"token floor ask events \u00b6 reservoir.token_floor_ask_events \u00b6 This table contains records with information about each NFT token floor ask change. Query examples can be found here: https://dune.com/queries/1302852/2232169 https://dune.com/queries/1302854/2232173 Column name Type Description id bigint Internal token attribute id kind string Event type (new-order, expiry, sale, cancel, balance-change, approval-change, bootstrap, revalidation, reprice) contract string Contract address token_id string Id of the token in the collection order_id string Associated Ask id maker string Associated Ask maker wallet address price decimal Associated ask price (native currency) previous_price decimal Associated ask price (native currency) nonce string The order nonce of the maker valid_from bigint Associated ask validity start valid_until bigint Associated ask validity expiration source string Source of the order (e.g. opensea.io) tx_hash string Associated transaction hash tx_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the event was recorded","title":"token floor ask events"},{"location":"reference/tables/community/reservoir/token-floor-ask-events/#token-floor-ask-events","text":"","title":"token floor ask events"},{"location":"reference/tables/community/reservoir/token-floor-ask-events/#reservoirtoken_floor_ask_events","text":"This table contains records with information about each NFT token floor ask change. Query examples can be found here: https://dune.com/queries/1302852/2232169 https://dune.com/queries/1302854/2232173 Column name Type Description id bigint Internal token attribute id kind string Event type (new-order, expiry, sale, cancel, balance-change, approval-change, bootstrap, revalidation, reprice) contract string Contract address token_id string Id of the token in the collection order_id string Associated Ask id maker string Associated Ask maker wallet address price decimal Associated ask price (native currency) previous_price decimal Associated ask price (native currency) nonce string The order nonce of the maker valid_from bigint Associated ask validity start valid_until bigint Associated ask validity expiration source string Source of the order (e.g. opensea.io) tx_hash string Associated transaction hash tx_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the event was recorded","title":"reservoir.token_floor_ask_events"},{"location":"reference/tables/community/reservoir/tokens/","text":"tokens \u00b6 reservoir.tokens \u00b6 This table contains records with information about each NFT token. Query examples can be found here: https://dune.com/queries/1303052/2232521 https://dune.com/queries/1303064/2232571 Column name Type Description id string Internal token id contract string Contract address token_id string Id of the token in the collection name string NFT name description string NFT description collection_id string Associated Collection id owner string Owner wallet address floor_ask_id string Floor ask id floor_ask_value bigint Floor ask value floor_ask_maker string Floor ask maker wallet address floor_ask_valid_from bigint Floor ask Listing start time floor_ask_valid_to bigint Floor ask Listing end time floor_ask_source string Floor ask source (e.g. opensea.io) last_sale_value bigint Associated transaction timestamp last_sale_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the token was created updated_at timestamp Timestamp the token was updated","title":"tokens"},{"location":"reference/tables/community/reservoir/tokens/#tokens","text":"","title":"tokens"},{"location":"reference/tables/community/reservoir/tokens/#reservoirtokens","text":"This table contains records with information about each NFT token. Query examples can be found here: https://dune.com/queries/1303052/2232521 https://dune.com/queries/1303064/2232571 Column name Type Description id string Internal token id contract string Contract address token_id string Id of the token in the collection name string NFT name description string NFT description collection_id string Associated Collection id owner string Owner wallet address floor_ask_id string Floor ask id floor_ask_value bigint Floor ask value floor_ask_maker string Floor ask maker wallet address floor_ask_valid_from bigint Floor ask Listing start time floor_ask_valid_to bigint Floor ask Listing end time floor_ask_source string Floor ask source (e.g. opensea.io) last_sale_value bigint Associated transaction timestamp last_sale_timestamp bigint Associated transaction timestamp created_at timestamp Timestamp the token was created updated_at timestamp Timestamp the token was updated","title":"reservoir.tokens"},{"location":"reference/tables/decoded/","text":"Instead of working with the transactions, logs, and traces in their raw states, on Dune we decode smart contract activity into nice human-readable tables. We create tables for each event and function defined in the smart contract's ABI(Application Binary Interface). Subsequently, every event, message call or transaction made to that contract is decoded and inserted as a row into these tables. The tables are named accordingly: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) events: [projectname_blockchain].contractName_evt_eventName function calls: [projectname_blockchain].contractName_call_eventName As an example, decoded data for the swap -event of the Uniswap V2 pair contract on Ethereum is found in the table uniswap_v2_ethereum.Pair_evt_Swap . events: [projectname].\"contractName_evt_eventName\" function calls: [projectname].\"contractName_call_eventName\" As an example, decoded data for the swap -event of the uniswap V2 pair contract is found in the table uniswap_v2.\"Pair_evt_Swap\" . If a contract has multiple instances, we will decode all of them into the same table, you will be able to identify the specific smart contract using the contract_address column. Since all chain's data resides in one database, but the multichain world is a reality, contracts on Dune have a meta attribute that describes which blockchain this specific table is pulling the data from. Read more about the difference between calls and events here: Call tables Event logs Which contracts have decoded data? \u00b6 V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) You can check if contracts are already decoded by querying [blockchain].contracts tables through our database or use this dashboard . Select * from ethereum . contracts --you can change ethereum.contracts to the e.g. optimism.contracts where address = '0x429881672b9ae42b8eba0e26cd9c73711b891ca5' You can check if contracts are already decoded by querying \"[blockchain]\".contracts tables through our database or use this dashboard . Select * from ethereum . \"contracts\" --you can change ethereum.contracts to the e.g. optimism.contracts where address = '\\x429881672B9AE42b8EbA0E26cD9C73711b891Ca5' If the contract is not in our database yet, you can submit them here: dune.com/contracts/new . It usually takes about 24 hours to initially decode smart contracts, and you can check to see if your contract has been decoded yet here: Is my Contract decoded yet? Once a contract has been added to our Decoded Contracts system, you can check this dashboard to see the current delays between block published to decoded data ready for querying: Dune Meta Monitoring Read more about submitting contracts for decoding in this section: Adding new contracts How does decoding work? \u00b6 Smart Contracts on any EVM blockchain are mostly written in high level languages like Solidity or Vyper . In order for them to be able to be deployed to an EVM execution environment, they need to be compiled to EVM executable bytecode. Once deployed, the bytecode gets associated to an address on the respective chain and is permanently stored in this chain's state storage. To be able to interact with this smart contract, which is now just bytecode, we need a guide to be able to call the functions which are defined in the high-level languages. This translation of names and arguments into byte representation is done using an Application Binary Interface (ABI) . The ABI documents names, types, and arguments precisely which allows us to interact with the smart contract using a somewhat human readable format. The ABI can be compiled using the high level language source code. The ABI is used to call a smart contract or interpret the data it emits. An Example \u00b6 We are going to look at an event log of an ERC20 transfer event from the smart contract that represents the $PICKLE token. On Etherscan the undecoded event looks like this: If we query for this transaction in the ethereum.logs table in the dune database, we will receive the same encoded bytecode as our result dataset. V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Select * from ethereum . logs where tx_hash = '0x2bb7c8283b782355875fa37d05e4bd962519ea294678a3dcf2fdffbbd0761bc5' Result: Select * from ethereum . \"logs\" where tx_hash = '\\x2bb7c8283b782355875fa37d05e4bd962519ea294678a3dcf2fdffbbd0761bc5' Result: Now this is not at all helpful to analyze data. Using the contract's ABI we can convert this encoded bytecode to decoded data. The event log we are looking at here is from the $PICKLE ERC20 token transfer event log. Since this table is decoded on Dune, we can query the table in Dune to receive the decoded information: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) SELECT * FROM pickle_finance_ethereum . PickleToken_evt_Transfer WHERE evt_tx_hash = '0x2bb7c8283b782355875fa37d05e4bd962519ea294678a3dcf2fdffbbd0761bc5' Result: SELECT * FROM pickle_finance . \"PickleToken_evt_Transfer\" WHERE evt_tx_hash = '\\x2bb7c8283b782355875fa37d05e4bd962519ea294678a3dcf2fdffbbd0761bc5' Result: Now this is actually useful for analyzing this transaction! How exactly does this work? Since we know which event we are looking at here, we can simply convert the encoded bytecode to decoded data by decoding the bytecode according to it's datatype. The structure for the Transfer event log of an ERC20 token will always be: Transfer ( address from , address to , uint256 value ) This basically tells us that topic2 and topic3 are of the type address (32bytes) and are respectively the sender and recipient of the token transfer. An event log only has 3 indexed fields, so the data field is used to store the information about how much units of the token have been moved in this transaction. This field is called value . Since topic1 always is just the Keccak-256 hash of the signature of the event, we are left with decoding topic2 , topic3 and data . In this case, they map out like this: raw data field decoded data description raw data decoded data topic1 keccak256(\"Transfer(address,address,uint256)\") 0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef not needed, this table only contains event logs from the transfer event log topic2 from 0x00000000000000000000000075e89d5979e4f6fba9f97c104c2f0afb3f1dcb88 0x75e89d5979e4f6fba9f97c104c2f0afb3f1dcb88 topic3 to 0x00000000000000000000000087d9da48db6e1f925cb67d3b7d2a292846c24cf7 0x87d9da48db6e1f925cb67d3b7d2a292846c24cf7 data value 0x00000000000000000000000000000000000000000000001a894d51f85cb08000 489509000000000000000 In summary: We can use the contracts ABI to go from encoded bytecode to decoded data. This helps you run analysis fast and efficient as the decoded data is easy to work with. How do I understand decoded data? \u00b6 Decoded data is the high level programming language representation of two pieces of software talking to each other via the blockchain. It's not always easy for a human to understand what exactly is going on in these interactions, but most of the time, looking at column names and the data that is transmitted within them should help you to understand what is happening within that specific log or call. If you are not able to make sense of the data by just searching the tables, it usually helps to look at single transactions using the transaction hash and Etherscan. Furthermore, actually going into the smart contracts code (our favorite way to do this is DethCode ) to read the comments or the actual logic can help to understand the smart contract's emitted data. If that also doesn't lead to satisfactory results, scouring the relevant docs and GitHub of the project can lead you to the desired answers. Furthermore, talking to the developers and core community of a project can also help you to get an understanding of the smart contracts. In Summary : Working with decoded data allows you deep access to information stored on the blockchain and is very information rich, but understanding that data sometimes takes a bit of effort on your side since you are interacting with the data of the contract in a direct way. Which tables should I use? \u00b6 Events are designed to be analyzed and stored on the blockchain to allow backward looking analysis of what is happening, transactions and message calls are made to pass information between smart contracts. Therefore, in most cases the easiest and most accessible way to analyze various things happening on the blockchain is by looking at events. However, there is some cases where the emitted events miss some crucial information or there is just no events that get emitted. In these cases you might have to fall back to transaction and message calls (found in call tables). Cases where no event gets emitted get rarer over time as developers now mostly understand that events are important enough to be emitted, but they still exist. In some cases, it might make sense to combine the decoded data with raw data in order to get metadata about the transaction or dive even deeper. Queries to explore decoded Contracts \u00b6 See all projects we have decoded data for \u00b6 V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) SELECT DISTINCT namespace FROM [ blockchain ]. contracts ; --change [blockchain] the chain you're interested in e.g. ethereum.contracts Example: SELECT DISTINCT namespace FROM [ blockchain ]. \"contracts\" ; --change [blockchain] the chain you're interested in e.g. ethereum.contracts Example: Check for multiple instances of a contract \u00b6 If you are working with an event or call table directly you can see if there are several instances of that contract with this query. V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) SELECT DISTINCT contract_address FROM [ projectname_blockchain ].[ contractName ] _evt_ [ eventName ]; --change [projectname_blockchain] to the project name and blockchain you're interested in,[contractName] and [eventName] to the specific contract e.g. uniswap_v2_ethereum.Factory_evt_PairCreated Example: SELECT DISTINCT contract_address FROM [ projectname ]. \"[contractName]_evt_[eventName]\" ; --change [projectname] to the project name and blockchain you're interested in,[contractName] and [eventName] to the specific contract e.g. uniswap_v2.\"Factory_evt_PairCreated\" Example:","title":"Decoded Tables"},{"location":"reference/tables/decoded/#which-contracts-have-decoded-data","text":"V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) You can check if contracts are already decoded by querying [blockchain].contracts tables through our database or use this dashboard . Select * from ethereum . contracts --you can change ethereum.contracts to the e.g. optimism.contracts where address = '0x429881672b9ae42b8eba0e26cd9c73711b891ca5' You can check if contracts are already decoded by querying \"[blockchain]\".contracts tables through our database or use this dashboard . Select * from ethereum . \"contracts\" --you can change ethereum.contracts to the e.g. optimism.contracts where address = '\\x429881672B9AE42b8EbA0E26cD9C73711b891Ca5' If the contract is not in our database yet, you can submit them here: dune.com/contracts/new . It usually takes about 24 hours to initially decode smart contracts, and you can check to see if your contract has been decoded yet here: Is my Contract decoded yet? Once a contract has been added to our Decoded Contracts system, you can check this dashboard to see the current delays between block published to decoded data ready for querying: Dune Meta Monitoring Read more about submitting contracts for decoding in this section: Adding new contracts","title":"Which contracts have decoded data?"},{"location":"reference/tables/decoded/#how-does-decoding-work","text":"Smart Contracts on any EVM blockchain are mostly written in high level languages like Solidity or Vyper . In order for them to be able to be deployed to an EVM execution environment, they need to be compiled to EVM executable bytecode. Once deployed, the bytecode gets associated to an address on the respective chain and is permanently stored in this chain's state storage. To be able to interact with this smart contract, which is now just bytecode, we need a guide to be able to call the functions which are defined in the high-level languages. This translation of names and arguments into byte representation is done using an Application Binary Interface (ABI) . The ABI documents names, types, and arguments precisely which allows us to interact with the smart contract using a somewhat human readable format. The ABI can be compiled using the high level language source code. The ABI is used to call a smart contract or interpret the data it emits.","title":"How does decoding work?"},{"location":"reference/tables/decoded/#an-example","text":"We are going to look at an event log of an ERC20 transfer event from the smart contract that represents the $PICKLE token. On Etherscan the undecoded event looks like this: If we query for this transaction in the ethereum.logs table in the dune database, we will receive the same encoded bytecode as our result dataset. V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Select * from ethereum . logs where tx_hash = '0x2bb7c8283b782355875fa37d05e4bd962519ea294678a3dcf2fdffbbd0761bc5' Result: Select * from ethereum . \"logs\" where tx_hash = '\\x2bb7c8283b782355875fa37d05e4bd962519ea294678a3dcf2fdffbbd0761bc5' Result: Now this is not at all helpful to analyze data. Using the contract's ABI we can convert this encoded bytecode to decoded data. The event log we are looking at here is from the $PICKLE ERC20 token transfer event log. Since this table is decoded on Dune, we can query the table in Dune to receive the decoded information: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) SELECT * FROM pickle_finance_ethereum . PickleToken_evt_Transfer WHERE evt_tx_hash = '0x2bb7c8283b782355875fa37d05e4bd962519ea294678a3dcf2fdffbbd0761bc5' Result: SELECT * FROM pickle_finance . \"PickleToken_evt_Transfer\" WHERE evt_tx_hash = '\\x2bb7c8283b782355875fa37d05e4bd962519ea294678a3dcf2fdffbbd0761bc5' Result: Now this is actually useful for analyzing this transaction! How exactly does this work? Since we know which event we are looking at here, we can simply convert the encoded bytecode to decoded data by decoding the bytecode according to it's datatype. The structure for the Transfer event log of an ERC20 token will always be: Transfer ( address from , address to , uint256 value ) This basically tells us that topic2 and topic3 are of the type address (32bytes) and are respectively the sender and recipient of the token transfer. An event log only has 3 indexed fields, so the data field is used to store the information about how much units of the token have been moved in this transaction. This field is called value . Since topic1 always is just the Keccak-256 hash of the signature of the event, we are left with decoding topic2 , topic3 and data . In this case, they map out like this: raw data field decoded data description raw data decoded data topic1 keccak256(\"Transfer(address,address,uint256)\") 0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef not needed, this table only contains event logs from the transfer event log topic2 from 0x00000000000000000000000075e89d5979e4f6fba9f97c104c2f0afb3f1dcb88 0x75e89d5979e4f6fba9f97c104c2f0afb3f1dcb88 topic3 to 0x00000000000000000000000087d9da48db6e1f925cb67d3b7d2a292846c24cf7 0x87d9da48db6e1f925cb67d3b7d2a292846c24cf7 data value 0x00000000000000000000000000000000000000000000001a894d51f85cb08000 489509000000000000000 In summary: We can use the contracts ABI to go from encoded bytecode to decoded data. This helps you run analysis fast and efficient as the decoded data is easy to work with.","title":"An Example"},{"location":"reference/tables/decoded/#how-do-i-understand-decoded-data","text":"Decoded data is the high level programming language representation of two pieces of software talking to each other via the blockchain. It's not always easy for a human to understand what exactly is going on in these interactions, but most of the time, looking at column names and the data that is transmitted within them should help you to understand what is happening within that specific log or call. If you are not able to make sense of the data by just searching the tables, it usually helps to look at single transactions using the transaction hash and Etherscan. Furthermore, actually going into the smart contracts code (our favorite way to do this is DethCode ) to read the comments or the actual logic can help to understand the smart contract's emitted data. If that also doesn't lead to satisfactory results, scouring the relevant docs and GitHub of the project can lead you to the desired answers. Furthermore, talking to the developers and core community of a project can also help you to get an understanding of the smart contracts. In Summary : Working with decoded data allows you deep access to information stored on the blockchain and is very information rich, but understanding that data sometimes takes a bit of effort on your side since you are interacting with the data of the contract in a direct way.","title":"How do I understand decoded data?"},{"location":"reference/tables/decoded/#which-tables-should-i-use","text":"Events are designed to be analyzed and stored on the blockchain to allow backward looking analysis of what is happening, transactions and message calls are made to pass information between smart contracts. Therefore, in most cases the easiest and most accessible way to analyze various things happening on the blockchain is by looking at events. However, there is some cases where the emitted events miss some crucial information or there is just no events that get emitted. In these cases you might have to fall back to transaction and message calls (found in call tables). Cases where no event gets emitted get rarer over time as developers now mostly understand that events are important enough to be emitted, but they still exist. In some cases, it might make sense to combine the decoded data with raw data in order to get metadata about the transaction or dive even deeper.","title":"Which tables should I use?"},{"location":"reference/tables/decoded/#queries-to-explore-decoded-contracts","text":"","title":"Queries to explore decoded Contracts"},{"location":"reference/tables/decoded/#see-all-projects-we-have-decoded-data-for","text":"V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) SELECT DISTINCT namespace FROM [ blockchain ]. contracts ; --change [blockchain] the chain you're interested in e.g. ethereum.contracts Example: SELECT DISTINCT namespace FROM [ blockchain ]. \"contracts\" ; --change [blockchain] the chain you're interested in e.g. ethereum.contracts Example:","title":"See all projects we have decoded data for"},{"location":"reference/tables/decoded/#check-for-multiple-instances-of-a-contract","text":"If you are working with an event or call table directly you can see if there are several instances of that contract with this query. V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) SELECT DISTINCT contract_address FROM [ projectname_blockchain ].[ contractName ] _evt_ [ eventName ]; --change [projectname_blockchain] to the project name and blockchain you're interested in,[contractName] and [eventName] to the specific contract e.g. uniswap_v2_ethereum.Factory_evt_PairCreated Example: SELECT DISTINCT contract_address FROM [ projectname ]. \"[contractName]_evt_[eventName]\" ; --change [projectname] to the project name and blockchain you're interested in,[contractName] and [eventName] to the specific contract e.g. uniswap_v2.\"Factory_evt_PairCreated\" Example:","title":"Check for multiple instances of a contract"},{"location":"reference/tables/decoded/call-tables/","text":"Smart contracts generally have functions that are able to be called by either an externally owned account(EOA) or other smart contracts. Functions can be anything from a simple state read and return to changing multiple states and invoking message calls to other smart contracts. On Dune, we parse all message calls and transactions made to smart contracts in their own tables. The tables are then accordingly named: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) [projectname_blockchain].contractName_call_functionName [projectname].\"contractName_call_functionName\" This is either done on an individual contract level like for the uniswap v3 factory, or a class of contracts like the uniswap v3 pairs. For example, when a uniswap v3 pool gets created via the uniswap v3 factory (on Ethereum) function createPool , Dune will record that transaction in the table: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) uniswap_v3_ethereum.Factory_call_createPool [ uniswap_v3.\"Factory_call_createPool\" ] This will happen whether this was done by an externally owned account (EOA) through a transaction or a smart contract by the means of a message call. Multiple Instances \u00b6 For a contract where multiple instances exist, we will decode all calls to all instances of this smart contract into one table. If there is a transaction calling the swap function of any instance of a Uniswap v3 pair contract, we will collect this data in the table: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) uniswap_v3_ethereum.Pair_call_swap uniswap_v3.\"Pair_call_swap\" Common misconceptions \u00b6 One thing to keep in mind here is that web3.js , web3.py and all other methods of (locally) calling a pure , read , or constant function do not broadcast or publish anything on the blockchain and are therefore not recorded in Dune. However, if one of these functions is invoked by another smart contract in the context of a transaction, this will be broadcast on the chain and therefore accessible in Dune. In short: State data stored in the memory of a smart contract is not available on Dune! A good example of this is the function decimals of the erc20 token contract Uni which is a constant state variable that is able to be accessed through an automatically created \" getter function \". Should a smart contract invoke this function in the context of transaction, this message call will be recorded in the Dune table uniswap.\"UNI_call_decimals\" . This is in contrast to anyone calling this function locally using web3.py/web3.js or using the Etherscan frontend to access this state. These local calls are not recorded in Dune. Further Reading \u00b6 What is the difference between a transaction and a call? Soliditylang.org documentation","title":"Call Tables"},{"location":"reference/tables/decoded/call-tables/#multiple-instances","text":"For a contract where multiple instances exist, we will decode all calls to all instances of this smart contract into one table. If there is a transaction calling the swap function of any instance of a Uniswap v3 pair contract, we will collect this data in the table: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) uniswap_v3_ethereum.Pair_call_swap uniswap_v3.\"Pair_call_swap\"","title":"Multiple Instances"},{"location":"reference/tables/decoded/call-tables/#common-misconceptions","text":"One thing to keep in mind here is that web3.js , web3.py and all other methods of (locally) calling a pure , read , or constant function do not broadcast or publish anything on the blockchain and are therefore not recorded in Dune. However, if one of these functions is invoked by another smart contract in the context of a transaction, this will be broadcast on the chain and therefore accessible in Dune. In short: State data stored in the memory of a smart contract is not available on Dune! A good example of this is the function decimals of the erc20 token contract Uni which is a constant state variable that is able to be accessed through an automatically created \" getter function \". Should a smart contract invoke this function in the context of transaction, this message call will be recorded in the Dune table uniswap.\"UNI_call_decimals\" . This is in contrast to anyone calling this function locally using web3.py/web3.js or using the Etherscan frontend to access this state. These local calls are not recorded in Dune.","title":"Common misconceptions"},{"location":"reference/tables/decoded/call-tables/#further-reading","text":"What is the difference between a transaction and a call? Soliditylang.org documentation","title":"Further Reading"},{"location":"reference/tables/decoded/event-logs/","text":"Smart Contracts emit event logs when certain predefined actions are completed. The structure published in these logs is predefined by the developer of the smart contract, the content is dynamically created during the transaction. Logs are useful for monitoring, alerting and in general keeping track of what happens inside of a smart contract. Logs are your best friend as a data analyst since they reliably present you with data that is intended to be analyzed post factum. If you ever want to see which logs can be emitted by a smart contract, you can simply search for the keyword emit in the source code of the smart contract. We will decode all event logs for smart contracts into tables named accordingly to this schema: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) [projectname_blockchain].[contractName]_evt_[eventName] [projectname].\"[contractName]_evt_[eventName]\" Let's stay in the context of the uniswap v3 factory and look at the event that gets emitted upon the creation of a new pool. The event is called PoolCreated and gets emitted every time somebody successfully deployed a new Uniswap V3 pool by calling the function createPool . The event will readily give us information like the tokens in the pool, the fee tier of this pool and the tick spacing. In Etherscan, you can easily look at the event logs of transaction by opening the logs tab . In Dune, this particular event will be stored in the table: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) uniswap_v3_ethereum.Factory_evt_PoolCreated uniswap_v3.\"Factory_evt_PoolCreated\" Multiple Instances \u00b6 If there is multiple instances of a contract we will collect all event logs across all instances of this smart contract in one table. For example, all uniswap v3 pool swap events (on ethereum) are stored in the table: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) uniswap_v3_ethereum.Pair_evt_Swap uniswap_v3.\"Pair_evt_Swap\" The column contract_address indicates as to which smart contract emitted this event. Further Reading \u00b6 Understanding event logs on the Ethereum blockchain Everything You Ever Wanted to Know About Events and Logs on Ethereum","title":"Event Logs"},{"location":"reference/tables/decoded/event-logs/#multiple-instances","text":"If there is multiple instances of a contract we will collect all event logs across all instances of this smart contract in one table. For example, all uniswap v3 pool swap events (on ethereum) are stored in the table: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) uniswap_v3_ethereum.Pair_evt_Swap uniswap_v3.\"Pair_evt_Swap\" The column contract_address indicates as to which smart contract emitted this event.","title":"Multiple Instances"},{"location":"reference/tables/decoded/event-logs/#further-reading","text":"Understanding event logs on the Ethereum blockchain Everything You Ever Wanted to Know About Events and Logs on Ethereum","title":"Further Reading"},{"location":"reference/tables/raw/","text":"Raw tables provide you raw, unfiltered and unedited data. This allows you to query for any transaction, block, event log or trace across the blockchains Dune supports. Raw data tables are very useful to get meta information about the blockchain, a transaction, traces or certain events. Additionally, with a few tricks and a few tricks, you can actually gain substantial insights into systems of smart contracts using the encoded data. Alex Kroeger wrote a great article about this exact topic. We have a several SQL functions in our database that allow you to more easily work with encoded data. However, queries that have been written using raw data tables are notoriously hard to understand and audit due to the nature of the the encoded data commonly found in these tables. Furthermore, the raw data tables have a very large number of rows and hence can be slow to query. Most of the time you are better off submitting contracts for decoding and working with decoded data . Note EVM chains at large follow the same execution model, however there is sometimes is differences in the consensus algorithm, the gas costs or even the calculation of gas costs. Solana has an entirely different methodology and thus it's data is entirely different than our EVM based data tables. Available Data \u00b6 Blocks : Blocks are the building blocks of blockchains and rollups. Event Logs : Event Logs are data that gets generated by smart contracts. Traces : Traces contain information about the execution of smaller atomic actions generated by transactions. Transactions : Transactions are cryptographically signed instructions from accounts.","title":"Raw Tables"},{"location":"reference/tables/raw/#available-data","text":"Blocks : Blocks are the building blocks of blockchains and rollups. Event Logs : Event Logs are data that gets generated by smart contracts. Traces : Traces contain information about the execution of smaller atomic actions generated by transactions. Transactions : Transactions are cryptographically signed instructions from accounts.","title":"Available Data"},{"location":"reference/tables/raw/blocks/","text":"Blocks are the building blocks of blockchains and rollups. A block contains transactions which will alter the state of an EVM system incrementally. Transaction within a block can only be executed one after the other, not in parallel. These tables are useful for identifying block activity and transaction changes over time. Tables \u00b6 V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Chain Table Notes Ethereum Mainnet ethereum.blocks Gnosis Chain gnosis.blocks Does not contain nonce Polygon polygon.blocks Optimism optimism.blocks Does not contain miner nonce base_fee_per_gas Optimism (legacy) optimism_legacy_ovm1.blocks Does not contain miner nonce base_fee_per_gas BNB Chain bnb.blocks Does not contain base_fee_per_gas Solana solana.blocks Find details here Arbitrum arbitrum.blocks Does not contain miner difficulty total_difficulty nonce size base_fee_per_gas Avalanche C-Chain avalanche_c.blocks Does not contain miner difficulty Chain Table Notes Ethereum Mainnet ethereum.blocks Gnosis Chain (xDai) xdai.blocks Does not contain nonce Polygon polygon.blocks Optimism (OVM 1 & 2) optimism.blocks Does not contain miner nonce base_fee_per_gas BNB Chain (BSC) bsc.blocks Does not contain base_fee_per_gas Column Data \u00b6 Example \u00b6 Description \u00b6 Column name Data type Description time timestamptz The time when the block was mined number numeric The length of the blockchain in blocks hash bytea A unique identifier for that block parent hash bytea The unique identifier for the prior block gas_limit numeric The gas limit of the current block gas_used numeric The gas used in this block miner bytea The address of the miner difficulty numeric The effort required to mine the block total_difficulty numeric Total difficulty of the chain until this block nonce bytea The block nonce is used to demonstrate the proof of work during mining size numeric This block's size in bytes (limited by gas limit) base_fee_per_gas numeric This block's base fee (introduced by EIP1559 )","title":"Blocks"},{"location":"reference/tables/raw/blocks/#tables","text":"V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Chain Table Notes Ethereum Mainnet ethereum.blocks Gnosis Chain gnosis.blocks Does not contain nonce Polygon polygon.blocks Optimism optimism.blocks Does not contain miner nonce base_fee_per_gas Optimism (legacy) optimism_legacy_ovm1.blocks Does not contain miner nonce base_fee_per_gas BNB Chain bnb.blocks Does not contain base_fee_per_gas Solana solana.blocks Find details here Arbitrum arbitrum.blocks Does not contain miner difficulty total_difficulty nonce size base_fee_per_gas Avalanche C-Chain avalanche_c.blocks Does not contain miner difficulty Chain Table Notes Ethereum Mainnet ethereum.blocks Gnosis Chain (xDai) xdai.blocks Does not contain nonce Polygon polygon.blocks Optimism (OVM 1 & 2) optimism.blocks Does not contain miner nonce base_fee_per_gas BNB Chain (BSC) bsc.blocks Does not contain base_fee_per_gas","title":"Tables"},{"location":"reference/tables/raw/blocks/#column-data","text":"","title":"Column Data"},{"location":"reference/tables/raw/blocks/#example","text":"","title":"Example"},{"location":"reference/tables/raw/blocks/#description","text":"Column name Data type Description time timestamptz The time when the block was mined number numeric The length of the blockchain in blocks hash bytea A unique identifier for that block parent hash bytea The unique identifier for the prior block gas_limit numeric The gas limit of the current block gas_used numeric The gas used in this block miner bytea The address of the miner difficulty numeric The effort required to mine the block total_difficulty numeric Total difficulty of the chain until this block nonce bytea The block nonce is used to demonstrate the proof of work during mining size numeric This block's size in bytes (limited by gas limit) base_fee_per_gas numeric This block's base fee (introduced by EIP1559 )","title":"Description"},{"location":"reference/tables/raw/event-logs/","text":"Event Logs tables store all logs data that gets generated by smart contracts. This can be useful for querying contracts that are not yet decoded or are not able to be decoded since the code of the smart contract is not public. Logs are an elegant way to store tiny amounts of data on EVM blockchains for a small amount of gas. Specifically, event logs are useful to let other people know something has happened without them having to query contracts individually. For more on this topic read this article . Note Our topic index counts from 1, so topic0 shows up as topic1 , topic1 shows up as topic2 and so on. Tables \u00b6 V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Chain Table Notes Ethereum Mainnet ethereum.logs Gnosis Chain gnosis.logs Polygon polygon.logs Optimism optimism.logs Optimism (legacy) optimism_legacy_ovm1.logs BNB Chain bnb.logs Solana solana.logs Arbitrum arbitrum.logs Avalanche C-Chain avalanche_c.logs Chain Table Notes Ethereum Mainnet ethereum.logs Gnosis Chain (xDai) xdai.logs Polygon polygon.logs Optimism (OVM 1 & 2) optimism.logs BNB Chain (BSC) bsc.logs Column Data \u00b6 Example \u00b6 Description \u00b6 Column name Data type Description contract_address bytea The address of the contract that emitted the log topic1 bytea keccak256 hash of a flattened event declaration string topic2 bytea Second indexed topic of the event topic3 bytea Third indexed topic of the event topic4 bytea Fourth indexed topic of the event data bytea Unindexed data containing further information on the event tx_hash bytea The transaction hash of the transaction that produced this log block_hash bytea A unique identifier for that block block_number int8 The length of the blockchain in blocks block_time timestamptz The time when the block was mined that includes this log index numeric This logs index position in the block (cumulative amount of logs ordered by execution) tx_index numeric The index position of the transaction in this block (cumulative amount of transactions ordered by execution)","title":"Event Logs"},{"location":"reference/tables/raw/event-logs/#tables","text":"V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Chain Table Notes Ethereum Mainnet ethereum.logs Gnosis Chain gnosis.logs Polygon polygon.logs Optimism optimism.logs Optimism (legacy) optimism_legacy_ovm1.logs BNB Chain bnb.logs Solana solana.logs Arbitrum arbitrum.logs Avalanche C-Chain avalanche_c.logs Chain Table Notes Ethereum Mainnet ethereum.logs Gnosis Chain (xDai) xdai.logs Polygon polygon.logs Optimism (OVM 1 & 2) optimism.logs BNB Chain (BSC) bsc.logs","title":"Tables"},{"location":"reference/tables/raw/event-logs/#column-data","text":"","title":"Column Data"},{"location":"reference/tables/raw/event-logs/#example","text":"","title":"Example"},{"location":"reference/tables/raw/event-logs/#description","text":"Column name Data type Description contract_address bytea The address of the contract that emitted the log topic1 bytea keccak256 hash of a flattened event declaration string topic2 bytea Second indexed topic of the event topic3 bytea Third indexed topic of the event topic4 bytea Fourth indexed topic of the event data bytea Unindexed data containing further information on the event tx_hash bytea The transaction hash of the transaction that produced this log block_hash bytea A unique identifier for that block block_number int8 The length of the blockchain in blocks block_time timestamptz The time when the block was mined that includes this log index numeric This logs index position in the block (cumulative amount of logs ordered by execution) tx_index numeric The index position of the transaction in this block (cumulative amount of transactions ordered by execution)","title":"Description"},{"location":"reference/tables/raw/traces/","text":"Transactions can trigger smaller atomic actions that modify the internal state of an Ethereum Virtual Machine. Information about the execution of these actions is logged and can be found stored as an EVM execution trace, or just a trace . In Etherscan these are referred to as \"internal transactions\". Read more here . Tables \u00b6 V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Chain Table Notes Ethereum Mainnet ethereum.traces value measured in wei Gnosis Chain gnosis.traces value measured in wei Polygon polygon.traces value measured in wei , does not contain gas Optimism optimism.traces value measured in wei Optimism (legacy) optimism_legacy_ovm1.traces value measured in wei BNB Chain bnb.traces value measured in wei Solana solana.traces value measured in wei Arbitrum arbitrum.traces value measured in ArbGas , does not contain gas Avalanche C-Chain avalanche_c.traces value measured in nanoavax Chain Table Notes Ethereum Mainnet ethereum.traces value measured in wei Gnosis Chain (xDai) xdai.traces value measured in wei Polygon polygon.traces value measured in wei Optimism (OVM 1 & 2) optimism.traces value measured in wei BNB Chain (BSC) bsc.traces value measured in wei Column Data \u00b6 Example \u00b6 Description \u00b6 Column name Data type Description block_time timestamptz The time when the block was mined block_number int8 The length of the blockchain in blocks value numeric The amount of [chain_gas_token] sent in this transaction gas numeric Gas provided with the message call gas_used numeric The gas consumed by the transaction in wei block_hash bytea A unique identifier for that block success boolean A true/false value that shows if the trace action succeeded tx_index numeric The position of the transaction in a block sub_traces numeric Number of children of a trace error text The error message the EVM throws if the execution of one of a contract's instructions fails. See a list of unique Ethereum Errors this past week here . tx_success boolean A true/false value that indicates if the transaction succeeded tx_hash bytea The transaction hash of the event from bytea Address of the sender to bytea Address of the receiver. null when its a contract creation transaction trace_address array Address of the trace within the call graph forest. E.g., [0, 2, 1] is the parent of [0, 2, 1, 0] type text Can be reward , create , call or suicide . Describes the type of action taken in this trace. address bytea The contract that is called when the type is suicide or create code bytea The bytecode to deploy a new contract, only contains data when type is create . call_type bytea Can be staticcall , delegatecall or call . Learn more here input bytea The bytecode of the call that is made to another smart contract output bytea The bytecode answer the smart contract that was called gives back refund_address bytea Only contains data if type was suicide . Specifies where to send the outstanding BNB balance. Gas used in .traces \u00b6 The gas_used column in the .traces tables is a bit hard to understand, so here is some pointers: The gas_used of a trace will always include the gas consumed by the trace and all it's subtraces. The gas_used of the initial call will not contain the cost of making the call in the first place You need to add 21000 gas units + the cost of sending zero + non zero bytes to the gas_used value of the top trace to arrive at the \"true\" gas_used value. For more reading on this please refer to this StackExchange entry Here's an example query doing this in Dune Creation Traces \u00b6 Tables \u00b6 Chain Table Notes Ethereum Mainnet ethereum.creation_traces Gnosis Chain gnosis.creation_traces Polygon polygon.creation_traces Optimism optimism.creation_traces Optimism (legacy) optimism_legacy_ovm1.creation_traces BNB Chain bnb.creation_traces Solana solana.creation_traces Arbitrum arbitrum.creation_traces Avalanche C-Chain avalanche_c.creation_traces Example \u00b6 Description \u00b6 Transactions can trigger smaller atomic actions that modify the internal state of an Ethereum Virtual Machine. One type of trace, create , is used to create a smart contract then transfer ether to it. Read more here . Column name Data type Description block_time timestamptz The time when the block was mined block_number long The length of the blockchain in blocks tx_hash string The transaction hash of the event address string The address of the created contract from string Address of the contract that generated the create trace code string The function executed","title":"Traces"},{"location":"reference/tables/raw/traces/#tables","text":"V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Chain Table Notes Ethereum Mainnet ethereum.traces value measured in wei Gnosis Chain gnosis.traces value measured in wei Polygon polygon.traces value measured in wei , does not contain gas Optimism optimism.traces value measured in wei Optimism (legacy) optimism_legacy_ovm1.traces value measured in wei BNB Chain bnb.traces value measured in wei Solana solana.traces value measured in wei Arbitrum arbitrum.traces value measured in ArbGas , does not contain gas Avalanche C-Chain avalanche_c.traces value measured in nanoavax Chain Table Notes Ethereum Mainnet ethereum.traces value measured in wei Gnosis Chain (xDai) xdai.traces value measured in wei Polygon polygon.traces value measured in wei Optimism (OVM 1 & 2) optimism.traces value measured in wei BNB Chain (BSC) bsc.traces value measured in wei","title":"Tables"},{"location":"reference/tables/raw/traces/#column-data","text":"","title":"Column Data"},{"location":"reference/tables/raw/traces/#example","text":"","title":"Example"},{"location":"reference/tables/raw/traces/#description","text":"Column name Data type Description block_time timestamptz The time when the block was mined block_number int8 The length of the blockchain in blocks value numeric The amount of [chain_gas_token] sent in this transaction gas numeric Gas provided with the message call gas_used numeric The gas consumed by the transaction in wei block_hash bytea A unique identifier for that block success boolean A true/false value that shows if the trace action succeeded tx_index numeric The position of the transaction in a block sub_traces numeric Number of children of a trace error text The error message the EVM throws if the execution of one of a contract's instructions fails. See a list of unique Ethereum Errors this past week here . tx_success boolean A true/false value that indicates if the transaction succeeded tx_hash bytea The transaction hash of the event from bytea Address of the sender to bytea Address of the receiver. null when its a contract creation transaction trace_address array Address of the trace within the call graph forest. E.g., [0, 2, 1] is the parent of [0, 2, 1, 0] type text Can be reward , create , call or suicide . Describes the type of action taken in this trace. address bytea The contract that is called when the type is suicide or create code bytea The bytecode to deploy a new contract, only contains data when type is create . call_type bytea Can be staticcall , delegatecall or call . Learn more here input bytea The bytecode of the call that is made to another smart contract output bytea The bytecode answer the smart contract that was called gives back refund_address bytea Only contains data if type was suicide . Specifies where to send the outstanding BNB balance.","title":"Description"},{"location":"reference/tables/raw/traces/#gas-used-in-traces","text":"The gas_used column in the .traces tables is a bit hard to understand, so here is some pointers: The gas_used of a trace will always include the gas consumed by the trace and all it's subtraces. The gas_used of the initial call will not contain the cost of making the call in the first place You need to add 21000 gas units + the cost of sending zero + non zero bytes to the gas_used value of the top trace to arrive at the \"true\" gas_used value. For more reading on this please refer to this StackExchange entry Here's an example query doing this in Dune","title":"Gas used in .traces"},{"location":"reference/tables/raw/traces/#creation-traces","text":"","title":"Creation Traces"},{"location":"reference/tables/raw/traces/#tables_1","text":"Chain Table Notes Ethereum Mainnet ethereum.creation_traces Gnosis Chain gnosis.creation_traces Polygon polygon.creation_traces Optimism optimism.creation_traces Optimism (legacy) optimism_legacy_ovm1.creation_traces BNB Chain bnb.creation_traces Solana solana.creation_traces Arbitrum arbitrum.creation_traces Avalanche C-Chain avalanche_c.creation_traces","title":"Tables"},{"location":"reference/tables/raw/traces/#example_1","text":"","title":"Example"},{"location":"reference/tables/raw/traces/#description_1","text":"Transactions can trigger smaller atomic actions that modify the internal state of an Ethereum Virtual Machine. One type of trace, create , is used to create a smart contract then transfer ether to it. Read more here . Column name Data type Description block_time timestamptz The time when the block was mined block_number long The length of the blockchain in blocks tx_hash string The transaction hash of the event address string The address of the created contract from string Address of the contract that generated the create trace code string The function executed","title":"Description"},{"location":"reference/tables/raw/transactions/","text":"Transactions are cryptographically signed instructions from accounts. An account will initiate a transaction to update the state of the Ethereum network. Transactions will always originate from externally owned accounts, a smart contract can not initiate a transaction. Transactions need to be broadcast to the whole network. Any node can broadcast a request for a transaction to be executed on the EVM; after this happens, a miner will execute the transaction and propagate the resulting state change to the rest of the network. Read more in the official Ethereum documentation here . Tables \u00b6 V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Chain Table Notes Ethereum Mainnet ethereum.transactions Gnosis Chain gnosis.transactions Polygon polygon.transactions Optimism optimism.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy Optimism (legacy) optimism_legacy_ovm1.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy BNB Chain bnb.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy Solana solana.transactions Arbitrum arbitrum.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy . Gas is measured in ArbGas instead of wei Avalanche C-Chain avalanche_c.transactions Does not contain. Gas is measured in nanoavax instead of wei Chain Table Notes Ethereum Mainnet ethereum.transactions Gnosis Chain (xDai) xdai.transactions Polygon polygon.transactions Optimism (OVM 1 & 2) optimism.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy BNB Chain (BSC) bsc.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy Column Data \u00b6 Example \u00b6 Description \u00b6 Column ** Data type Description block_time timestamptz The time when the block was mined that includes this transaction block_number int8 The length of the blockchain in blocks value numeric The amount of [chain_gas_token] sent in this transaction in wei . Note that ERC20 tokens do not show up here gas_limit numeric The gas limit in wei (ArbGas for Arbitrum) gas_price numeric The gas price in wei gas_used numeric The gas consumed by the transaction in wei max_fee_per_gas numeric The maximum fee per gas the transaction sender is willing to pay total (introduced by EIP1559 ) max_priority_fee_per_gas numeric Maximum fee per gas the transaction sender is willing to give to miners to incentivize them to include their transaction (introduced by EIP1559 ) priority_fee_per_gas numeric The priority fee paid out to the miner for this transaction (introduced by EIP1559 ) nonce numeric The transaction nonce, unique to that wallet index numeric The transactions index position in the block success boolean A true/false value that shows if the transaction succeeded from bytea Address of the sender to bytea Address of the receiver. null when its a contract creation transaction block_hash bytea A unique identifier for that block data bytea Can either be empty, a hex encoded message or instructions for a smart contract call hash bytea The hash of the transaction type text The type of the transaction: Legacy , AccessList , or DynamicFee access_list jsonb A list of addresses and storage keys the transaction intends to access. See EIP2930 . Applicable if the transaction is of type AccessList or DynamicFee effective_gas_price numeric [Arbitrum and Avalanche C-Chain only] The gas price this transaction paid in wei (Arbitrum) or nanoavax (Avalanche) gas_used_for_l1 numeric [Arbitrum only] The gas consumed by the L1 resources used for this transaction in ArbGas l1_gas_used numeric [Optimism only] The costs to send the input calldata to L1 l1_gas_price numeric [Optimism only] The gas price on L1 l1_fee numeric [Optimism only] The amount in wei paid on L1 l1_fee_scalar numeric [Optimism only] Variable parameter that makes sure that gas costs on L1 get covered + profits l1_block_number numeric [Optimism only] The block_number of the block in which this transaction got batch settled on L1 l1_timestamp numeric [Optimism only] The timestamp of the block in which this transaction got batch settled on L1 l1_tx_origin numeric [Optimism only] ??","title":"Transactions"},{"location":"reference/tables/raw/transactions/#tables","text":"V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) Chain Table Notes Ethereum Mainnet ethereum.transactions Gnosis Chain gnosis.transactions Polygon polygon.transactions Optimism optimism.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy Optimism (legacy) optimism_legacy_ovm1.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy BNB Chain bnb.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy Solana solana.transactions Arbitrum arbitrum.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy . Gas is measured in ArbGas instead of wei Avalanche C-Chain avalanche_c.transactions Does not contain. Gas is measured in nanoavax instead of wei Chain Table Notes Ethereum Mainnet ethereum.transactions Gnosis Chain (xDai) xdai.transactions Polygon polygon.transactions Optimism (OVM 1 & 2) optimism.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy BNB Chain (BSC) bsc.transactions No EIP1559 so does not contain access_list , max_fee_per_gas , max_priority_fee_per_gas , priority_fee_per_gas and type is always Legacy","title":"Tables"},{"location":"reference/tables/raw/transactions/#column-data","text":"","title":"Column Data"},{"location":"reference/tables/raw/transactions/#example","text":"","title":"Example"},{"location":"reference/tables/raw/transactions/#description","text":"Column ** Data type Description block_time timestamptz The time when the block was mined that includes this transaction block_number int8 The length of the blockchain in blocks value numeric The amount of [chain_gas_token] sent in this transaction in wei . Note that ERC20 tokens do not show up here gas_limit numeric The gas limit in wei (ArbGas for Arbitrum) gas_price numeric The gas price in wei gas_used numeric The gas consumed by the transaction in wei max_fee_per_gas numeric The maximum fee per gas the transaction sender is willing to pay total (introduced by EIP1559 ) max_priority_fee_per_gas numeric Maximum fee per gas the transaction sender is willing to give to miners to incentivize them to include their transaction (introduced by EIP1559 ) priority_fee_per_gas numeric The priority fee paid out to the miner for this transaction (introduced by EIP1559 ) nonce numeric The transaction nonce, unique to that wallet index numeric The transactions index position in the block success boolean A true/false value that shows if the transaction succeeded from bytea Address of the sender to bytea Address of the receiver. null when its a contract creation transaction block_hash bytea A unique identifier for that block data bytea Can either be empty, a hex encoded message or instructions for a smart contract call hash bytea The hash of the transaction type text The type of the transaction: Legacy , AccessList , or DynamicFee access_list jsonb A list of addresses and storage keys the transaction intends to access. See EIP2930 . Applicable if the transaction is of type AccessList or DynamicFee effective_gas_price numeric [Arbitrum and Avalanche C-Chain only] The gas price this transaction paid in wei (Arbitrum) or nanoavax (Avalanche) gas_used_for_l1 numeric [Arbitrum only] The gas consumed by the L1 resources used for this transaction in ArbGas l1_gas_used numeric [Optimism only] The costs to send the input calldata to L1 l1_gas_price numeric [Optimism only] The gas price on L1 l1_fee numeric [Optimism only] The amount in wei paid on L1 l1_fee_scalar numeric [Optimism only] Variable parameter that makes sure that gas costs on L1 get covered + profits l1_block_number numeric [Optimism only] The block_number of the block in which this transaction got batch settled on L1 l1_timestamp numeric [Optimism only] The timestamp of the block in which this transaction got batch settled on L1 l1_tx_origin numeric [Optimism only] ??","title":"Description"},{"location":"reference/tables/raw/solana/","text":"As a non-EVM chain, Solana Raw data looks quite different from other chains. Data Available \u00b6 Account Activity : This table contains information from the transactions table focused on account usage. Blocks : Blocks are the building blocks of blockchains and rollups. Rewards : This table contains data about rewards paid out on Solana. Transactions : Transactions are cryptographically signed instructions from accounts. Vote Transactions : This table contains the full set of vote transactions that are submitted by validators to vote on a block. Changelog \u00b6 2022-03-25 \u00b6 the solana.account_activity table has been updated to a new version. The new version of the table contains additional information around token activity. The following columns were added to the table: pre_token_balances The token balance before the transaction was processed post_token_balances The token balance after the transaction was processed token_balance_changes The balance change that occurred as part of the transaction 2022-03-18 \u00b6 Released the solana.account_activity table that contains all of the information about an account\u2019s usage in a transaction. The table is optimized to run with \u2018WHERE address = \u2026\u2019 queries 2022-03-01 \u00b6 The solana.transactions table has now been upgraded to a new version. The new version of the table uses cleaner array structs to make it easier to extract useful information. The vote transactions have also been split into their own table solana.vote_transactions , so queries using solana.transactions will have better performance. Unfortunately, the table change also means that some existing queries will now break and need to be changed. What this means for your existing queries using solana.transactions : You won't need to check if a transaction is a vote transaction, which has typically been done with WHERE ARRAY_CONTAINS(account_keys, \"Vote111111111111111111111111111111111111111\") = false The error_index and error_message columns have been removed, and have been merged into the error column (which is a struct). So now instead of WHERE error_index is not null , a query should do WHERE error is not null . Structs containing indexes to account_keys now include the account address directly, so there is no need to use the account_keys column to look up the account addresses: before -> now account_keys[instructions[i]['program_id_index']] -> instructions[i].executing_account account_keys[pre_token_balances[i]['account_index']] -> pre_token_balances[i].account account_keys[post_token_balances[i]['account_index']] -> post_token_balances[i].account The pre_token_balances and post_token_balances columns have changed. The token balance is now included in the field amount . And as mentioned above, the struct in the array now has a field account , which is the account of the token balance. The instructions column has changed. As mentioned above, the struct in the array now has a field executing_account , which is the account executing the instruction. The inner_instructions column is removed, and inner instructions have been moved into the instructions column.","title":"Solana"},{"location":"reference/tables/raw/solana/#data-available","text":"Account Activity : This table contains information from the transactions table focused on account usage. Blocks : Blocks are the building blocks of blockchains and rollups. Rewards : This table contains data about rewards paid out on Solana. Transactions : Transactions are cryptographically signed instructions from accounts. Vote Transactions : This table contains the full set of vote transactions that are submitted by validators to vote on a block.","title":"Data Available"},{"location":"reference/tables/raw/solana/#changelog","text":"","title":"Changelog"},{"location":"reference/tables/raw/solana/#2022-03-25","text":"the solana.account_activity table has been updated to a new version. The new version of the table contains additional information around token activity. The following columns were added to the table: pre_token_balances The token balance before the transaction was processed post_token_balances The token balance after the transaction was processed token_balance_changes The balance change that occurred as part of the transaction","title":"2022-03-25"},{"location":"reference/tables/raw/solana/#2022-03-18","text":"Released the solana.account_activity table that contains all of the information about an account\u2019s usage in a transaction. The table is optimized to run with \u2018WHERE address = \u2026\u2019 queries","title":"2022-03-18"},{"location":"reference/tables/raw/solana/#2022-03-01","text":"The solana.transactions table has now been upgraded to a new version. The new version of the table uses cleaner array structs to make it easier to extract useful information. The vote transactions have also been split into their own table solana.vote_transactions , so queries using solana.transactions will have better performance. Unfortunately, the table change also means that some existing queries will now break and need to be changed. What this means for your existing queries using solana.transactions : You won't need to check if a transaction is a vote transaction, which has typically been done with WHERE ARRAY_CONTAINS(account_keys, \"Vote111111111111111111111111111111111111111\") = false The error_index and error_message columns have been removed, and have been merged into the error column (which is a struct). So now instead of WHERE error_index is not null , a query should do WHERE error is not null . Structs containing indexes to account_keys now include the account address directly, so there is no need to use the account_keys column to look up the account addresses: before -> now account_keys[instructions[i]['program_id_index']] -> instructions[i].executing_account account_keys[pre_token_balances[i]['account_index']] -> pre_token_balances[i].account account_keys[post_token_balances[i]['account_index']] -> post_token_balances[i].account The pre_token_balances and post_token_balances columns have changed. The token balance is now included in the field amount . And as mentioned above, the struct in the array now has a field account , which is the account of the token balance. The instructions column has changed. As mentioned above, the struct in the array now has a field executing_account , which is the account executing the instruction. The inner_instructions column is removed, and inner instructions have been moved into the instructions column.","title":"2022-03-01"},{"location":"reference/tables/raw/solana/account-activity/","text":"Account activity \u00b6 Solana.account_activity \u00b6 This table contains information from the transactions table focused on account usage. Each row contains all information about an account's usage in a transaction. Column Name Column Type Description block_slot bigint The slot of the block this transaction was in. block_hash string The hash of the block this transaction was in block_time timestamp The timestamp that this account usage occurred block_date date The date this account usage occurred address string The address of the account, also referred to as public key tx_index int The index of this transaction in the block tx_id string The ID of the transaction in which this account usage occurred tx_success boolean The transaction succeeded and was committed signed boolean This account signed this transaction writeable boolean This account was granted read-write access in this transaction pre_balance bigint The balance of this account before the transaction was processed pre_token___balance decimal The token balance before the transaction was processed post_balance bigint The balance of this account after the transaction was processed post_token___balance decimal The token balance after the transaction was processed balance_change bigint The balance change that occurred as part of the transaction token_balance___change decimal The balance change that occurred as part of the transaction","title":"Account activity"},{"location":"reference/tables/raw/solana/account-activity/#account-activity","text":"","title":"Account activity"},{"location":"reference/tables/raw/solana/account-activity/#solanaaccount_activity","text":"This table contains information from the transactions table focused on account usage. Each row contains all information about an account's usage in a transaction. Column Name Column Type Description block_slot bigint The slot of the block this transaction was in. block_hash string The hash of the block this transaction was in block_time timestamp The timestamp that this account usage occurred block_date date The date this account usage occurred address string The address of the account, also referred to as public key tx_index int The index of this transaction in the block tx_id string The ID of the transaction in which this account usage occurred tx_success boolean The transaction succeeded and was committed signed boolean This account signed this transaction writeable boolean This account was granted read-write access in this transaction pre_balance bigint The balance of this account before the transaction was processed pre_token___balance decimal The token balance before the transaction was processed post_balance bigint The balance of this account after the transaction was processed post_token___balance decimal The token balance after the transaction was processed balance_change bigint The balance change that occurred as part of the transaction token_balance___change decimal The balance change that occurred as part of the transaction","title":"Solana.account_activity"},{"location":"reference/tables/raw/solana/blocks/","text":"Blocks \u00b6 Solana.blocks \u00b6 This table contains the block data within Solana\u2019s blockchain. It can be used to identify block activity and transaction changes over time. Column Name Data Type Description hash string string The hash of this block, base-58 encoded height bigint The number of blocks beneath this block slot bigint This block\u2019s slot index in the ledger time timestamp The (estimated) time this block was produced date date Used to partition by parent_slot bigint The slot index of this block's parent previous_block___hash string The hash of this block's parent, base-58 encoded total_transactions bigint The total number of transactions in this block successful_transactions bigint The number of successful transactions in this block failed_transactions bigint The number of failed transactions in this block Solana Query examples can be found here: Solana blocks over time and Transactions per day","title":"Blocks"},{"location":"reference/tables/raw/solana/blocks/#blocks","text":"","title":"Blocks"},{"location":"reference/tables/raw/solana/blocks/#solanablocks","text":"This table contains the block data within Solana\u2019s blockchain. It can be used to identify block activity and transaction changes over time. Column Name Data Type Description hash string string The hash of this block, base-58 encoded height bigint The number of blocks beneath this block slot bigint This block\u2019s slot index in the ledger time timestamp The (estimated) time this block was produced date date Used to partition by parent_slot bigint The slot index of this block's parent previous_block___hash string The hash of this block's parent, base-58 encoded total_transactions bigint The total number of transactions in this block successful_transactions bigint The number of successful transactions in this block failed_transactions bigint The number of failed transactions in this block Solana Query examples can be found here: Solana blocks over time and Transactions per day","title":"Solana.blocks"},{"location":"reference/tables/raw/solana/rewards/","text":"Rewards \u00b6 Solana.rewards \u00b6 This table contains data about rewards paid out on Solana. One block may contain zero or more rewards, and each row corresponds to one reward. An example query can be found here: Solana rewards fee per day Column Name Column Type Description block_slot bigint This block\u2019s slot index in the ledger block_hash string The hash of this block, base-58 encoded block_time timestamp The (estimated) time this block was produced block_date date Event date commission string Vote account commission when the reward was credited, only present for voting and staking rewards lamports bigint Number of reward lamports credited or debited by the account pre_balance bigint Account balance in lamports before the reward was applied post_balance bigint Account balance in lamports after the reward was applied recipient string The public key, as base-58 encoded string, of the account that received the reward reward_type string Type of reward: \"fee\", \"rent\", \"voting\", \"staking\"","title":"Rewards"},{"location":"reference/tables/raw/solana/rewards/#rewards","text":"","title":"Rewards"},{"location":"reference/tables/raw/solana/rewards/#solanarewards","text":"This table contains data about rewards paid out on Solana. One block may contain zero or more rewards, and each row corresponds to one reward. An example query can be found here: Solana rewards fee per day Column Name Column Type Description block_slot bigint This block\u2019s slot index in the ledger block_hash string The hash of this block, base-58 encoded block_time timestamp The (estimated) time this block was produced block_date date Event date commission string Vote account commission when the reward was credited, only present for voting and staking rewards lamports bigint Number of reward lamports credited or debited by the account pre_balance bigint Account balance in lamports before the reward was applied post_balance bigint Account balance in lamports after the reward was applied recipient string The public key, as base-58 encoded string, of the account that received the reward reward_type string Type of reward: \"fee\", \"rent\", \"voting\", \"staking\"","title":"Solana.rewards"},{"location":"reference/tables/raw/solana/transactions/","text":"Transactions \u00b6 Solana.transactions \u00b6 This table contains the transaction data within Solana\u2019s blockchain. Most of the relevant data related to account, protocol, and program activity is available in this table. Query examples can be found here: NFT transactions of popular programs past 7 days and drift-protocol overview Column Name Column Type Description block_slot bigint This block\u2019s slot index in the ledger block_time timestamp The (estimated) time this block was produced block_date date Event date index bigint Index into the block\u2019s transactions fee bigint Fee this transaction was charged, as paid by first account block_hash string The hash of this block, base-58 encoded error STRUCT error NULL if success is true. required_signatures bigint The total number of signatures required to make the transaction valid. readonly_signed___accounts bigint The last readonly_signed_accounts of the signed keys are read-only accounts. readonly_unsigned___accounts bigint The last readonly_unsigned_accounts of the unsigned keys are read-only accounts. id string The first signature in the transaction success boolean The transaction was valid and thus committed. recent_block___hash string The hash of a recent block in the ledger, used to prevent transaction duplication and to give transactions lifetimes instructions array\\ Instructions to execute (in order) accountKeys array\\ The account keys used in the transaction log_messages array\\ The log messages emitted by the transaction pre_balances array\\ Array of account balances before the transaction was processed. The i-th balance is the balance of the i-th account key in account_keys post_balances array\\ Array of account balances after the transaction was processed. The i-th balance is the balance of the i-th account key in account_keys pre_token_balance array\\ List of token balances from before the transaction was processed or omitted if token balance recording was not yet enabled during this transaction post_token_balance array\\ List of token balances from after the transaction was processed or omitted if token balance recording was not yet enabled during this transaction signatures array\\ A list of base-58 encoded signatures applied to the transaction. Always of length numRequiredSignatures signer string The initial value from the account_keys array that initiates the transaction and pays the transaction fee Struct definitions \u00b6 Within several of these columns is a data type of STRUCT which allows for representing nested hierarchical data and has key-value pairs. It's similar to a dictionary in python and can be used to group fields together to make them more accessible. An example of how these can be used to extract data: # of Solana instructions by day for DEXes token_balance Field Data type Description account string The account key of the account that the token balance is provided for. mint string Public key of the token\u2019s mint. This is an account that stores metadata about the token: The supply, number of decimals, and various authorities with control over the mint. amount Decimal Derived from the token balance's raw amount (ui_token_amount.amount) and the number of decimals (ui_token_amount.decimals) instructions Field Data type Description account_arguments array\\ Ordered list of accounts to pass to the program data string Program input data in a base-58 string executing_account string The account key of the program that executed this instruction. inner_instructions array\\ The instructions invoked by this instruction. inner_instructions Field Data type Description account_arguments array\\ Ordered list of accounts to pass to the program data string Program input data in a base-58 string executing_account string The account key of the program that executed this instruction. error Field Data type Description instruction_index int The instruction number that failed message string The error message \u00b6","title":"Transactions"},{"location":"reference/tables/raw/solana/transactions/#transactions","text":"","title":"Transactions"},{"location":"reference/tables/raw/solana/transactions/#solanatransactions","text":"This table contains the transaction data within Solana\u2019s blockchain. Most of the relevant data related to account, protocol, and program activity is available in this table. Query examples can be found here: NFT transactions of popular programs past 7 days and drift-protocol overview Column Name Column Type Description block_slot bigint This block\u2019s slot index in the ledger block_time timestamp The (estimated) time this block was produced block_date date Event date index bigint Index into the block\u2019s transactions fee bigint Fee this transaction was charged, as paid by first account block_hash string The hash of this block, base-58 encoded error STRUCT error NULL if success is true. required_signatures bigint The total number of signatures required to make the transaction valid. readonly_signed___accounts bigint The last readonly_signed_accounts of the signed keys are read-only accounts. readonly_unsigned___accounts bigint The last readonly_unsigned_accounts of the unsigned keys are read-only accounts. id string The first signature in the transaction success boolean The transaction was valid and thus committed. recent_block___hash string The hash of a recent block in the ledger, used to prevent transaction duplication and to give transactions lifetimes instructions array\\ Instructions to execute (in order) accountKeys array\\ The account keys used in the transaction log_messages array\\ The log messages emitted by the transaction pre_balances array\\ Array of account balances before the transaction was processed. The i-th balance is the balance of the i-th account key in account_keys post_balances array\\ Array of account balances after the transaction was processed. The i-th balance is the balance of the i-th account key in account_keys pre_token_balance array\\ List of token balances from before the transaction was processed or omitted if token balance recording was not yet enabled during this transaction post_token_balance array\\ List of token balances from after the transaction was processed or omitted if token balance recording was not yet enabled during this transaction signatures array\\ A list of base-58 encoded signatures applied to the transaction. Always of length numRequiredSignatures signer string The initial value from the account_keys array that initiates the transaction and pays the transaction fee","title":"Solana.transactions"},{"location":"reference/tables/raw/solana/transactions/#struct-definitions","text":"Within several of these columns is a data type of STRUCT which allows for representing nested hierarchical data and has key-value pairs. It's similar to a dictionary in python and can be used to group fields together to make them more accessible. An example of how these can be used to extract data: # of Solana instructions by day for DEXes token_balance Field Data type Description account string The account key of the account that the token balance is provided for. mint string Public key of the token\u2019s mint. This is an account that stores metadata about the token: The supply, number of decimals, and various authorities with control over the mint. amount Decimal Derived from the token balance's raw amount (ui_token_amount.amount) and the number of decimals (ui_token_amount.decimals) instructions Field Data type Description account_arguments array\\ Ordered list of accounts to pass to the program data string Program input data in a base-58 string executing_account string The account key of the program that executed this instruction. inner_instructions array\\ The instructions invoked by this instruction. inner_instructions Field Data type Description account_arguments array\\ Ordered list of accounts to pass to the program data string Program input data in a base-58 string executing_account string The account key of the program that executed this instruction. error Field Data type Description instruction_index int The instruction number that failed message string The error message","title":"Struct definitions"},{"location":"reference/tables/raw/solana/transactions/#_1","text":"","title":""},{"location":"reference/tables/raw/solana/vote-transactions/","text":"Vote Transactions \u00b6 Solana.vote_transactions \u00b6 This table contains the full set of vote transactions that are submitted by validators to vote on a block. It can be joined with the non-vote transactions table above to get a full breakdown of all transactions. It has the same schema as the main transactions table. An example query that demonstrates that is available here: Solana transactions past 30 days Column Name Column Type Description block_slot bigint This block\u2019s slot index in the ledger block_time timestamp The (estimated) time this block was produced block_date date Event date index bigint Index into the block\u2019s transactions fee bigint Fee this transaction was charged, as paid by first account block_hash string The hash of this block, base-58 encoded error STRUCT error NULL if success is true. required_signatures bigint The total number of signatures required to make the transaction valid. readonly_signed___accounts bigint The last readonly_signed_accounts of the signed keys are read-only accounts. readonly_unsigned___accounts bigint The last readonly_unsigned_accounts of the unsigned keys are read-only accounts. id string The first signature in the transaction success boolean The transaction was valid and thus committed. recent_block___hash string The hash of a recent block in the ledger, used to prevent transaction duplication and to give transactions lifetimes instructions array\\ Instructions to execute (in order) accountKeys array\\ The account keys used in the transaction log_messages array\\ The log messages emitted by the transaction pre_balances array\\ Array of account balances before the transaction was processed. The i-th balance is the balance of the i-th account key in account_keys post_balances array\\ Array of account balances after the transaction was processed. The i-th balance is the balance of the i-th account key in account_keys pre_token_balance array\\ List of token balances from before the transaction was processed or omitted if token balance recording was not yet enabled during this transaction post_token_balance array\\ List of token balances from after the transaction was processed or omitted if token balance recording was not yet enabled during this transaction signatures array\\ A list of base-58 encoded signatures applied to the transaction. Always of length numRequiredSignatures signer string The initial value from the account_keys array that initiates the transaction and pays the transaction fee","title":"Vote Transactions"},{"location":"reference/tables/raw/solana/vote-transactions/#vote-transactions","text":"","title":"Vote Transactions"},{"location":"reference/tables/raw/solana/vote-transactions/#solanavote_transactions","text":"This table contains the full set of vote transactions that are submitted by validators to vote on a block. It can be joined with the non-vote transactions table above to get a full breakdown of all transactions. It has the same schema as the main transactions table. An example query that demonstrates that is available here: Solana transactions past 30 days Column Name Column Type Description block_slot bigint This block\u2019s slot index in the ledger block_time timestamp The (estimated) time this block was produced block_date date Event date index bigint Index into the block\u2019s transactions fee bigint Fee this transaction was charged, as paid by first account block_hash string The hash of this block, base-58 encoded error STRUCT error NULL if success is true. required_signatures bigint The total number of signatures required to make the transaction valid. readonly_signed___accounts bigint The last readonly_signed_accounts of the signed keys are read-only accounts. readonly_unsigned___accounts bigint The last readonly_unsigned_accounts of the unsigned keys are read-only accounts. id string The first signature in the transaction success boolean The transaction was valid and thus committed. recent_block___hash string The hash of a recent block in the ledger, used to prevent transaction duplication and to give transactions lifetimes instructions array\\ Instructions to execute (in order) accountKeys array\\ The account keys used in the transaction log_messages array\\ The log messages emitted by the transaction pre_balances array\\ Array of account balances before the transaction was processed. The i-th balance is the balance of the i-th account key in account_keys post_balances array\\ Array of account balances after the transaction was processed. The i-th balance is the balance of the i-th account key in account_keys pre_token_balance array\\ List of token balances from before the transaction was processed or omitted if token balance recording was not yet enabled during this transaction post_token_balance array\\ List of token balances from after the transaction was processed or omitted if token balance recording was not yet enabled during this transaction signatures array\\ A list of base-58 encoded signatures applied to the transaction. Always of length numRequiredSignatures signer string The initial value from the account_keys array that initiates the transaction and pays the transaction fee","title":"Solana.vote_transactions"},{"location":"reference/tables/spells/","text":"Spells are custom tables that are built and maintained by Dune and our community. To learn more about what Spells are see Spellbook . To see a complete list of Spellbook Spell tables, visit the Spellbook Model Docs . You can generally divide them into 2 distinct categories: Sector Spells \u00b6 Sector Spells are tables like dex.trades, erc20.stablecoins, lending.borrow etc. These spells take in data from multiple contracts and projects, standardize the data across them and therefore make it very easy to query for this data and compare the metrics of different projects with each other. Most of the sector Dashboards dashboards depend on sector spells. This introduces an interesting dynamic in which projects can easily get their data into these dashboards by making a pull request to our public github repo . Team Dune and the community are always improving on these sector spells, all new additions to existing ones are always welcome. Project Spells \u00b6 Projects can assemble their data into one neat table that has all the data they need in one place. To do this, you can construct views or tables in our spells. The main advantage here over just constructing a view is that you are able to deal with bigger amounts of data in our spells since we can run them automatically in the background every few hours. Contributing to Spellbook \u00b6 If you'd like to contribute to Dune spells, take a look at Spellbook . These enable you to effortlessly aggregate lots of data with as little friction as possible. To view available Spells, take a look at our Spellbook model documentation and learn how to contribute new spells [here](../../../../spellbook/index.md Our Spells are managed via the public Spellbook GitHub repository . We welcome pull requests! Abstractions (Dune V1 PostgreSQL) \u00b6 For our V1 Engine (PostgreSQL), abstractions are snippets of SQL executed the data platform. You can check for existing abstractions on GitHub , and view [documentation](v1/../v1/abstractions/index.mdn our most popular abstractions. You can check for existing abstractions in our public github repository , under deprecated-dune-v1-abstractions . Our abstractions for v1 are no longer open for contributions.","title":"Spells"},{"location":"reference/tables/spells/#sector-spells","text":"Sector Spells are tables like dex.trades, erc20.stablecoins, lending.borrow etc. These spells take in data from multiple contracts and projects, standardize the data across them and therefore make it very easy to query for this data and compare the metrics of different projects with each other. Most of the sector Dashboards dashboards depend on sector spells. This introduces an interesting dynamic in which projects can easily get their data into these dashboards by making a pull request to our public github repo . Team Dune and the community are always improving on these sector spells, all new additions to existing ones are always welcome.","title":"Sector Spells"},{"location":"reference/tables/spells/#project-spells","text":"Projects can assemble their data into one neat table that has all the data they need in one place. To do this, you can construct views or tables in our spells. The main advantage here over just constructing a view is that you are able to deal with bigger amounts of data in our spells since we can run them automatically in the background every few hours.","title":"Project Spells"},{"location":"reference/tables/spells/#contributing-to-spellbook","text":"If you'd like to contribute to Dune spells, take a look at Spellbook . These enable you to effortlessly aggregate lots of data with as little friction as possible. To view available Spells, take a look at our Spellbook model documentation and learn how to contribute new spells [here](../../../../spellbook/index.md Our Spells are managed via the public Spellbook GitHub repository . We welcome pull requests!","title":"Contributing to Spellbook"},{"location":"reference/tables/spells/#abstractions-dune-v1-postgresql","text":"For our V1 Engine (PostgreSQL), abstractions are snippets of SQL executed the data platform. You can check for existing abstractions on GitHub , and view [documentation](v1/../v1/abstractions/index.mdn our most popular abstractions. You can check for existing abstractions in our public github repository , under deprecated-dune-v1-abstractions . Our abstractions for v1 are no longer open for contributions.","title":"Abstractions (Dune V1 PostgreSQL)"},{"location":"reference/tables/spells/dex.trades/","text":"Decentralized exchanges are the beating heart of the DeFi industry. You can swap any ERC-20 token for any ERC-20 token through the magic of smart contracts. The problem here: there are so many decentralized exchanges out there that it's hardly possible for any single person to work with the smart contract data for all of them. That's why we created dex.trades . This table standardizes and normalizes the trading data across virtually all relevant decentralized exchanges. This in turn allows you to easily query for trading data for your favorite tokens without having to deal with all of the different DEX smart contracts yourself. The scripts that generate the table dex.trades can be found in this public github repo. Column Data \u00b6 Column name Data type Description block_time timestamptz The timestamp of the block that included this transaction token_a_symbol string The symbol of one of the two tokens that got traded token_b_symbol string The symbol of one of the two tokens that got traded token_a_amount numeric The amount of token A that got traded token_b_amount numeric The amount of token B that got traded project string The dex on which this trade was executed version string Which version of the dex got used? category string Is this project and aggregator or a decentralized exchange? trader_a bytea Which contract called the dex contract? trader_b bytea In some special cases there actually is a counter party to transactions, this party will get displayed here if applicable token_a_amount_raw numeric The raw amount of token A that got traded token_b_amount_raw numeric The raw amount of token B that got traded usd_amount numeric The USD value of this trade token_a_address bytea The ERC-20 token contract address of token A token_b_address bytea The ERC-20 token contract address of token B exchange_contract_address bytea The address of the decentralized exchange contract that made this trade possible tx_hash bytea The hash of the transaction that contained this trade tx_from bytea Which address initiated this transaction? tx_to bytea What was the first smart contract that got called during this tx? trace_address ARRAY Which position in the graph tree does the execution of the trade have? evt_index integer This logs index position in the block (cumulative amount of logs ordered by execution) trade_id integer Just for database magic","title":"dex.trades"},{"location":"reference/tables/spells/dex.trades/#column-data","text":"Column name Data type Description block_time timestamptz The timestamp of the block that included this transaction token_a_symbol string The symbol of one of the two tokens that got traded token_b_symbol string The symbol of one of the two tokens that got traded token_a_amount numeric The amount of token A that got traded token_b_amount numeric The amount of token B that got traded project string The dex on which this trade was executed version string Which version of the dex got used? category string Is this project and aggregator or a decentralized exchange? trader_a bytea Which contract called the dex contract? trader_b bytea In some special cases there actually is a counter party to transactions, this party will get displayed here if applicable token_a_amount_raw numeric The raw amount of token A that got traded token_b_amount_raw numeric The raw amount of token B that got traded usd_amount numeric The USD value of this trade token_a_address bytea The ERC-20 token contract address of token A token_b_address bytea The ERC-20 token contract address of token B exchange_contract_address bytea The address of the decentralized exchange contract that made this trade possible tx_hash bytea The hash of the transaction that contained this trade tx_from bytea Which address initiated this transaction? tx_to bytea What was the first smart contract that got called during this tx? trace_address ARRAY Which position in the graph tree does the execution of the trade have? evt_index integer This logs index position in the block (cumulative amount of logs ordered by execution) trade_id integer Just for database magic","title":"Column Data"},{"location":"reference/tables/spells/erc-20-balances/","text":"The following tables allow for easy tracking of wallet-balances, token allocations or supply of a token over time or in a snapshot format. On a raw data level it's pretty hard to work with ERC-20 tokens since you need to sum all transfers for all addresses over time. This unnecessarily bloats queries and quickly leads to human errors. To prevent that from happening we have constructed several views and tables that will help you query for ERC-20 data with ease. These tables can be used for all kinds of interesting analysis, but you still need to watch out for a few things while working with them. First, the mint/burn address is not standardized, so you need to find out those addresses and manually apply a fix in your queries. In most cases it will be x0000000000000000000000000000000000000000 for minting and burning, but always make sure that is indeed the case. In the example given that's exactly not the case, there is an additional burn address x000000000000000000000000000000000000dead . Example: Select wallet_address , amount , day , token_symbol from ERC - 20 . \"view_token_balances_daily\" where token_address = '\\x429881672B9AE42b8EbA0E26cD9C73711b891Ca5' and wallet_address != '\\x0000000000000000000000000000000000000000' --mint address and wallet_address != '\\x000000000000000000000000000000000000dead' --burn address Working with these tables quickly leads to a lot of individual data points that our visualization engine is not always able to handle perfectly. Instead of trying to display every unique holder it makes sense to group them by certain criteria and display the dataset that way. This is unique for every token, you might need to experiment a bit to see what works in your queries. Example: Select CASE WHEN wallet_address = '\\xbBCf169eE191A1Ba7371F30A1C344bFC498b29Cf' then 'dill' WHEN wallet_address = '\\xdc98556Ce24f007A5eF6dC1CE96322d65832A819' then 'uniswap' WHEN wallet_address = '\\xC52139a20A57c9002e9F5188901EF0ffC63c7205' then 'smart_treasury' WHEN wallet_address = '\\x40ec5b33f54e0e8a33a975908c5ba1c14e5bbbdf' then 'polygon' WHEN wallet_address = '\\x6cc5f688a315f3dc28a7781717a9a798a59fda7b' then 'OKEX' WHEN amount between 0 and 10 then 'Plankton(0-10)' WHEN amount between 10 and 100 then 'shrimp(10-100)' WHEN amount between 100 and 1000 then 'fish(100-1,000)' WHEN amount between 1000 and 10000 then 'dolphin(1,000-10,000)' WHEN amount > 10000 then 'whale (>10000)' --note that the order of case statements matters here end as classification , sum ( amount ) as amount , token_symbol from ERC - 20 . \"view_token_balances_latest\" where token_address = '\\x429881672B9AE42b8EbA0E26cD9C73711b891Ca5' and wallet_address != '\\x0000000000000000000000000000000000000000' and wallet_address != '\\x000000000000000000000000000000000000dead' and amount > 0 . 1 --filter out dust amounts, adjust this for different tokens based on economic value group by 1 , 3 Dashboard example This Dashboard by 0xBoxer contains the most important use cases related to a single ERC-20 token that is used as gov token. ERC-20.view_token_balances_latest \u00b6 This view depends on the ERC-20.token_balances table and gives you the information of the latest distribution of that token. Column name Data type Description amount numeric The correct display format for that token amount_raw numeric The raw amount of that token (need to divide by decimals!) amount_usd float8 The current price (if we have data on the price) last_transfer_timestamp timestamptz The date on which the balance of this token last changed in this particular wallet address token_address bytea The address of the token token_symbol text The symbol of the token wallet_address bytea The address of the wallet holding this token ERC-20.view_token_balances_hourly \u00b6 This table will provide information about all token balances on an hourly basis. It also already includes decimals and prices in most cases, so they are pretty much ready to go out of the box. Column name Data type Description amount numeric The correct display format for that token amount_raw numeric The raw amount of that token (need to divide by decimals!) amount_usd float8 The current price (if we have data on the price) hour timestamptz The time in the resolution of hours token_address bytea The address of the token token_symbol text The symbol of the token wallet_address bytea The address of the wallet holding this token ERC-20.view_token_balances_daily \u00b6 This table will perform much better than ERC-20.view_token_balances_hourly since it's only querying for data on a daily basis. If you want to make high level analysis, this is your way to go. Column name Data type Description amount numeric The correct display format for that token amount_raw numeric The raw amount of that token (need to divide by decimals!) amount_usd float8 The current price (if we have data on the price) day timestamptz The time in the resolution of days token_address bytea The address of the token token_symbol text The symbol of the token wallet_address bytea The address of the wallet holding this token ERC-20.token_balances \u00b6 This table contains the hourly balance of all ERC-20 tokens over the entire existence of these tokens. You can use this table as a fallback option might the views we have provided above not be sufficient for the usecase you are trying to establish. Column name Data type Description amount numeric The correct display format for that token amount_raw numeric The raw amount of that token (need to divide by decimals!) timestamp timestamptz The time in the resolution of hours token_address bytea The address of the token token_symbol text The symbol of the token wallet_address bytea The address of the wallet holding this token","title":"ERC-20 balances"},{"location":"reference/tables/spells/erc-20-balances/#erc-20view_token_balances_latest","text":"This view depends on the ERC-20.token_balances table and gives you the information of the latest distribution of that token. Column name Data type Description amount numeric The correct display format for that token amount_raw numeric The raw amount of that token (need to divide by decimals!) amount_usd float8 The current price (if we have data on the price) last_transfer_timestamp timestamptz The date on which the balance of this token last changed in this particular wallet address token_address bytea The address of the token token_symbol text The symbol of the token wallet_address bytea The address of the wallet holding this token","title":"ERC-20.view_token_balances_latest"},{"location":"reference/tables/spells/erc-20-balances/#erc-20view_token_balances_hourly","text":"This table will provide information about all token balances on an hourly basis. It also already includes decimals and prices in most cases, so they are pretty much ready to go out of the box. Column name Data type Description amount numeric The correct display format for that token amount_raw numeric The raw amount of that token (need to divide by decimals!) amount_usd float8 The current price (if we have data on the price) hour timestamptz The time in the resolution of hours token_address bytea The address of the token token_symbol text The symbol of the token wallet_address bytea The address of the wallet holding this token","title":"ERC-20.view_token_balances_hourly"},{"location":"reference/tables/spells/erc-20-balances/#erc-20view_token_balances_daily","text":"This table will perform much better than ERC-20.view_token_balances_hourly since it's only querying for data on a daily basis. If you want to make high level analysis, this is your way to go. Column name Data type Description amount numeric The correct display format for that token amount_raw numeric The raw amount of that token (need to divide by decimals!) amount_usd float8 The current price (if we have data on the price) day timestamptz The time in the resolution of days token_address bytea The address of the token token_symbol text The symbol of the token wallet_address bytea The address of the wallet holding this token","title":"ERC-20.view_token_balances_daily"},{"location":"reference/tables/spells/erc-20-balances/#erc-20token_balances","text":"This table contains the hourly balance of all ERC-20 tokens over the entire existence of these tokens. You can use this table as a fallback option might the views we have provided above not be sufficient for the usecase you are trying to establish. Column name Data type Description amount numeric The correct display format for that token amount_raw numeric The raw amount of that token (need to divide by decimals!) timestamp timestamptz The time in the resolution of hours token_address bytea The address of the token token_symbol text The symbol of the token wallet_address bytea The address of the wallet holding this token","title":"ERC-20.token_balances"},{"location":"reference/tables/spells/labels/","text":"Have you ever made a query on Dune where you get a list of addresses, only to stop and wonder what\u2019s behind these beautiful, random hexadecimal encoded strings? So have we. Address labels is a feature on Dune where you as a user can add , update and query labels for any address. What is a label? \u00b6 A label is a piece of metadata about an address , a tag or metadata if you will. It comes in the form of a key-value pair. The key is the label type , and the value the label name . Browse addresses and and labels at the labels page . What labels look like \u00b6 Check out this dashboard for examples on what can be created with labels. The address 0xD551234Ae421e3BCBA99A0Da6d736074f22192FF can be labeled like this: Type Name owner binance wallet_type exchange The address is controlled by the exchange Binance. The address 0xe65040f61701940b62e18da7a53126a58525588b can be labeled like this: Type Name dapp_usage uniswap user activity dex trader The address in the past interacted with Uniswap. You are free to come up with both new types and label names, as labels on Dune are open ended and crowd sourced . Adding labels \u00b6 Use Dune queries to label addresses. A very powerful and scalable way to add labels like \u201call these addresses used Uniswap\u201d, and much much more. Please see our GitHub for examples of labels created with queries and PR in your own! Examples of what you can do: Label all addresses that used a certain dapp Label all addresses that hold a certain amount of a token Label all addresses that use a dapp more than X times per month Label all addresses that sent money to Binance You could also do more novel and involved things around user patterns like who did arbitrage trades or profited from flash loans and so much more. Note that there might be a few minutes delay from adding the label on dune.com until you can query it in SQL. The labels table \u00b6 Labels are stored in the new labels.labels table which has the following schema: Column name Data type Description id int incrementing integer address bytea The address of a contract or wallet this label describes name text label name type text label type author text The username of the user who created this label source text The source of this label, autopopulated by dune updated_at timestamptz The last time this label was changed Using labels \u00b6 Note that this table holds multiple rows per address, and therefore joins against it can be tricky to get right. For that reason we\u2019ve made the convenient function: labels.get(address bytea, type text default null) RETURNS text[] which we anticipate will be the primary way to use labels. See examples below. Typically if you do a query that returns address you can use labels.get(address) to get all labels for that address independent of label type. If you want to see labels of the type owner you can do labels.get(address, 'owner') . You can also pass this function several label types you want included like: labels.get(address, 'owner', 'project') . We\u2019ve also added the function labels.url(address bytea) . Pass that function an address from your query and your results table will contain a clickable link to for instance: https://dune.com/ethereum/address/0xD551234Ae421e3BCBA99A0Da6d736074f22192FF Usecase 1: I want to display labels for a list of addresses \u00b6 Note We encourage you to run these queries in Dune while you read this Say you\u2019re looking at the top 10 traders of DAI across all dexes last 24 hours: SELECT trader_a , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' GROUP BY 1 ORDER BY 3 DESC LIMIT 10 ; If you want to have labels for these addresses simply alter the trader_a column to labels.get(trader_a) . Note In the examples below --- represents lines removed, and +++ lines added. SELECT trader_a , labels . get ( trader_a ) as label , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' and not labels . get ( trader_a ) isnull GROUP BY 1 ORDER BY 2 DESC LIMIT 100 ; Now you\u2019ve replaced the addresses with lists of all labels for trader_a. Sometimes you\u2019re only interested in a subset of labels: labels.get accepts an optional list of type names which filter the type of labels you get. Say you\u2019re only interested in \u2018activity\u2019 labels: SELECT trader_a , labels . get ( trader_a , 'activity' ) as label , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' and not labels . get ( trader_a ) isnull GROUP BY 1 ORDER BY 2 DESC LIMIT 100 ; Of course you can also show the address, and filter for multiple label types SELECT trader_a , labels . get ( trader_a , 'activity' , 'project' , 'contract_name' ) as label , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' and not labels . get ( trader_a ) isnull GROUP BY 1 ORDER BY 2 DESC LIMIT 100 ; You can also use labels.url to make the addresses clickable: SELECT labels . url ( trader_a ), labels . get ( trader_a , 'activity' ) as labels , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' GROUP BY 1 , 2 ORDER BY 3 DESC LIMIT 10 ; This way people who look at your dashboard can easily contribute even better labels to it! Usecase 2: I want to filter my query by labels that exist. \u00b6 In this usecase you wouldn\u2019t want to use labels.get , because it can be slow to operate with. Instead you\u2019ll use the fantastic EXISTS function in SQL. As an example: you\u2019re querying Uniswap , but are interested in the behavior of users who have traded previously on 1inch . Here\u2019s how you\u2019d go about that: SELECT \"to\" FROM uniswap_v2 . \"Pair_evt_Swap\" WHERE EXISTS ( SELECT * FROM labels . labels WHERE address = \"to\" AND type = 'dapp usage' AND name = '1inch user' ) LIMIT 10 ; The above query will give you 10 address that has swapped on Uniswap and traded on 1inch. Of course, you can use the two patterns in conjunction! If you are interested for labels on those addresses, go ahead and use labels.get in addition to the WHERE EXISTS pattern: --- SELECT \"to\" +++ SELECT \"to\" , labels . get ( \"to\" ) FROM uniswap_v2 . \"Pair_evt_Swap\" WHERE EXISTS ( SELECT * FROM labels . labels WHERE address = \"to\" AND type = 'dapp usage' AND name = '1inch user' ) LIMIT 10 ; There you have it: you see addresses that traded on both Uniswap and 1inch and all associated address labels.","title":"Labels"},{"location":"reference/tables/spells/labels/#what-is-a-label","text":"A label is a piece of metadata about an address , a tag or metadata if you will. It comes in the form of a key-value pair. The key is the label type , and the value the label name . Browse addresses and and labels at the labels page .","title":"What is a label?"},{"location":"reference/tables/spells/labels/#what-labels-look-like","text":"Check out this dashboard for examples on what can be created with labels. The address 0xD551234Ae421e3BCBA99A0Da6d736074f22192FF can be labeled like this: Type Name owner binance wallet_type exchange The address is controlled by the exchange Binance. The address 0xe65040f61701940b62e18da7a53126a58525588b can be labeled like this: Type Name dapp_usage uniswap user activity dex trader The address in the past interacted with Uniswap. You are free to come up with both new types and label names, as labels on Dune are open ended and crowd sourced .","title":"What labels look like"},{"location":"reference/tables/spells/labels/#adding-labels","text":"Use Dune queries to label addresses. A very powerful and scalable way to add labels like \u201call these addresses used Uniswap\u201d, and much much more. Please see our GitHub for examples of labels created with queries and PR in your own! Examples of what you can do: Label all addresses that used a certain dapp Label all addresses that hold a certain amount of a token Label all addresses that use a dapp more than X times per month Label all addresses that sent money to Binance You could also do more novel and involved things around user patterns like who did arbitrage trades or profited from flash loans and so much more. Note that there might be a few minutes delay from adding the label on dune.com until you can query it in SQL.","title":"Adding labels"},{"location":"reference/tables/spells/labels/#the-labels-table","text":"Labels are stored in the new labels.labels table which has the following schema: Column name Data type Description id int incrementing integer address bytea The address of a contract or wallet this label describes name text label name type text label type author text The username of the user who created this label source text The source of this label, autopopulated by dune updated_at timestamptz The last time this label was changed","title":"The labels table"},{"location":"reference/tables/spells/labels/#using-labels","text":"Note that this table holds multiple rows per address, and therefore joins against it can be tricky to get right. For that reason we\u2019ve made the convenient function: labels.get(address bytea, type text default null) RETURNS text[] which we anticipate will be the primary way to use labels. See examples below. Typically if you do a query that returns address you can use labels.get(address) to get all labels for that address independent of label type. If you want to see labels of the type owner you can do labels.get(address, 'owner') . You can also pass this function several label types you want included like: labels.get(address, 'owner', 'project') . We\u2019ve also added the function labels.url(address bytea) . Pass that function an address from your query and your results table will contain a clickable link to for instance: https://dune.com/ethereum/address/0xD551234Ae421e3BCBA99A0Da6d736074f22192FF","title":"Using labels"},{"location":"reference/tables/spells/labels/#usecase-1-i-want-to-display-labels-for-a-list-of-addresses","text":"Note We encourage you to run these queries in Dune while you read this Say you\u2019re looking at the top 10 traders of DAI across all dexes last 24 hours: SELECT trader_a , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' GROUP BY 1 ORDER BY 3 DESC LIMIT 10 ; If you want to have labels for these addresses simply alter the trader_a column to labels.get(trader_a) . Note In the examples below --- represents lines removed, and +++ lines added. SELECT trader_a , labels . get ( trader_a ) as label , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' and not labels . get ( trader_a ) isnull GROUP BY 1 ORDER BY 2 DESC LIMIT 100 ; Now you\u2019ve replaced the addresses with lists of all labels for trader_a. Sometimes you\u2019re only interested in a subset of labels: labels.get accepts an optional list of type names which filter the type of labels you get. Say you\u2019re only interested in \u2018activity\u2019 labels: SELECT trader_a , labels . get ( trader_a , 'activity' ) as label , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' and not labels . get ( trader_a ) isnull GROUP BY 1 ORDER BY 2 DESC LIMIT 100 ; Of course you can also show the address, and filter for multiple label types SELECT trader_a , labels . get ( trader_a , 'activity' , 'project' , 'contract_name' ) as label , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' and not labels . get ( trader_a ) isnull GROUP BY 1 ORDER BY 2 DESC LIMIT 100 ; You can also use labels.url to make the addresses clickable: SELECT labels . url ( trader_a ), labels . get ( trader_a , 'activity' ) as labels , SUM ( token_a_amount ) FROM dex . trades WHERE token_a_symbol = 'DAI' AND block_time > now () - interval '24 hours' GROUP BY 1 , 2 ORDER BY 3 DESC LIMIT 10 ; This way people who look at your dashboard can easily contribute even better labels to it!","title":"Usecase 1: I want to display labels for a list of addresses"},{"location":"reference/tables/spells/labels/#usecase-2-i-want-to-filter-my-query-by-labels-that-exist","text":"In this usecase you wouldn\u2019t want to use labels.get , because it can be slow to operate with. Instead you\u2019ll use the fantastic EXISTS function in SQL. As an example: you\u2019re querying Uniswap , but are interested in the behavior of users who have traded previously on 1inch . Here\u2019s how you\u2019d go about that: SELECT \"to\" FROM uniswap_v2 . \"Pair_evt_Swap\" WHERE EXISTS ( SELECT * FROM labels . labels WHERE address = \"to\" AND type = 'dapp usage' AND name = '1inch user' ) LIMIT 10 ; The above query will give you 10 address that has swapped on Uniswap and traded on 1inch. Of course, you can use the two patterns in conjunction! If you are interested for labels on those addresses, go ahead and use labels.get in addition to the WHERE EXISTS pattern: --- SELECT \"to\" +++ SELECT \"to\" , labels . get ( \"to\" ) FROM uniswap_v2 . \"Pair_evt_Swap\" WHERE EXISTS ( SELECT * FROM labels . labels WHERE address = \"to\" AND type = 'dapp usage' AND name = '1inch user' ) LIMIT 10 ; There you have it: you see addresses that traded on both Uniswap and 1inch and all associated address labels.","title":"Usecase 2: I want to filter my query by labels that exist."},{"location":"reference/tables/spells/nft.trades/","text":"nft.trades is an effort to make NFT trading data easily available to everyone on Dune. This table aggregates and standardizes the data between different data platforms and provides auxiliary information and metadata all in one table. The culmination of this is a dataset which makes it extremely easy to query for any NFT related trading data across all indexed platforms. You can find the specifications for nft.trades on our Spellbook documentation . So far we have indexed the data of the following platforms: OpenSea Rarible SuperRare CryptoPunks (They get traded in their own contracts) Foundation LooksRare How it works \u00b6 Single Item Trade \u00b6 A trade occurs between a buyer and a seller . They exchange an item which is uniquely identified by the combination of nft_contract_address and token_id . The Buyer will pay the Seller a given original_amount of tokens in any given original_currency . To make it easier, we have calculated the usd_amount that this was worth at the time of the trade for you. Most trades will be done in ETH or WETH, but especially non OpenSea trades often contain other currencies. The trade is committed on any of the indexed platforms and will be facilitated through a smart contract of those platform's exchange_contract_address . Each trade will have metadata like block_time , tx_hash , block_number , platform version , evt_index etc. Additionally, we also provide metadata about the traded NFT. nft_project_name and erc_standard will help you in analyzing your dataset more easily. nft_project_name data gets pulled from the nft.tokens table , if your NFT is missing in that table, you are welcome to make a PR to add it. Bundle Trade \u00b6 There can also be trades in which a single trade transaction contains multiple Items. Each of these Items is uniquely identified through a combination of nft_contract_address and token_id . Unfortunately, in these trades there is not a clear way to determine a corresponding usd_amount for each of the items. A possible workaround is to divide the number of items by the payment made for the bundle, but this logic very quickly falls apart when Items that are not one in kind/value get sold in a bundle. We recommend removing bundle transfers from the dataset that you are working with since it can heavily influence the results in either direction. Note that token_id and ' erc_standard will be null if tokens with different tokens IDs or erc type are transferred within the same transaction. Aggregator Trade \u00b6 There can also be trades in which a single trade transaction contains multiple items, especially when using NFT aggregator platforms. Our approach is to unravel aggregator trades so that each row correspond to a unique item that was traded, with its associated ID, price, collection, etc. Importantly, the trade_type will be indicated as Aggregator Trade , and platform names and address can be found in the nft.aggregators table . If your aggregator platform is missing in that table, you are welcome to make a PR to add it. Platform and Royalty Fees \u00b6 In the most recent version of nft.trades , information about the amount and percent of royalty fees in the original amount and in USD is available when this information was able to be retrieved. Royalty fees are going to the creator, and Platform fees are collected by the NFT platform. Note that royalty fees cannot always be retrieved, and are set to null by default. Examples \u00b6 Queries \u00b6 All trades for a given NFT \u00b6 SQL select * from nft . trades where nft_contract_address = '\\xb47e3cd837ddf8e4c57f05d70ab865de6e193bbb' --this is the cryptopunks address Results Trades in the last 24 hour on a given platform \u00b6 SQL select date_trunc ( 'day' , block_time ), usd_amount , nft_contract_address , token_id from nft . trades where platform = 'OpenSea' --only shows trades on given Platform and block_time > now () - interval '24hours' Results Platform volumes in the last year \u00b6 SQL select sum ( usd_amount ), date_trunc ( 'day' , block_time ) as day , platform from nft . trades where block_time > now () - interval '365 days' group by platform , day Results Dashboards \u00b6 That utilize parameters \u00b6 NFT by @0xBoxer NFT Sales Overview by Project by @rantum That look across the entire Ecosystem \u00b6 NFT Collection Dashboard by @rantum NFT by @sealaunch Column Data \u00b6 Column name Data type Description block_time timestamp with time zone When was this trade executed block_time text NFT project name (e.g. \"the dudes\") nft_token_id text The token_id that was traded (e.g. 235) erc_standard text The Token Standard of the traded token ERC-721 or ERC-1155 platform text Which Platform the trade was executed on platform_version text Which version of this platform was utilized? trade_type text \"Single Item Sale\" or \"Bundle Sale\" number_of_items integer How many NFTs were included in this trade category text Was this an auction or a direct sale evt_type text Currently not in use, default 'Trade' aggregator text Was this trade made using an aggregator (Yes : Name of aggregator, No : Null) usd_amount numeric USD value of the trade at time of execution seller bytea Seller of NFTs buyer bytea Buyer of NFTs original_amount numeric The amount in the right format original_amount_raw numeric Raw amount of the currency eth_amount numeric ETH value of the trade at time of execution royalty_fees_percent numeric Royalty fees going to the creator (in %) original_royalty_fees numeric Royalty fees in the currency used for this trade usd_royalty_fees numeric USD value of royalty fees at time of execution platform_fees_percent numeric Platform fees (in %) original_platform_fees numeric Platform fees in the currency used for this trade usd_platform_fees numeric USD value of platform fees at time of execution original_currency text The Currency used for this trade original_currency_contract bytea The ERC-20 address of the currency used in this trade (does not work with raw ETH) currency_contract bytea The corrected currency contract nft_contract_address bytea The contract address of the NFT traded exchange_contract_address bytea The platform contract that facilitated this trade tx_hash bytea The hash of this transaction block_number integer The block_number that this trade was done in tx_from bytea Initiated this transaction tx_to bytea Received this transaction trace_address ARRAY n/a evt_index integer Event index trade_id integer n/a Ser, my platform is not indexed \u00b6 The SQL code that processes the data for every market place is open source and available in our github repository . Everyone can review the code, make pull requests and submit code to add more marketplaces.","title":"nft.trades"},{"location":"reference/tables/spells/nft.trades/#how-it-works","text":"","title":"How it works"},{"location":"reference/tables/spells/nft.trades/#single-item-trade","text":"A trade occurs between a buyer and a seller . They exchange an item which is uniquely identified by the combination of nft_contract_address and token_id . The Buyer will pay the Seller a given original_amount of tokens in any given original_currency . To make it easier, we have calculated the usd_amount that this was worth at the time of the trade for you. Most trades will be done in ETH or WETH, but especially non OpenSea trades often contain other currencies. The trade is committed on any of the indexed platforms and will be facilitated through a smart contract of those platform's exchange_contract_address . Each trade will have metadata like block_time , tx_hash , block_number , platform version , evt_index etc. Additionally, we also provide metadata about the traded NFT. nft_project_name and erc_standard will help you in analyzing your dataset more easily. nft_project_name data gets pulled from the nft.tokens table , if your NFT is missing in that table, you are welcome to make a PR to add it.","title":"Single Item Trade"},{"location":"reference/tables/spells/nft.trades/#bundle-trade","text":"There can also be trades in which a single trade transaction contains multiple Items. Each of these Items is uniquely identified through a combination of nft_contract_address and token_id . Unfortunately, in these trades there is not a clear way to determine a corresponding usd_amount for each of the items. A possible workaround is to divide the number of items by the payment made for the bundle, but this logic very quickly falls apart when Items that are not one in kind/value get sold in a bundle. We recommend removing bundle transfers from the dataset that you are working with since it can heavily influence the results in either direction. Note that token_id and ' erc_standard will be null if tokens with different tokens IDs or erc type are transferred within the same transaction.","title":"Bundle Trade"},{"location":"reference/tables/spells/nft.trades/#aggregator-trade","text":"There can also be trades in which a single trade transaction contains multiple items, especially when using NFT aggregator platforms. Our approach is to unravel aggregator trades so that each row correspond to a unique item that was traded, with its associated ID, price, collection, etc. Importantly, the trade_type will be indicated as Aggregator Trade , and platform names and address can be found in the nft.aggregators table . If your aggregator platform is missing in that table, you are welcome to make a PR to add it.","title":"Aggregator Trade"},{"location":"reference/tables/spells/nft.trades/#platform-and-royalty-fees","text":"In the most recent version of nft.trades , information about the amount and percent of royalty fees in the original amount and in USD is available when this information was able to be retrieved. Royalty fees are going to the creator, and Platform fees are collected by the NFT platform. Note that royalty fees cannot always be retrieved, and are set to null by default.","title":"Platform and Royalty Fees"},{"location":"reference/tables/spells/nft.trades/#examples","text":"","title":"Examples"},{"location":"reference/tables/spells/nft.trades/#queries","text":"","title":"Queries"},{"location":"reference/tables/spells/nft.trades/#all-trades-for-a-given-nft","text":"SQL select * from nft . trades where nft_contract_address = '\\xb47e3cd837ddf8e4c57f05d70ab865de6e193bbb' --this is the cryptopunks address Results","title":"All trades for a given NFT"},{"location":"reference/tables/spells/nft.trades/#trades-in-the-last-24-hour-on-a-given-platform","text":"SQL select date_trunc ( 'day' , block_time ), usd_amount , nft_contract_address , token_id from nft . trades where platform = 'OpenSea' --only shows trades on given Platform and block_time > now () - interval '24hours' Results","title":"Trades in the last 24 hour on a given platform"},{"location":"reference/tables/spells/nft.trades/#platform-volumes-in-the-last-year","text":"SQL select sum ( usd_amount ), date_trunc ( 'day' , block_time ) as day , platform from nft . trades where block_time > now () - interval '365 days' group by platform , day Results","title":"Platform volumes in the last year"},{"location":"reference/tables/spells/nft.trades/#dashboards","text":"","title":"Dashboards"},{"location":"reference/tables/spells/nft.trades/#that-utilize-parameters","text":"NFT by @0xBoxer NFT Sales Overview by Project by @rantum","title":"That utilize parameters"},{"location":"reference/tables/spells/nft.trades/#that-look-across-the-entire-ecosystem","text":"NFT Collection Dashboard by @rantum NFT by @sealaunch","title":"That look across the entire Ecosystem"},{"location":"reference/tables/spells/nft.trades/#column-data","text":"Column name Data type Description block_time timestamp with time zone When was this trade executed block_time text NFT project name (e.g. \"the dudes\") nft_token_id text The token_id that was traded (e.g. 235) erc_standard text The Token Standard of the traded token ERC-721 or ERC-1155 platform text Which Platform the trade was executed on platform_version text Which version of this platform was utilized? trade_type text \"Single Item Sale\" or \"Bundle Sale\" number_of_items integer How many NFTs were included in this trade category text Was this an auction or a direct sale evt_type text Currently not in use, default 'Trade' aggregator text Was this trade made using an aggregator (Yes : Name of aggregator, No : Null) usd_amount numeric USD value of the trade at time of execution seller bytea Seller of NFTs buyer bytea Buyer of NFTs original_amount numeric The amount in the right format original_amount_raw numeric Raw amount of the currency eth_amount numeric ETH value of the trade at time of execution royalty_fees_percent numeric Royalty fees going to the creator (in %) original_royalty_fees numeric Royalty fees in the currency used for this trade usd_royalty_fees numeric USD value of royalty fees at time of execution platform_fees_percent numeric Platform fees (in %) original_platform_fees numeric Platform fees in the currency used for this trade usd_platform_fees numeric USD value of platform fees at time of execution original_currency text The Currency used for this trade original_currency_contract bytea The ERC-20 address of the currency used in this trade (does not work with raw ETH) currency_contract bytea The corrected currency contract nft_contract_address bytea The contract address of the NFT traded exchange_contract_address bytea The platform contract that facilitated this trade tx_hash bytea The hash of this transaction block_number integer The block_number that this trade was done in tx_from bytea Initiated this transaction tx_to bytea Received this transaction trace_address ARRAY n/a evt_index integer Event index trade_id integer n/a","title":"Column Data"},{"location":"reference/tables/spells/nft.trades/#ser-my-platform-is-not-indexed","text":"The SQL code that processes the data for every market place is open source and available in our github repository . Everyone can review the code, make pull requests and submit code to add more marketplaces.","title":"Ser, my platform is not indexed"},{"location":"reference/tables/spells/prices/","text":"We pull price data from the coinpaprika API. The Price is the volume-weighted price based on real-time market data, translated to USD. prices.usd \u00b6 This table supports a range of erc20.tokens. If the token you desire is not listed in here, please make a pull request to our GitHub repository . (For V1 Engine, you can also use the decentralized price feed dex.view_token_prices. ) Column name Data type Description contract_address bytea string the contract address of the erc20 token symbol string the identifier of the asset (ticker, cashtag) price numeric The price of the asset in any given minute minute timestampz The resolution for this table is by minute Note that WETH can be used for ETH price as it trades at virtually the same price. How we get prices from DEXs \u00b6 We created a table that creates price feeds based on decentralized exchange trading data. This table covers much more assets than prices.usd , since it covers all assets that are traded on any of the decentralized exchanges that are indexed in dex.trades . Please keep in mind that this script can generate wrong prices in rare cases. This table is very resource intensive and can therefore only be updated every few hours, please keep that in mind when utilizing it. **** Also the resolution is only hourly, so if you need minutely prices do refer to prices.usd . This table currently only exists for Ethereum on our old database architecture. The logic of how this table works can be accessed in our public github repo. This script generates median hourly prices based on data from decentralized exchanges found in dex.trades . It will assign asset prices based on a trading pair which has a pricefeed in prices.usd . Let's take the $SPELL/ETH Pool for example. $ETH price is contained in prices.usd $SPELL price is not contained in prices.usd In order to get the $SPELL price, the script will dynamically calculate the price of $SPELL based on the price of $ETH that was exchanged for it. e.g. 5 $ETH were exchanged for 1,086,083 $SPELL. Dex.trades will assign a usd_amount to this trade based on the $ETH price data in prices.usd . That usd_amount is $23,498. 5 * price of ETH (4.699,6) = $23,498 Calculating the price of $SPELL is now as simple as dividing the amount of tokens exchanged with the usd_amount recorded in dex.trades . $23,498/1,086,083 \u2248 $0,02163 We now have successfully calculated the price of 1 $SPELL. In order to correct for extreme outliers and in order for this table to be performant the script then aggregates all recorded data into one median_price per hour. Known issues \u00b6 In rare cases this script will generate price feeds that are based on illiquid pairs and therefore report wrong data. This happens when all liquid trading pools of this token do not have a price feed in prices.usd . An example of this would be $PLAY, a metaverse index from PieDAO. The liquid trading pair for this asset is \\(PLAY/\\) DOUGH. The \"correct\" price of $PLAY is represented in this pool, but the combination of dex.trades and prices.prices_from_dex_data are not able to pick up this price. Instead, dex.trades will only have a usd_amount for illiquid pairs of this asset. In this case, the \\(PLAY/\\) ETH pool has trades once in a while and these will have a usd_amount in dex.trades . The liquidity of the \\(PLAY/\\) ETH pool is very low and it pretty much only consists of arbitrage trades. Therefore, the resulting pricefeed in prices.prices_from_dex_data is faulty since it depends on the usd_amount in dex.trades . In order to check for this, you should manually verify the results of prices.prices_from_dex_data in order to make sure arbitrage trades do not disturb the price feed constructed. A simple way of validating that the script is working with the right pools is checking the sample_size column. If the number seems suspiciously low, the script probably doesn't pick up the right price. In cases like this, you have to manually construct a price feed. We are always looking to improve this table, if you have any ideas or comments don't hesitate to open a PR or contact us in our Discord.","title":"Prices"},{"location":"reference/tables/spells/prices/#pricesusd","text":"This table supports a range of erc20.tokens. If the token you desire is not listed in here, please make a pull request to our GitHub repository . (For V1 Engine, you can also use the decentralized price feed dex.view_token_prices. ) Column name Data type Description contract_address bytea string the contract address of the erc20 token symbol string the identifier of the asset (ticker, cashtag) price numeric The price of the asset in any given minute minute timestampz The resolution for this table is by minute Note that WETH can be used for ETH price as it trades at virtually the same price.","title":"prices.usd"},{"location":"reference/tables/spells/prices/#how-we-get-prices-from-dexs","text":"We created a table that creates price feeds based on decentralized exchange trading data. This table covers much more assets than prices.usd , since it covers all assets that are traded on any of the decentralized exchanges that are indexed in dex.trades . Please keep in mind that this script can generate wrong prices in rare cases. This table is very resource intensive and can therefore only be updated every few hours, please keep that in mind when utilizing it. **** Also the resolution is only hourly, so if you need minutely prices do refer to prices.usd . This table currently only exists for Ethereum on our old database architecture. The logic of how this table works can be accessed in our public github repo. This script generates median hourly prices based on data from decentralized exchanges found in dex.trades . It will assign asset prices based on a trading pair which has a pricefeed in prices.usd . Let's take the $SPELL/ETH Pool for example. $ETH price is contained in prices.usd $SPELL price is not contained in prices.usd In order to get the $SPELL price, the script will dynamically calculate the price of $SPELL based on the price of $ETH that was exchanged for it. e.g. 5 $ETH were exchanged for 1,086,083 $SPELL. Dex.trades will assign a usd_amount to this trade based on the $ETH price data in prices.usd . That usd_amount is $23,498. 5 * price of ETH (4.699,6) = $23,498 Calculating the price of $SPELL is now as simple as dividing the amount of tokens exchanged with the usd_amount recorded in dex.trades . $23,498/1,086,083 \u2248 $0,02163 We now have successfully calculated the price of 1 $SPELL. In order to correct for extreme outliers and in order for this table to be performant the script then aggregates all recorded data into one median_price per hour.","title":"How we get prices from DEXs"},{"location":"reference/tables/spells/prices/#known-issues","text":"In rare cases this script will generate price feeds that are based on illiquid pairs and therefore report wrong data. This happens when all liquid trading pools of this token do not have a price feed in prices.usd . An example of this would be $PLAY, a metaverse index from PieDAO. The liquid trading pair for this asset is \\(PLAY/\\) DOUGH. The \"correct\" price of $PLAY is represented in this pool, but the combination of dex.trades and prices.prices_from_dex_data are not able to pick up this price. Instead, dex.trades will only have a usd_amount for illiquid pairs of this asset. In this case, the \\(PLAY/\\) ETH pool has trades once in a while and these will have a usd_amount in dex.trades . The liquidity of the \\(PLAY/\\) ETH pool is very low and it pretty much only consists of arbitrage trades. Therefore, the resulting pricefeed in prices.prices_from_dex_data is faulty since it depends on the usd_amount in dex.trades . In order to check for this, you should manually verify the results of prices.prices_from_dex_data in order to make sure arbitrage trades do not disturb the price feed constructed. A simple way of validating that the script is working with the right pools is checking the sample_size column. If the number seems suspiciously low, the script probably doesn't pick up the right price. In cases like this, you have to manually construct a price feed. We are always looking to improve this table, if you have any ideas or comments don't hesitate to open a PR or contact us in our Discord.","title":"Known issues"},{"location":"reference/tables/spells/token-standards/","text":"When we\u2019re interested in a subset of events logs fired regardless of the origin contract, Dune uses interface-decoding. Notable examples include ERC20 , ERC721 and ERC1155 transfer events. This method is reserved for special cases. These tables make it easy to keep track of fungible and non fungible tokens flowing in and out of contracts and wallets and are widely used across Dune. You can read more about the individual token standards and the tables here: ERC-20 ERC-721 ERC-1155","title":"Token Standards"},{"location":"reference/tables/spells/token-standards/erc-1155/","text":"The ERC1155 standard interface is a multi token standard. It can include any combination of fungible and non-fungible tokens and can therefore be used for a wide range of use cases. Most commonly ERC1155 smart contracts are used today to reflect in-game items and currencies, digital collectibles, art and membership passes, but they really can be used for anything since they combine all properties of ERC20 and ERC721 tokens in a single smart contract. Using a single smart contract for all types of tokens that may exist within a given project saves on gas and complexity during usage and deployment. Each id in an ERC1155 smart contract can have a different value of tokens that exist. A non-fungible token will simply have the value 1. Fungible tokens will have a value > 1 . All ERC1155 tokens are distinct and lack the decimal property of ERC20 tokens. In usage, this lack of decimal property can easily be corrected for in the frontend of where ever you are dealing with these tokens. For more reading, check out the proposal for the standard , the ethereum.org docs or the open Zeppelin docs . Methods \u00b6 /* transfers a quantity(_value) of a specific token type(_id) from an address(_from) to a receiver(_to)*/ function safeTransferFrom ( address _from , address _to , uint256 _id , uint256 _value , bytes calldata _data ) external ; /* transfers multiple token types in different quantities as specified in the _ids and _values arrays. Can only have one sender and one receiver.*/ function safeBatchTransferFrom ( address _from , address _to , uint256 [] calldata _ids , uint256 [] calldata _values , bytes calldata _data ) external ; /* returns the quantity of tokens for a specific token type for a specific owner */ function balanceOf ( address _owner , uint256 _id ) external view returns ( uint256 ); /*returns the balance of multiple owners and multiple token types */ function balanceOfBatch ( address [] calldata _owners , uint256 [] calldata _ids ) external view returns ( uint256 [] memory ); /* sets the approval for a certain operator for all token types(Ids) */ function setApprovalForAll ( address _operator , bool _approved ) external ; /* returns whether a operator is approved for all */ function isApprovedForAll ( address _owner , address _operator ) external view returns ( bool ); /* returns the uri of the specified id */ function uri ( uint256 _id ) external view returns ( string memory ); Events \u00b6 /* gets emitted after a successful safeTransferFrom call */ event TransferSingle ( address indexed _operator , address indexed _from , address indexed _to , uint256 _id , uint256 _value ); /* gets emitted after a successful safeBtachTransferFrom call */ event TransferBatch ( address indexed _operator , address indexed _from , address indexed _to , uint256 [] _ids , uint256 [] _values ); /* gets emitted after a successful setApprovalForAll call */ event ApprovalForAll ( address indexed _owner , address indexed _operator , bool _approved ); /* gets emitted when the URI gets changed */ event URI ( string _value , uint256 indexed _id ); Tables \u00b6 We decode all events of all ERC1155 contracts into the respective event tables. V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) erc1155_blockchain.ERC1155_evt_Transfer_Single This event gets emitted when a single type of token within one erc1155 contract is transferred using the safeTransferFrom function. Column name Data type Description operator string The address of the account/contract that is approved to make the transfer and has initiated this transaction from string The sender of the ERC1155 token to string The receiver of the ERC1155 token id numeric The Token ID property of this class of token value numeric The amount of tokens that have been transferred for a specific id contract_address string The contract_address of this ERC1155 smart contract evt_tx_hash string The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc1155_blockchain.ERC1155_evt_TransferBatch This event gets emitted when multiple types of tokens within one erc1155 contract are transferred using the safeBatchTransferFrom function. Column name Data type Description operator string The address of the account/contract that is approved to make the transfer and has initiated this transaction from string The sender of the ERC1155 token to string The receiver of the ERC1155 token id ARRAY An array that contains the token id properties of the tokens that have been moved in this transaction value ARRAY An array that contains the token value properties of the tokens that have been moved in this transaction contract_address string The contract_address of this ERC1155 smart contract evt_tx_hash string The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc1155_blockchain.ERC1155_evt_URI This event gets emitted when the URI of a token type gets changed. This can be done using multiple functions, as long as the conform to the Column name Data type Description value text The new URI id numeric The id of the token's URI that got changed contract_address string The contract_address of this ERC1155 smart contract evt_tx_hash string The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc1155.\"ERC1155_evt_Transfer_Single\" This event gets emitted when a single type of token within one erc1155 contract is transferred using the safeTransferFrom function. Column name Data type Description operator bytea The address of the account/contract that is approved to make the transfer and has initiated this transaction from bytea The sender of the ERC1155 token to bytea The receiver of the ERC1155 token id numeric The Token ID property of this class of token value numeric The amount of tokens that have been transferred for a specific id contract_address bytea The contract_address of this ERC1155 smart contract evt_tx_hash bytea The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc1155.\"ERC1155_evt_TransferBatch\" This event gets emitted when multiple types of tokens within one erc1155 contract are transferred using the safeBatchTransferFrom function. Column name Data type Description operator bytea The address of the account/contract that is approved to make the transfer and has initiated this transaction from bytea The sender of the ERC1155 token to bytea The receiver of the ERC1155 token id ARRAY An array that contains the token id properties of the tokens that have been moved in this transaction value ARRAY An array that contains the token value properties of the tokens that have been moved in this transaction contract_address bytea The contract_address of this ERC1155 smart contract evt_tx_hash bytea The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain ERC1155_evt_URI This event gets emitted when the URI of a token type gets changed. This can be done using multiple functions, as long as the conform to the Column name Data type Description value text The new URI id numeric The id of the token's URI that got changed contract_address bytea The contract_address of this ERC1155 smart contract evt_tx_hash bytea The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain","title":"ERC-1155"},{"location":"reference/tables/spells/token-standards/erc-1155/#methods","text":"/* transfers a quantity(_value) of a specific token type(_id) from an address(_from) to a receiver(_to)*/ function safeTransferFrom ( address _from , address _to , uint256 _id , uint256 _value , bytes calldata _data ) external ; /* transfers multiple token types in different quantities as specified in the _ids and _values arrays. Can only have one sender and one receiver.*/ function safeBatchTransferFrom ( address _from , address _to , uint256 [] calldata _ids , uint256 [] calldata _values , bytes calldata _data ) external ; /* returns the quantity of tokens for a specific token type for a specific owner */ function balanceOf ( address _owner , uint256 _id ) external view returns ( uint256 ); /*returns the balance of multiple owners and multiple token types */ function balanceOfBatch ( address [] calldata _owners , uint256 [] calldata _ids ) external view returns ( uint256 [] memory ); /* sets the approval for a certain operator for all token types(Ids) */ function setApprovalForAll ( address _operator , bool _approved ) external ; /* returns whether a operator is approved for all */ function isApprovedForAll ( address _owner , address _operator ) external view returns ( bool ); /* returns the uri of the specified id */ function uri ( uint256 _id ) external view returns ( string memory );","title":"Methods"},{"location":"reference/tables/spells/token-standards/erc-1155/#events","text":"/* gets emitted after a successful safeTransferFrom call */ event TransferSingle ( address indexed _operator , address indexed _from , address indexed _to , uint256 _id , uint256 _value ); /* gets emitted after a successful safeBtachTransferFrom call */ event TransferBatch ( address indexed _operator , address indexed _from , address indexed _to , uint256 [] _ids , uint256 [] _values ); /* gets emitted after a successful setApprovalForAll call */ event ApprovalForAll ( address indexed _owner , address indexed _operator , bool _approved ); /* gets emitted when the URI gets changed */ event URI ( string _value , uint256 indexed _id );","title":"Events"},{"location":"reference/tables/spells/token-standards/erc-1155/#tables","text":"We decode all events of all ERC1155 contracts into the respective event tables. V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) erc1155_blockchain.ERC1155_evt_Transfer_Single This event gets emitted when a single type of token within one erc1155 contract is transferred using the safeTransferFrom function. Column name Data type Description operator string The address of the account/contract that is approved to make the transfer and has initiated this transaction from string The sender of the ERC1155 token to string The receiver of the ERC1155 token id numeric The Token ID property of this class of token value numeric The amount of tokens that have been transferred for a specific id contract_address string The contract_address of this ERC1155 smart contract evt_tx_hash string The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc1155_blockchain.ERC1155_evt_TransferBatch This event gets emitted when multiple types of tokens within one erc1155 contract are transferred using the safeBatchTransferFrom function. Column name Data type Description operator string The address of the account/contract that is approved to make the transfer and has initiated this transaction from string The sender of the ERC1155 token to string The receiver of the ERC1155 token id ARRAY An array that contains the token id properties of the tokens that have been moved in this transaction value ARRAY An array that contains the token value properties of the tokens that have been moved in this transaction contract_address string The contract_address of this ERC1155 smart contract evt_tx_hash string The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc1155_blockchain.ERC1155_evt_URI This event gets emitted when the URI of a token type gets changed. This can be done using multiple functions, as long as the conform to the Column name Data type Description value text The new URI id numeric The id of the token's URI that got changed contract_address string The contract_address of this ERC1155 smart contract evt_tx_hash string The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc1155.\"ERC1155_evt_Transfer_Single\" This event gets emitted when a single type of token within one erc1155 contract is transferred using the safeTransferFrom function. Column name Data type Description operator bytea The address of the account/contract that is approved to make the transfer and has initiated this transaction from bytea The sender of the ERC1155 token to bytea The receiver of the ERC1155 token id numeric The Token ID property of this class of token value numeric The amount of tokens that have been transferred for a specific id contract_address bytea The contract_address of this ERC1155 smart contract evt_tx_hash bytea The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc1155.\"ERC1155_evt_TransferBatch\" This event gets emitted when multiple types of tokens within one erc1155 contract are transferred using the safeBatchTransferFrom function. Column name Data type Description operator bytea The address of the account/contract that is approved to make the transfer and has initiated this transaction from bytea The sender of the ERC1155 token to bytea The receiver of the ERC1155 token id ARRAY An array that contains the token id properties of the tokens that have been moved in this transaction value ARRAY An array that contains the token value properties of the tokens that have been moved in this transaction contract_address bytea The contract_address of this ERC1155 smart contract evt_tx_hash bytea The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain ERC1155_evt_URI This event gets emitted when the URI of a token type gets changed. This can be done using multiple functions, as long as the conform to the Column name Data type Description value text The new URI id numeric The id of the token's URI that got changed contract_address bytea The contract_address of this ERC1155 smart contract evt_tx_hash bytea The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain","title":"Tables"},{"location":"reference/tables/spells/token-standards/erc-20/","text":"The ERC20 standard is the token standard that is used for fungible assets on all EVM blockchains. ERC20s can represent anything from magic internet points to USD to gold tokens. The ERC-20 standard was proposed by Fabian Vogelsteller in November 2015 and represents the first token standard that implements an API for tokens within Smart Contracts. This standardization of smart contracts solves the issue of interoperability with other applications on the blockchain. Since all tokens share the same interface, other smart contracts are easily able to interact with them. A smart contract that contains the erc20 token standard is not limited to only having these functions, but it needs to contain these functions to be within the standard. For more reading check out the initial proposal or the ethereum.org documentation . Please note that Binance Smart Chain chose to rename ERC to BEP, this is reflected in out tables. Methods \u00b6 /* returns the full name of this token */ function name () public view returns ( _string_ ) /* returns the ticker of this token */ function symbol () public view returns ( _string_ ) /* returns the amount of decimals of this token */ function decimals () public view returns ( u_int8_ ) /* returns the current circulating supply of this token */ function totalSupply () public view returns ( uint256 ) /* returns the balance of the specified address */ function balanceOf ( address _owner ) public view returns ( uint256 balance ) /* is used to transfer the specified quantity(_value) of tokens to the address specified*/ function transfer ( address _to , uint256 _value ) public returns ( bool success ) /* is used when a third address has permission to move tokens from the sender(_from) to the receiver(_to)*/ function transferFrom ( address _from , address _to , uint256 _value ) public returns ( bool success ) /* is used to approve a spender for a specific quantity(_value) of tokens*/ function approve ( address _spender , uint256 _value ) public returns ( bool success ) /* returns the quantity(_value) of tokens that the spender is still allowed to spend from the owners address*/ function allowance ( address _owner , address _spender ) public view returns ( uint256 remaining ) Events \u00b6 /* gets emitted upon successful transfer of tokens*/ event Transfer ( address indexed _from , address indexed _to , uint256 _value ) /* gets emitted upon successful approval of an owner address with an allowed quantity(_value) of tokens that can be moved*/ event Approval ( address indexed _owner , address indexed _spender , uint256 _value ) Tables \u00b6 In Dune, we decode all transfer events across all smart contracts that use the ERC20 token standard into these tables: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) erc20_blockchain.ERC20_evt_Transfer This is the event that gets emitted upon successful transfer of a token within a ERC20 smart contract. This can be triggered by calling the transfer or transferFrom function. Column name Data type Description from string The sender of the ERC20 token to string The receiver of the ERC20 token value numeric The amount of ERC20 tokens sent. Notice that you have to divide this by the relevant decimals of the ERC20 token to get to the commonly used denomination of this token contract_address string The contract_address of the ERC20 token evt_tx_hash string The hash of the transaction in which this log is contained evt_index numeric This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamptz The time when the block was mined that includes this log evt_block_number int8 The block_number of the block that includes this log erc20_blockchain.ERC20_evt_Approval ERC20 tokens can be moved by other smart contracts. In order to allow this action, user will call the approve function first. Should that transaction complete successfully, the Approval event will get emitted. Column name Data type Description owner string The address giving the approval spender string The address which has approval to move the tokens value numeric The spending limit contract_address string The contract address of the ERC20 token that can be moved evt_tx_hash string The hash of the transaction in which this log is contained evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time when the block was mined that includes this log evt_block_number bigint The block_number of the block that includes this log erc20.\"ERC20_evt_Transfer\" This is the event that gets emitted upon successful transfer of a token within a ERC20 smart contract. This can be triggered by calling the transfer or transferFrom function. Column name Data type Description from bytea the sender of the ERC20 token to bytea the receiver of the ERC20 token value numeric the amount of ERC20 tokens sent. Notice that you have to divide this by the relevant decimals of the ERC20 token to get to the commonly used denomination of this token. contract_address bytea the contract_address of the ERC20 token evt_tx_hash bytea the hash of the transaction in which this log is contained evt_index numeric this logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamptz the time when the block was mined that includes this log evt_block_number int8 the block_number of the block that includes this log erc20.\"ERC20_evt_Approval\" ERC20 tokens can be moved by other smart contracts. In order to allow this action, user will call the approve function first. Should that transaction complete successfully, the Approval event will get emitted. Column name Data type Description owner bytea the address giving the approval spender bytea the address which has approval to move the tokens value numeric the spending limit contract_address bytea the contract address of the erc20 token that can be moved evt_tx_hash bytea the hash of the transaction in which this log is contained evt_index bigint this logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone the time when the block was mined that includes this log evt_block_number bigint the block_number of the block that includes this log","title":"ERC-20"},{"location":"reference/tables/spells/token-standards/erc-20/#methods","text":"/* returns the full name of this token */ function name () public view returns ( _string_ ) /* returns the ticker of this token */ function symbol () public view returns ( _string_ ) /* returns the amount of decimals of this token */ function decimals () public view returns ( u_int8_ ) /* returns the current circulating supply of this token */ function totalSupply () public view returns ( uint256 ) /* returns the balance of the specified address */ function balanceOf ( address _owner ) public view returns ( uint256 balance ) /* is used to transfer the specified quantity(_value) of tokens to the address specified*/ function transfer ( address _to , uint256 _value ) public returns ( bool success ) /* is used when a third address has permission to move tokens from the sender(_from) to the receiver(_to)*/ function transferFrom ( address _from , address _to , uint256 _value ) public returns ( bool success ) /* is used to approve a spender for a specific quantity(_value) of tokens*/ function approve ( address _spender , uint256 _value ) public returns ( bool success ) /* returns the quantity(_value) of tokens that the spender is still allowed to spend from the owners address*/ function allowance ( address _owner , address _spender ) public view returns ( uint256 remaining )","title":"Methods"},{"location":"reference/tables/spells/token-standards/erc-20/#events","text":"/* gets emitted upon successful transfer of tokens*/ event Transfer ( address indexed _from , address indexed _to , uint256 _value ) /* gets emitted upon successful approval of an owner address with an allowed quantity(_value) of tokens that can be moved*/ event Approval ( address indexed _owner , address indexed _spender , uint256 _value )","title":"Events"},{"location":"reference/tables/spells/token-standards/erc-20/#tables","text":"In Dune, we decode all transfer events across all smart contracts that use the ERC20 token standard into these tables: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) erc20_blockchain.ERC20_evt_Transfer This is the event that gets emitted upon successful transfer of a token within a ERC20 smart contract. This can be triggered by calling the transfer or transferFrom function. Column name Data type Description from string The sender of the ERC20 token to string The receiver of the ERC20 token value numeric The amount of ERC20 tokens sent. Notice that you have to divide this by the relevant decimals of the ERC20 token to get to the commonly used denomination of this token contract_address string The contract_address of the ERC20 token evt_tx_hash string The hash of the transaction in which this log is contained evt_index numeric This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamptz The time when the block was mined that includes this log evt_block_number int8 The block_number of the block that includes this log erc20_blockchain.ERC20_evt_Approval ERC20 tokens can be moved by other smart contracts. In order to allow this action, user will call the approve function first. Should that transaction complete successfully, the Approval event will get emitted. Column name Data type Description owner string The address giving the approval spender string The address which has approval to move the tokens value numeric The spending limit contract_address string The contract address of the ERC20 token that can be moved evt_tx_hash string The hash of the transaction in which this log is contained evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time when the block was mined that includes this log evt_block_number bigint The block_number of the block that includes this log erc20.\"ERC20_evt_Transfer\" This is the event that gets emitted upon successful transfer of a token within a ERC20 smart contract. This can be triggered by calling the transfer or transferFrom function. Column name Data type Description from bytea the sender of the ERC20 token to bytea the receiver of the ERC20 token value numeric the amount of ERC20 tokens sent. Notice that you have to divide this by the relevant decimals of the ERC20 token to get to the commonly used denomination of this token. contract_address bytea the contract_address of the ERC20 token evt_tx_hash bytea the hash of the transaction in which this log is contained evt_index numeric this logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamptz the time when the block was mined that includes this log evt_block_number int8 the block_number of the block that includes this log erc20.\"ERC20_evt_Approval\" ERC20 tokens can be moved by other smart contracts. In order to allow this action, user will call the approve function first. Should that transaction complete successfully, the Approval event will get emitted. Column name Data type Description owner bytea the address giving the approval spender bytea the address which has approval to move the tokens value numeric the spending limit contract_address bytea the contract address of the erc20 token that can be moved evt_tx_hash bytea the hash of the transaction in which this log is contained evt_index bigint this logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone the time when the block was mined that includes this log evt_block_number bigint the block_number of the block that includes this log","title":"Tables"},{"location":"reference/tables/spells/token-standards/erc-721/","text":"The ERC721 standard is the token standard that is used for non-fungible assets on all EVM blockchains. ERC721s can represent digital art, a deed to a house, an insurance policy or even a uniswap V3 liquidity position. The shared property of all these things is that they are not fungible and therefore unique. ERC721 tokens are uniquely identified by a combination of contract_address and tokenId . There only ever can exist one of each tokenId within each smart contract. an optional feature of the interface is a tokenURI property in which the associated media content of the tokenId within this smart contract is stored. This mostly is links to decentralized storage solutions like ifps . The standard contains many functions similar to ERC20 tokens as it was influenced by it, but does expand on the functionality **** due to the added complexity regarding tokenId . Methods \u00b6 /* returns the amount of tokens a certain address(_owner) contains */ function balanceOf ( address _owner ) external view returns ( uint256 ); /* returns the owner of a specific NFT, identified by it's tokenId*/ function ownerOf ( uint256 _tokenId ) external view returns ( address ); /* is used to transfer a specific token from the sender(_from) to the receiver(_to)*/ function safeTransferFrom ( address _from , address _to , uint256 _tokenId ) external payable ; /* is used to transfer a specific token from the sender(_from) to the receiver(_to)*/ function transferFrom ( address _from , address _to , uint256 _tokenId ) external payable ; /* is used to allow a certain address to move a specific tokenID */ function approve ( address _approved , uint256 _tokenId ) external payable ; /* is used to allow a certain address to move any tokenID */ function setApprovalForAll ( address _operator , bool _approved ) external ; /* returns which address is currently able to move this token. Only one can be active at a time.*/ function getApproved ( uint256 _tokenId ) external view returns ( address ); /* Tells whether an operator is approved by a given owner. */ function isApprovedForAll ( address _owner , address _operator ) external view returns ( bool ); Events \u00b6 /* gets emitted upon successful transfer of a token. Only one unit can ever be transferred at a time*/ event Transfer ( address _from , address _to , uint256 _tokenId ); /* gets emitted upon the approval of an operator for a specific tokenId */ event Approval ( address _owner , address _approved , uint256 _tokenId ); /* gets emitted upon the approval of an operator for all tokens of this collection */ event ApprovalForAll ( address indexed _owner , address indexed _operator , bool _approved ); Tables \u00b6 In Dune, we decode all erc721 transfer events from all smart contracts into into these tables: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) erc721_blockchain.ERC721_evt_Transfer This is the event that gets emitted upon successful transfer of a token within a ERC721 smart contract. This can be triggered by either the safeTransferFrom or the transferFrom functions. Column name Data type Description from string The sender of the ERC721 token to string The receiver of the ERC721 token tokenID numeric The Token ID which uniquely identifies this NFT contract_address string The contract_address of this ERC721 token evt_tx_hash string The transaction hash in which this event got emitted evt_index numeric This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamptz The time at which the block that contains this event log got mined evt_block_number int8 The length of the blockchain erc721_blockchain.ERC721_evt_Approval This is the event that gets emitted upon a successful call of the approve function. Column name Data type Description owner string The address which gives permission to move this token approved string The operator which is able to move the specified tokenId tokenId numeric The tokenId in question contract_address string The contract_address of this ERC721 token evt_tx_hash string The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc721.\"ERC721_evt_Transfer\" This is the event that gets emitted upon successful transfer of a token within a ERC721 smart contract. This can be triggered by either the safeTransferFrom or the transferFrom functions. Column name Data type Description from bytea The sender of the ERC721 token to bytea The receiver of the ERC721 token tokenID numeric The Token ID which uniquely identifies this NFT contract_address bytea The contract_address of this ERC721 token evt_tx_hash bytea The transaction hash in which this event got emitted evt_index numeric This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamptz The time at which the block that contains this event log got mined evt_block_number int8 The length of the blockchain erc721.\"ERC721_evt_Approval\" This is the event that gets emitted upon a successful call of the approve function. Column name Data type Description owner bytea The address which gives permission to move this token approved bytea The operator which is able to move the specified tokenId tokenId numeric The tokenId in question contract_address bytea The contract_address of this ERC721 token evt_tx_hash bytea The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain","title":"ERC-721"},{"location":"reference/tables/spells/token-standards/erc-721/#methods","text":"/* returns the amount of tokens a certain address(_owner) contains */ function balanceOf ( address _owner ) external view returns ( uint256 ); /* returns the owner of a specific NFT, identified by it's tokenId*/ function ownerOf ( uint256 _tokenId ) external view returns ( address ); /* is used to transfer a specific token from the sender(_from) to the receiver(_to)*/ function safeTransferFrom ( address _from , address _to , uint256 _tokenId ) external payable ; /* is used to transfer a specific token from the sender(_from) to the receiver(_to)*/ function transferFrom ( address _from , address _to , uint256 _tokenId ) external payable ; /* is used to allow a certain address to move a specific tokenID */ function approve ( address _approved , uint256 _tokenId ) external payable ; /* is used to allow a certain address to move any tokenID */ function setApprovalForAll ( address _operator , bool _approved ) external ; /* returns which address is currently able to move this token. Only one can be active at a time.*/ function getApproved ( uint256 _tokenId ) external view returns ( address ); /* Tells whether an operator is approved by a given owner. */ function isApprovedForAll ( address _owner , address _operator ) external view returns ( bool );","title":"Methods"},{"location":"reference/tables/spells/token-standards/erc-721/#events","text":"/* gets emitted upon successful transfer of a token. Only one unit can ever be transferred at a time*/ event Transfer ( address _from , address _to , uint256 _tokenId ); /* gets emitted upon the approval of an operator for a specific tokenId */ event Approval ( address _owner , address _approved , uint256 _tokenId ); /* gets emitted upon the approval of an operator for all tokens of this collection */ event ApprovalForAll ( address indexed _owner , address indexed _operator , bool _approved );","title":"Events"},{"location":"reference/tables/spells/token-standards/erc-721/#tables","text":"In Dune, we decode all erc721 transfer events from all smart contracts into into these tables: V2 Engine (Databricks SQL) V1 Engine (PosgreSQL) erc721_blockchain.ERC721_evt_Transfer This is the event that gets emitted upon successful transfer of a token within a ERC721 smart contract. This can be triggered by either the safeTransferFrom or the transferFrom functions. Column name Data type Description from string The sender of the ERC721 token to string The receiver of the ERC721 token tokenID numeric The Token ID which uniquely identifies this NFT contract_address string The contract_address of this ERC721 token evt_tx_hash string The transaction hash in which this event got emitted evt_index numeric This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamptz The time at which the block that contains this event log got mined evt_block_number int8 The length of the blockchain erc721_blockchain.ERC721_evt_Approval This is the event that gets emitted upon a successful call of the approve function. Column name Data type Description owner string The address which gives permission to move this token approved string The operator which is able to move the specified tokenId tokenId numeric The tokenId in question contract_address string The contract_address of this ERC721 token evt_tx_hash string The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain erc721.\"ERC721_evt_Transfer\" This is the event that gets emitted upon successful transfer of a token within a ERC721 smart contract. This can be triggered by either the safeTransferFrom or the transferFrom functions. Column name Data type Description from bytea The sender of the ERC721 token to bytea The receiver of the ERC721 token tokenID numeric The Token ID which uniquely identifies this NFT contract_address bytea The contract_address of this ERC721 token evt_tx_hash bytea The transaction hash in which this event got emitted evt_index numeric This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamptz The time at which the block that contains this event log got mined evt_block_number int8 The length of the blockchain erc721.\"ERC721_evt_Approval\" This is the event that gets emitted upon a successful call of the approve function. Column name Data type Description owner bytea The address which gives permission to move this token approved bytea The operator which is able to move the specified tokenId tokenId numeric The tokenId in question contract_address bytea The contract_address of this ERC721 token evt_tx_hash bytea The transaction hash in which this event got emitted evt_index bigint This logs index position in the block (cumulative amount of logs ordered by execution) evt_block_time timestamp with time zone The time at which the block that contains this event log got mined evt_block_number bigint The length of the blockchain","title":"Tables"},{"location":"reference/wizard-tools/","text":"Here are some of the awesome non-Dune tools our wizards use to make \ud83c\udf87. OpenZepplin's ERC-xxx Standards Docs are helpful for learning how each ERC-xxx standard works/ The EVM Handbook is a curated list of EVM resources for aspiring blockchain data analysts. Solidity Visual Developer for VS Code is a helpful suit of tools for smart contract auditing inside of VS code. deth.net is a set of Ethereum developer tools that help with blockchain data sleuthing. ZK Zhao's map of the Crypto Data Landscape is helpful for getting a sense of the current crypto data ecosystem His Chaininsight tool is pretty cool too, could help you build the next trending dashboard before someone else does \ud83e\uddd9 Our Recommended Reading List for a bunch of guides on crypto data topics and projects to check out. DeepDAO - stats on DAOs and their treasuries. DethCode lets you go into an Ethereum smart contract's code to read the comments or the actual logic can help to understand the smart contract's emitted data.","title":"Wizard Tools"},{"location":"reference/wizard-tools/blockchain-explorers/","text":"Here you'll find the primary web-based explorers for the most popular blockchains. While they won't let you do half of the cool things Dune does, they're indispensible for figuring out how certain protocols work and what data you need to include in your queries: Ethereum - Etherscan Solana - Solscan Arbitrum - Arbiscan Polygon - Polygonscan Optimism - Optimistic.Etherscan Avalanche C-Chain - Snowtrace Binance Smart Chain - BscScan Gnosis Chain - Gnosisscan","title":"Blockchain Explorers"},{"location":"reference/wizard-tools/helpful-dashboards/","text":"Some endlessly amazing Wizards have put together a few dashboards that are super helpful for those looking to become master Wizard too. \ud83e\uddd9 If you find (or make) another Dashboard like these submit a GitHub Pull Request and if we think it's helpful we'll add it to the list! Tutorial Dashboards \u00b6 These dashboards are great for learning new Dune, Data Analytics, and Blockchain Analysis skills! @springzhang 's Tips and Tricks for Dune V2 Queries and Visualizations @1chioku 's Data Visualization Cheat Sheet @agaperste 's Event and Function Signature Sleuthing @dune 's Getting Started @duniversity 's Contract Quickstart Reference Dashboards \u00b6 These dashboards are helpful to refer to while building Queries. @dune 's Meta Monitoring @dune 's Is my Contract Decoded Yet? @subinium 's Dune DB Guide","title":"Helpful Dashboards"},{"location":"reference/wizard-tools/helpful-dashboards/#tutorial-dashboards","text":"These dashboards are great for learning new Dune, Data Analytics, and Blockchain Analysis skills! @springzhang 's Tips and Tricks for Dune V2 Queries and Visualizations @1chioku 's Data Visualization Cheat Sheet @agaperste 's Event and Function Signature Sleuthing @dune 's Getting Started @duniversity 's Contract Quickstart","title":"Tutorial Dashboards"},{"location":"reference/wizard-tools/helpful-dashboards/#reference-dashboards","text":"These dashboards are helpful to refer to while building Queries. @dune 's Meta Monitoring @dune 's Is my Contract Decoded Yet? @subinium 's Dune DB Guide","title":"Reference Dashboards"},{"location":"reference/wizard-tools/utility-queries/","text":"Here you'll find a list of Queries that help you build Queries. \ud83e\ude84 Have a Query you think we should include? Propose your changes here ! Name/Link Tag Is my Contract decoded yet by @0xBoxer decoding Is my Contract decoded yet (Polygon Version) by @0xBoxer decoding Is my Contract decoded yet (xDAI Version) by @0xBoxer decoding Is my Contract decoded yet (BSC Version) by @chenxiangli decoding Getting most common event signatures for a contract by @ilemi query starting point Getting most common functions called by signers for a contract by @ilemi query starting point Getting most common functions called in traces for a contract by @ilemi query starting point Getting all NFTs held by an address (ERC721 + ERC1155) by @ilemi NFT Getting the address from ENS by @hildobby parameters","title":"Utility Queries"},{"location":"spellbook/","text":"Spellbook is a data transformation layer for Dune, built by the community. Spells are recipes to build high level tables that support common use cases, like NFT trades. You write them in SQL, wrapped in a Python templating language called Jinja2. Spellbook automates the build, maintenance and data quality of these tables. Anyone in our community can contribute to our spells, whether that is adding a new exchange or writing an entirely new spell. Note Spellbook is available on our Dune V2 Engine. For abstractions on Dune V1 Engine, see abstractions . New level unlocked. \u00b6 We\u2019ve updated our database and now it\u2019s time to update our contribution tools. On January 31, 2020, we launched the abstractions repo as a place for wizards to create views and later tables. Since then, we\u2019ve had over three hundred contributors and nearly 1k commits. Abstractions are some of the most queried tables on Dune. That makes creating them one of the highest leverage things a wizard can do. We want to make that experience better by launching the Spellbook , a retooling of our existing abstractions repo + a first-in-class open-source analytics engineering tool called dbt. dbt-core (data build tool) is an open-source framework that injects more classical software engineering practices into writing SQL by mixing SQL with JINJA templating. Abstractions, henceforth models in dbt-speak or spells in dune-speak, can be materialized into views and tables, but there are many possible refinements, including incrementally-loaded tables, date-partitioned tables, and more. These can all be compiled into SQL and run on dune.com. No more contributing code that you can\u2019t test without our help. dbt allows us to write and manage unit tests to spot and prevent any issues in our abstractions. Data integrity tests can easily be added with a single line in a YAML file. Models can be checked for unique primary keys, non-null values, accepted values, and relational integrity with minimal effort. dbt natively understands the dependencies between all models. In our old abstractions logic we were managing dependencies manually, which made deploying and maintaining them a mess. With dependency management, we can guarantee that all models are deployed in the correct order. We hope you are as excited as we are about this new tool. Spellbook is now live in prod and we welcome new contributors. Getting Started \u00b6 To get a better sense of what Spells are already available to you, check out our Spellbook Model Docs: Spellbook Model Docs To take a look under the hood, check out Spellbook on GitHub: Spellbook on GitHub To learn how to build your own Spells, check out our Getting Started section here: Spellbook Getting Started Guide","title":"Spellbook"},{"location":"spellbook/#new-level-unlocked","text":"We\u2019ve updated our database and now it\u2019s time to update our contribution tools. On January 31, 2020, we launched the abstractions repo as a place for wizards to create views and later tables. Since then, we\u2019ve had over three hundred contributors and nearly 1k commits. Abstractions are some of the most queried tables on Dune. That makes creating them one of the highest leverage things a wizard can do. We want to make that experience better by launching the Spellbook , a retooling of our existing abstractions repo + a first-in-class open-source analytics engineering tool called dbt. dbt-core (data build tool) is an open-source framework that injects more classical software engineering practices into writing SQL by mixing SQL with JINJA templating. Abstractions, henceforth models in dbt-speak or spells in dune-speak, can be materialized into views and tables, but there are many possible refinements, including incrementally-loaded tables, date-partitioned tables, and more. These can all be compiled into SQL and run on dune.com. No more contributing code that you can\u2019t test without our help. dbt allows us to write and manage unit tests to spot and prevent any issues in our abstractions. Data integrity tests can easily be added with a single line in a YAML file. Models can be checked for unique primary keys, non-null values, accepted values, and relational integrity with minimal effort. dbt natively understands the dependencies between all models. In our old abstractions logic we were managing dependencies manually, which made deploying and maintaining them a mess. With dependency management, we can guarantee that all models are deployed in the correct order. We hope you are as excited as we are about this new tool. Spellbook is now live in prod and we welcome new contributors.","title":"New level unlocked."},{"location":"spellbook/#getting-started","text":"To get a better sense of what Spells are already available to you, check out our Spellbook Model Docs: Spellbook Model Docs To take a look under the hood, check out Spellbook on GitHub: Spellbook on GitHub To learn how to build your own Spells, check out our Getting Started section here: Spellbook Getting Started Guide","title":"Getting Started"},{"location":"spellbook/spellbook-model-docs/","text":"Spellbook comes with dedicated dbt documentation to help you navigate the data within! Find it here: Spellbook Model Docs These work similarly to the V2 table documentation you can find here , with a couple of special features. Warning These Spellbook Table Docs and the Spellbook data lake are a work in progress. If you're not able to find a specific V2 table/column, it likely isn't ready for use in Spells yet. If you do find it but it's not yet fully labeled in the Spellbook Model docs, please check our V2 table documentation here . If you still have questions drop them in the #data-tables Discord channel ! General Navigation \u00b6 You can use the Project and Database navigation tabs on the left side of the window to explore Spellbook models, as well as the search bar up top. Project Tab \u00b6 The Project tab is where you'll find the Sources and Spellbook models. Sources models \u00b6 Sources are dbt data models built from data tables contained in the main Dune V2 data lake. Data must first be pulled into Spellbook Source models before it can be used in Spells, so using this list you can get an idea of what data is ready for use in your Spells, as well as what data might need to be added first as a Source model. For example, Arbitrum blocks data is available for Spells, but Arbitrum event logs are not. Spellbook models \u00b6 Below Sources you'll find a \"Projects\" heading, the only important thing here is the Spellbook folder which contains documentation on the various Spell-related code and data tables. Macros contains functions that make Spellbook work Models contains Spells Seeds contains static data used for testing Tests contains the unit tests that ensure Spells work as intended. The Lineage Graph \u00b6 You can click the blue icon on the bottom-right corner of a page to view the lineage graph of the model you're looking at: In the lineage graph, you'll see the immediate parents and children of the model you're exploring. Click the Expand button at the top-right of this lineage pane to see all of the models that are used to build, or are built from, the model you're exploring. Once expanded, click on any model name to highlight its related parent/child models: You can also right-click on models to interactively explore and filter the graph: Lastly, there are a variety of options for filtering the graph at the bottom:","title":"Spellbook Model Docs"},{"location":"spellbook/spellbook-model-docs/#general-navigation","text":"You can use the Project and Database navigation tabs on the left side of the window to explore Spellbook models, as well as the search bar up top.","title":"General Navigation"},{"location":"spellbook/spellbook-model-docs/#project-tab","text":"The Project tab is where you'll find the Sources and Spellbook models.","title":"Project Tab"},{"location":"spellbook/spellbook-model-docs/#sources-models","text":"Sources are dbt data models built from data tables contained in the main Dune V2 data lake. Data must first be pulled into Spellbook Source models before it can be used in Spells, so using this list you can get an idea of what data is ready for use in your Spells, as well as what data might need to be added first as a Source model. For example, Arbitrum blocks data is available for Spells, but Arbitrum event logs are not.","title":"Sources models"},{"location":"spellbook/spellbook-model-docs/#spellbook-models","text":"Below Sources you'll find a \"Projects\" heading, the only important thing here is the Spellbook folder which contains documentation on the various Spell-related code and data tables. Macros contains functions that make Spellbook work Models contains Spells Seeds contains static data used for testing Tests contains the unit tests that ensure Spells work as intended.","title":"Spellbook models"},{"location":"spellbook/spellbook-model-docs/#the-lineage-graph","text":"You can click the blue icon on the bottom-right corner of a page to view the lineage graph of the model you're looking at: In the lineage graph, you'll see the immediate parents and children of the model you're exploring. Click the Expand button at the top-right of this lineage pane to see all of the models that are used to build, or are built from, the model you're exploring. Once expanded, click on any model name to highlight its related parent/child models: You can also right-click on models to interactively explore and filter the graph: Lastly, there are a variety of options for filtering the graph at the bottom:","title":"The Lineage Graph"},{"location":"spellbook/spellbook-workshop-prerequisite/","text":"This is list of prerequisites strongly recommended before you go to a spellbook workshop! Have Python 3.9 installed on your computer. Install an IDE to edit your code. VSCode is a nice free one. Ensure that pip is installed. Install pipenv , this will allow us to create a virtualenv with dbt. Set up git and github including authentication. Make a fork of the spellbook repo . Including cloning locally and adding an upstream. Review Github\u2019s instructions on how to make a pull request from a fork. P.S. If you have trouble with these prerequisites, please join our \ud83e\uddd9 Community Discord and reach out for help in the #\ud83d\udcdcspellbook channel!","title":"Spellbook Workshop Prerequisites"},{"location":"spellbook/examples/","text":"As an example, we'll look at ERC-20. ERC-20 tokens are fungible tokens that all follow a contract standard set by the Ethereum Foundation. To track daily balances, we need to first identify the transfers. The main base Dune table we\u2019ll use for this purpose is erc20_ethereum.evt_Transfer which you can find via the data explorer. In our case, we have broken down the spell into a more modular series of spells: Reformatted transfers Daily aggregation of transfers Rolling sum of daily transfers Final daily balances for Ethereum ERC20 tokens","title":"Spellbook Examples"},{"location":"spellbook/examples/daily-aggregation/","text":"This sums all transfers for the day. This table is materialized as an incrementally loaded table updated every 15 minutes because the next step includes a slower ` window ` function to capture a rolling sum. There are a few novel components to make this spell incremental: The <div data-gb-custom-block data-tag=\"if\"> </div> JINJA block allows us to add an arbitrary filter when running in \u201cincremental\u201d mode. Incremental mode is default and a full refresh is denoted by a command line arg to completely recreate the table. Here we use this block to filter for all data timestamped in the last two days. We are running this model every fifteen minutes, but we allow a look back of 2 days to account for data arriving late from the blockchain. Does that create duplicates? It would, but we are also using a \u201c merge \u201d incremental_strategy. Merge strategies require a unique key and deduplicate the table upon each update. You\u2019ll see above in the transfers view, we created our own 'unique_transfer_id' by coalescing several transfer features together that we utilize here. You\u2019ll also note that this is the first use of \u201crefs\u201d in this spellset. A ref, like {{ ref('tokens_ethereum_erc20') }} is simply a reference to another model in the DBT project. It doesn\u2019t matter what the name of the view or table is. The ref references the name of the file itself. That means, we can\u2019t have duplicate file names. transfers_ethereum_erc20_agg_day.sql {{ config ( alias = 'erc20_agg_day' , materialized = 'incremental' , file_format = 'delta' , incremental_strategy = 'merge' , unique_key = 'unique_transfer_id' ) }} select 'ethereum' as blockchain , date_trunc ( 'day' , tr . evt_block_time ) as day , tr . wallet_address , tr . token_address , t . symbol , sum ( tr . amount_raw ) as amount_raw , sum ( tr . amount_raw / power ( 10 , t . decimals )) as amount , unique_tx_id || '-' || wallet_address || '-' || token_address || '-' || sum ( tr . amount_raw ):: string as unique_transfer_id from {{ ref ( 'transfers_ethereum_erc20' ) }} tr left join {{ ref ( 'tokens_ethereum_erc20' ) }} t on t . contract_address = tr . token_address { % raw % } { % if is_incremental () % } -- this filter will only be applied on an incremental run where date_trunc ( 'day' , tr . evt_block_time ) > now () - interval 2 days { % endif % } { % endraw % } group by date_trunc ( 'day' , tr . evt_block_time ), tr . wallet_address , tr . token_address , t . symbol , unique_tx_id transfers_ethereum_schema.yml - name : transfers_ethereum_erc20_agg_hour meta : blockchain : ethereum sector : transfers project : erc20 contibutors : soispoke, dot2dotseurat config : tags : [ 'transfers' , 'ethereum' , 'erc20' , 'agg_hour' , 'soispoke' , 'dot2dotseurat' ] columns : - *blockchain - &hour name : hour description : \"UTC event block time truncated to the hour mark.\" - *wallet_address - *token_address - name : symbol description : \"ERC20 token symbol\" - *amount_raw - name : amount description : \"Raw amount of ERC20 token held *after* taking into account token decimals\" - name : amount_usd description : \"Amount of ERC20 token held in USD (fiat value at time of transaction)\"","title":"Daily Aggregation"},{"location":"spellbook/examples/final-day-balance/","text":"This is our final daily Ethereum ERC20 token balances spell. We expand our spell to cover all days, not just the days with transfer activity. We add price data, we remove known rebase tokens and any tokens that resulted in large negative balances. The ref tokens_ethereum_rebase is a static list of known rebase tokens that we manage. Whereas, the ref 'balances_ethereum_erc20_noncompliant' is a table we derive from transfers_ethereum_erc20_rolling_day. That table looks for unique token_addresses with larger negative balances which indicate the contract may not be compliant with ERC20. balances_ethereum_erc20_day.sql {{ config ( alias = 'erc20_day' ) }} with days as ( select explode ( sequence ( to_date ( '2015-01-01' ), date_trunc ( 'day' , now ()), interval 1 day ) ) as day ) , daily_balances as ( SELECT wallet_address , token_address , amount_raw , amount , day , symbol , lead ( day , 1 , now ()) OVER ( PARTITION BY token_address , wallet_address ORDER BY day ) AS next_day FROM {{ ref ( 'transfers_ethereum_erc20_rolling_day' ) }} ) SELECT 'ethereum' as blockchain , d . day , b . wallet_address , b . token_address , b . amount_raw , b . amount , b . amount * p . price as amount_usd , b . symbol FROM daily_balances b INNER JOIN days d ON b . day <= d . day AND d . day < b . next_day LEFT JOIN {{ source ( 'prices' , 'usd' ) }} p ON p . contract_address = b . token_address AND d . day = p . minute AND p . blockchain = 'ethereum' -- Removes rebase tokens from balances LEFT JOIN {{ ref ( 'tokens_ethereum_rebase' ) }} as r ON b . token_address = r . contract_address -- Removes likely non-compliant tokens due to negative balances LEFT JOIN {{ ref ( 'balances_ethereum_erc20_noncompliant' ) }} as nc ON b . token_address = nc . token_address WHERE r . contract_address is null and nc . token_address is null transfers_ethereum_schema.yml - name : balances_ethereum_erc20_day meta : blockchain : ethereum sector : balances project : erc20 contibutors : soispoke, dot2dotseurat config : tags : [ 'balances' , 'ethereum' , 'erc20' , 'day' , 'soispoke' , 'dot2dotseurat' ] description : > Daily token balances of ERC20 Ethereum tokens per wallet and contract address pair. Depends on erc20_ethereum_transfers. columns : - *blockchain - &day name : day description : \"UTC event block time truncated to the day mark\" - *wallet_address - *token_address - *amount_raw - *amount - *amount_usd - *symbolyam","title":"Final Daily Balance"},{"location":"spellbook/examples/reformatted/","text":"Our base table records the transfer amount to and from an account. To make it easier to sum of transfers, we munge this into a union of sent txns and received txns. Additionally, WETH requires special handling given the additional functions of deposit and withdrawal. This means we need to add zeroex_ethereum.weth9_evt_deposit as a source like we did for erc20_ethereum.evt_transfer above. Similar to a source, the model is defined in a YAML file. This is where things like the description, tests, and metadata are defined. This is also where we track \u201ccontributors\u201d. So make sure you get your clout and add your handle when writing or editing a spell. Then you\u2019ll be credited for your contribution in the documentation . In the JINJA config block, we define that the alias for this view is erc20 . Without this alias, the table name would default to the file name. The schema name for this view is defined in the dbt_project.yml file in the root of the Spellbook project. Schema\u2019s are defined there by the directory structure. The name of this view would be transfers_ethereum.erc20 given the current structure. Note: we're generally against using SHOUT CASE, that\u2019s what IDEs are for. Sue us. transfers_ethereum_erc20.sql {{ config ( materialized = 'view' , alias = 'erc20' ) }} with sent_transfers as ( select 'send' || '-' || evt_tx_hash || '-' || evt_index || '-' || ` to ` as unique_tx_id , ` to ` as wallet_address , contract_address as token_address , evt_block_time , value as amount_raw from {{ source ( 'erc20_ethereum' , 'evt_transfer' ) }} ) , received_transfers as ( select 'receive' || '-' || evt_tx_hash || '-' || evt_index || '-' || ` from ` as unique_tx_id , ` from ` as wallet_address , contract_address as token_address , evt_block_time , - value as amount_raw from {{ source ( 'erc20_ethereum' , 'evt_transfer' ) }} ) , deposited_weth as ( select 'deposit' || '-' || evt_tx_hash || '-' || evt_index || '-' || dst as unique_tx_id , dst as wallet_address , contract_address as token_address , evt_block_time , wad as amount_raw from {{ source ( 'zeroex_ethereum' , 'weth9_evt_deposit' ) }} ) , withdrawn_weth as ( select 'withdrawn' || '-' || evt_tx_hash || '-' || evt_index || '-' || src as unique_tx_id , src as wallet_address , contract_address as token_address , evt_block_time , - wad as amount_raw from {{ source ( 'zeroex_ethereum' , 'weth9_evt_withdrawal' ) }} ) select unique_tx_id , 'ethereum' as blockchain , wallet_address , token_address , evt_block_time , amount_raw from sent_transfers union select unique_tx_id , 'ethereum' as blockchain , wallet_address , token_address , evt_block_time , amount_raw from received_transfers union select unique_tx_id , 'ethereum' as blockchain , wallet_address , token_address , evt_block_time , amount_raw from deposited_weth union select unique_tx_id , 'ethereum' as blockchain , wallet_address , token_address , evt_block_time , amount_raw from withdrawn_weth transfers_ethereum_schema.yml models : - name : transfers_ethereum_erc20 meta : blockchain : ethereum sector : transfers project : erc20 contibutors : soispoke, dot2dotseurat config : tags : [ 'transfers' , 'ethereum' , 'erc20' , 'soispoke' , 'dot2dotseurat' ] description : \"ERC20 Token Transfers on Ethereum. This table is updated every 15 minutes.\" columns : - name : unique_transfer_id description : \"Unique transfer ID (used for testing for duplicates)\" tests : - unique - &blockchain name : blockchain description : \"Blockchain\" - &wallet_address name : wallet_address description : \"Wallet address of sender or receiver. If amount is negative, wallet address is the sender's.\" - &token_address name : token_address description : \"Contract address for token\" - &evt_block_time name : evt_block_time description : \"Timestamp for block event time in UTC\" - &amount_raw name : amount_raw description : \"Raw amount of ERC20 token held *before* taking into account token decimals\" dbt_project.yml transfers : +schema : transfers +materialized : view ethereum : +schema : transfers_ethereum +materialized : view","title":"Reformatted Transfers"},{"location":"spellbook/examples/rolling-sum/","text":"The next step is to apply the rolling sum window function to each daily transfer sum. This is a pretty straightforward query. We\u2019d end here for balances if it was guaranteed that each wallet/contract pair made a transfer every day. But since that\u2019s not the case we\u2019ll finish the spell in the next model by filling in all the missing days and doing a few more clean up steps. transfers_ethereum_erc20_rolling_day.sql {{ config ( alias = 'erc20_rolling_day' ) }} select 'ethereum' as blockchain , day , wallet_address , token_address , symbol , current_timestamp () as last_updated , row_number () over ( partition by token_address , wallet_address order by day desc ) as recency_index , sum ( amount_raw ) over ( partition by token_address , wallet_address order by day ) as amount_raw , sum ( amount ) over ( partition by token_address , wallet_address order by day ) as amount from {{ ref ( 'transfers_ethereum_erc20_agg_day' ) }} transfers_ethereum_schema.yml - name : transfers_ethereum_erc20_rolling_hour meta : blockchain : ethereum sector : transfers project : erc20 contibutors : soispoke, dot2dotseurat config : tags : [ 'transfers' , 'ethereum' , 'erc20' , 'rolling_hour' , 'soispoke' , 'dot2dotseurat' ] columns : - *blockchain - *hour - *wallet_address - *token_address - name : symbol description : \"ERC20 token symbol\" - *amount_raw - name : amount description : \"Rolling sum of raw amount of ERC20 token held *after* taking into account token decimals\" - name : amount_usd description : \"Rolling sum of amount of ERC20 token held in USD (fiat value at time of transaction)\" - name : updated_at description : \"UTC timestamp when table was last updated\" - name : recency_index description : \"Index of most recent balance ascending. recency_index=1 is the wallet/contract pair's most recent balance\"","title":"Rolling Sum of Daily Transfers"},{"location":"spellbook/getting-started/","text":"Spellbook is essentially a DBT project that runs on Dune's data platform. At a high level, you need to setup our DBT project locally, develop a spell in Jinja-templated SQL, and get it to run on our data platform. To start with, clone the Spellbook GitHub repository and setup your local dev env as per the README: Spellbook on GitHub Decide what problem you're trying to solve. Think about the data model . Write your spell. At minimum, you'll need to: Reference your data sources , with tests and freshness checks Define a unit test for your spell Write your spell as a SQL select statement with Jinja templating Write a schema in a separate YAML file To learn more about all the features in DBT, take a look at their documentation . We've written a few example guides for building spells for ERC20. Tip As you go along, you can use dbt compile to generate a SQL statement, and validate it on dune.com in the Query editor. Once you're happy with your spell, you can submit it to Dune : Fork the Spellbook repository and open a pull request Tag duneanalytics/data-experience and write a short description of your spell Your PR will trigger your spell to be created in the staging database and run tests If everything looks good, Dune will merge your changes and deploy them to production After that, your spell will be visible on dune.com under \"Spells\". LFG! How To Guides \u00b6 Check out our How To Guides section for a variety of step-by-step tutorials to help you start casting Spells in no time!","title":"Spellbook Getting Started"},{"location":"spellbook/getting-started/#how-to-guides","text":"Check out our How To Guides section for a variety of step-by-step tutorials to help you start casting Spells in no time!","title":"How To Guides"},{"location":"spellbook/getting-started/data-modelling/","text":"This is the most important part of spell building. A spell should be a view (ideally) or table you or others can use to do great analysis. The benefits of building a spell are: Consistency and maintainability. Forking and copying queries to remix for ad-hoc analysis is fantastic but ngmi for the long run. Imagine finding a bug in your query or something simply changing in a contract. You can update your query but everyone who forked or copy pasta\u2019d the query won\u2019t get the update. Improved clarity on metrics. There are often a few ways a metric can be calculated. If everyone builds their version, it\u2019s difficult to determine the source of truth, and time is wasted hunting down the inconsistencies. Spells are built in a completely transparent fashion on a public GitHub repo which means the community can work to agree upon implementations. Simplicity and developer experience. Who wants to write the same query over and over again? Automate the boring and focus on novel analysis. DRY applies to SQL as much as it does to other programming languages. Some signs it\u2019s time to make a spell: When you have multiple queries that include the same subquery. You\u2019re copying long(ish) lists of static values between queries. The query has been forked or copied many times. Your query contains complex logic that you\u2019d like to use elsewhere or that others might benefit from.","title":"Data Modelling"},{"location":"spellbook/getting-started/data-sources/","text":"In the Spellbook project, the source (i.e. raw and decoded tables your spell depends on) must be defined in a YAML file. Once a source has been defined it does not need to be defined again. Defining a source can be as simple as adding its schema and name. But to harness the full power of the tool, we\u2019ll also want to define tests, freshness checks, and add column descriptions. Define the table name and description under the schema. sources : - name : erc20_ethereum description : \"Transfers events for ERC20 tokens.\" tables : - name : evt_transfer Add a freshness check. Freshness checks use a timestamp type field to validate when the last rows of data were loaded. Here we\u2019ll use evt_block_time and use the DBT default warning and error checks. sources : - name : erc20_ethereum description : \"Transfers events for ERC20 tokens.\" tables : - name : evt_transfer loaded_at_field : evt_block_time description : \"Transfers events for ERC20 tokens.\" freshness : warn_after : { count : 12 , period : hour } error_after : { count : 24 , period : hour } Add column descriptions and tests. Regarding tests, generally, you\u2019ll want to make the primary key both unique and non_null. You can use an ampersand anchor & before a column name to reuse its description later in the same YAML file. * column name will reuse the column description defined earlier. sources : - name : erc20_ethereum description : \"Transfers events for ERC20 tokens.\" tables : - name : evt_transfer loaded_at_field : evt_block_time description : \"Transfers events for ERC20 tokens.\" freshness : warn_after : { count : 12 , period : hour } error_after : { count : 24 , period : hour } columns : - name : contract_address description : \"ERC20 token contract address\" - &evt_tx_hash name : evt_tx_hash description : \"Transaction hash of the event\" - &evt_index name : evt_index description : \"Event index\" - &evt_block_time name : evt_block_time description : \"Timestamp for block event time in UTC\" - &evt_block_number name : evt_block_number description : \"Event block number\" - *from - *to - name : value description : \"Amount of ERC20 token transferred\" These descriptions are rendered in the docs by dbt. Locally, you can open them by running dbt docs generate and dbt docs serve from your CLI in the Spellbook directory. They are automatically deployed as our Spellbook docs .","title":"Data Sources"},{"location":"spellbook/getting-started/spells/","text":"Everything in a .sql file should solely consist of a select statement. You don\u2019t need to specify create view or create table . Materialization strategies are handled by JINJA blocks or by the YAML schema file for the model. Models will (by default) be views but we can override that to make them tables or incrementally loaded tables. The basic trade-off is that a view is fast to create and doesn\u2019t require additional storage but is slower to query. A table is much slower to create and does require additional storage but is faster to query. Generally, we\u2019ll try to stick to views but upgrade to tables or incrementally loaded tables if performance is an issue. To learn more about all the features in DBT, take a look at their documentation . We've written a few example guides for building spells for ERC20. Checking your spell \u00b6 Copy your spell from the target folder and try running it on dune.com in a new query window. We are working diligently on a secure sandbox where you can freely run your spells directly from DBT. But in the meantime, the best way to check your work locally is to \u201ccompile\u201d your spells. After you\u2019re instatited DBT with `dbt init`, run `dbt compile`. This will create a new folder called \u201ctarget\u201d. Inside the target folder will be all of the spells compiled into plain SQL. Then you can copy your spell and run it directly on dune.com.","title":"Spells"},{"location":"spellbook/getting-started/spells/#checking-your-spell","text":"Copy your spell from the target folder and try running it on dune.com in a new query window. We are working diligently on a secure sandbox where you can freely run your spells directly from DBT. But in the meantime, the best way to check your work locally is to \u201ccompile\u201d your spells. After you\u2019re instatited DBT with `dbt init`, run `dbt compile`. This will create a new folder called \u201ctarget\u201d. Inside the target folder will be all of the spells compiled into plain SQL. Then you can copy your spell and run it directly on dune.com.","title":"Checking your spell"},{"location":"spellbook/getting-started/submissions/","text":"Here's how you can contribute your Spell on GitHub. Fork the Spellbook GitHub Repo and open a Pull Request. \u00b6 When you\u2019re happy with your Spell, from a fork of the Spellbook GitHub Repo , open a Pull Request. This step is non-specific to Spellbook. So I will farm out the explanation to Jake Jarvis who has an excellent step by step guide to forking and committing a Pull Request to a public GitHub repository. Once you\u2019ve opened your pull request, fill out the pull request template and tag the duneanalytics/team-data-experience team-data-experience. We will review your code and run your tests. Our goal is to eliminate this step and provide a sandbox where you can run your spells and tests directly. Unfortunately (fortunately?) GitHub rightfully blocks secrets from pull requests from forks which is why we can\u2019t run DBT from your original pull requests. Dune will merge and deploy \u00b6 If everything looks good, Dune will merge your changes and deploy them to production. Your Spells will be visible on Dune V2 Data Explorer under Spells. And ta-da! You are a Spell casting wizard.","title":"Submissions"},{"location":"spellbook/getting-started/submissions/#fork-the-spellbook-github-repo-and-open-a-pull-request","text":"When you\u2019re happy with your Spell, from a fork of the Spellbook GitHub Repo , open a Pull Request. This step is non-specific to Spellbook. So I will farm out the explanation to Jake Jarvis who has an excellent step by step guide to forking and committing a Pull Request to a public GitHub repository. Once you\u2019ve opened your pull request, fill out the pull request template and tag the duneanalytics/team-data-experience team-data-experience. We will review your code and run your tests. Our goal is to eliminate this step and provide a sandbox where you can run your spells and tests directly. Unfortunately (fortunately?) GitHub rightfully blocks secrets from pull requests from forks which is why we can\u2019t run DBT from your original pull requests.","title":"Fork the Spellbook GitHub Repo and open a Pull Request."},{"location":"spellbook/getting-started/submissions/#dune-will-merge-and-deploy","text":"If everything looks good, Dune will merge your changes and deploy them to production. Your Spells will be visible on Dune V2 Data Explorer under Spells. And ta-da! You are a Spell casting wizard.","title":"Dune will merge and deploy"},{"location":"spellbook/getting-started/tests/","text":"How to define a unit test for your spell. Writing a test before I write my spell? What is this software engineering? Is this language even Turing Complete?! We are striving for test-driven development. That means, we think about what results we want from our spell before we write it. Then, we write a test that will pass if those results match our output. If you are having trouble imagining what the output of your spell should look like, you might want to go back to data modelling. Writing a good unit test requires creativity and a little grit. You need to find a way to validate some of your output which might require some manual calculations. For example, Etherscan token check will return a token balance for a given wallet address, contract address, and date. We can manually record results for a handful of tests cases, either in a CSV file to be uploaded as a dbt seed or as plain values in the unit test itself. Another option could be using your wallet to track function calls and calculating what the output of the spell would be just for that wallet. These tests do not need to be particularly comprehensive. They should be added whenever someone is making a substantive change to the model in the future. The main goal here is to first ensure your model is likely correct and prevent regressions in the future. Tests should return zero rows to pass. You can get creative, there\u2019s no set way to write the test. One way I like to write tests against individual values is to compare values in a case when statement. I hard code in values that I compare to my final model. The \u201cref\u201d should be the file name for your intended spell, e.g. {{ ref('balances_ethereum_erc20_day' )}}. A ref is how we reference other spells or models in the DBT project. You don\u2019t need the model written at this point, just decide on the file name. You will run the test after you finish your spell. Unless you want to try running your test now to confirm it will fail \ud83d\ude09. WITH unit_test1 AS ( SELECT CASE WHEN amount == 100 THEN TRUE ELSE FALSE END AS test FROM {{ ref ( 'balances_ethereum_erc20_day' ) }} WHERE wallet_address = '0x8de61aeacd24d2865a4fb471b8e746b02ef4e346' AND contract_address = '0x00000000000045166c45af0fc6e4cf31d9e14b9a' AND DATE = '2022-06-27 00:00' ), unit_test2 AS ( SELECT CASE WHEN token == 'ONT' AND amount == 7 THEN TRUE ELSE FALSE END AS test FROM {{ ref ( 'balances_ethereum_erc20_day' ) }} WHERE wallet_address = '0x01bcb7117f00c4d3141ccab2432c7ae3bd5b00d3' AND contract_address = '0x0000000000004946c0e9f43f4dee607b0ef1fa1c' AND DATE = '2022-06-27 00:00' ) SELECT * FROM ( SELECT * FROM unit_test1 UNION SELECT * FROM unit_test2 ) WHERE test = FALSE","title":"Tests"},{"location":"spellbook/how-to-guides/","text":"Here you'll find a variety of how-to guides for making \u2728 with Spellbook! Casting a Spell from Scratch \u00b6 In collaboration with MetricsDAO , in this workshop @agaperste walks us through casting a Spell from scratch! Spellbook Intro with Megan Heintz \u00b6 In this DuneCon workshop , Dune Team member Megan Heintz (who came up with the name \"Spellbook\") walks us through Spellbook's infrastructure and how to migrate data to a Spell: web3 Data Guide + Spellbook Tutorial \u00b6 In this tutorial, @ilemi aka Andrew Hong shows us the main protocol interactions (creating a pair, managing liquidity, swapping through pairs) and how to pull and transform data on Ethereum using. Read his guide here or watch the video below:","title":"How-To Guides"},{"location":"spellbook/how-to-guides/#casting-a-spell-from-scratch","text":"In collaboration with MetricsDAO , in this workshop @agaperste walks us through casting a Spell from scratch!","title":"Casting a Spell from Scratch"},{"location":"spellbook/how-to-guides/#spellbook-intro-with-megan-heintz","text":"In this DuneCon workshop , Dune Team member Megan Heintz (who came up with the name \"Spellbook\") walks us through Spellbook's infrastructure and how to migrate data to a Spell:","title":"Spellbook Intro with Megan Heintz"},{"location":"spellbook/how-to-guides/#web3-data-guide-spellbook-tutorial","text":"In this tutorial, @ilemi aka Andrew Hong shows us the main protocol interactions (creating a pair, managing liquidity, swapping through pairs) and how to pull and transform data on Ethereum using. Read his guide here or watch the video below:","title":"web3 Data Guide + Spellbook Tutorial"}]}